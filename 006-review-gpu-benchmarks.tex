\section{GPU benchmarking}
\label{sec:gpu}

\subsection{Software Variability}
\label{sec:gpu-swVar}

Software variability (Juriâ€™s work in Tensorflow vs. PyTorch)

\subsection{Performance Variability}
\label{sec:gpu-var}

% Measurement variability: often much lower for GPUs (Matt Sinclair can help with this)

Modern science applications frequently require peta- or exascale levels of compute to model topics with a high level of fidelity.
To meet these demands in reasonable timeframes, scientists and researchers typically run these workloads on massively parallel systems such as GPUs.
For example, workloads such as graph analytics~\cite{CheBeckmann2013, WangPan2017-gunrock}, scientific computing~\cite{coral2, olcf6-bmks, kim2018qmcpack, WuTaylor2019-candle}, ML~\cite{BanburyReddi2021-tinyMLPerf, BaruahShivdikar2021-gnnMark, DongKaeli2017-dnnmark, Narang2017-deepBench, MattsonCheng2019-mlperfTrain, MattsonReddi2020-mlPerf, Reddi2020mlperf-Infer, ReddiCheng2021-mlPerfVision} heavily utilize GPUs.
Increasingly, ML is also impacting scientific applications~\cite{fan2021predicting, jumper2021highly, kates2019predicting, ThiyagalingamShankar2022-mlSci, ThiyagalingamVonLaszewski2022-aiForSciMLCommons}, by replacing or supplementing traditional computing methods in application domains like molecular dynamics (e.g., DeePMD~\cite{WangZhang2018-deepmd, ZengZhang2023-deepmd2}), protein folding (e.g., OpenFold2~\cite{openfold2}), and scientific AI models (e.g., AuroraGPT~\cite{Stevens2023-auroraGPT}).
However, given the scale of data these workloads operate on, as well as the large size of the workloads themselves, typically these workloads must partition their work across many GPUs.

% why this matters a lot
Given their widespread use and trend towards many GPU applications, it is desirable from a benchmark carpentry perspective to make GPU experiments repeatable and consistent.
% variability trends
For older HPC systems composed of multiple CPUs, prior work showed that this was difficult to achieve: application performance varied by up to 20\%, even for CPUs with the same architecture and vendor SKU (Stock-Keeping Unit)~\cite{AcunLanger2016-power,chasapis2016runtime, ChasapisMoreto2019-powerEfficJobSched, InadomiPatki2015-scVar, PatelWagenhauser2020-hpcPowerConsump, SkinnerKramer2005-perfVarCauses}. This variation occurs due to the manufacturing process and the chip's power constraints~\cite{ChasapisMoreto2019-powerEfficJobSched,Scogland2015-pwrPerspectives}.
Such dynamic behavior makes it challenging for %applications to achieve
repeatable, high performance and can lead to resource underutilization.
Unfortunately, similar issues also arise in modern systems composed of many GPUs.
Recent work has demonstrated that GPU-rich systems suffer from significant performance variability~\cite{DeBardeleben-LBNL-EuroPar13, Fraternali-EEHPCVar-2018, Scogland2015-pwrPerspectives, sinha2022notall}.

Across five modern GPU-rich clusters with a variety of sizes, cooling approaches, and GPU vendors, this work found that applications exhibited performance variability of 8\% on average (max 22\%) with outliers up to 1.5$\times$ slower than the median GPU.
Moreover, these results were consistent over time (i.e., not transient) and were unaffected by GPU vendors or cooling type.
Interestingly, this performance variability was also application-specific: the more compute-intensive the application was, the more performance variability the application observed due to effects of the GPU's power management algorithm (e.g., Dynamic Voltage \& Frequency Scaling, or DVFS).
%This performance variability is application-specific, not vendor specific, not cluster size specific, not resolved by cooling, not transient
%Gets worse as transistors scale, especially for more compute-intensive workloads.
Furthermore, performance variability is getting worse as transistors continue scaling~\cite{DRAMthermalissues}.

Although the impact of performance variability is significant for single GPU workloads, for multi-GPU workloads the impact is even larger.
Currently, GPU-rich systems focus on scheduling work to minimize the number of nodes an application requests, without considering 
In the five clusters from this prior work, users asking for 4 GPUs for a given application would get a slower GPU allocated to them between 22\% (Sandia's Vortex cluster) and 50\% (TACC's Longhorn cluster~\cite{stanzione2020frontera, tacc}) of the time.
Thus, users are likely to get a slow GPU frequently, especially since modern scientific workloads often request 64 or more GPUs for a given experiment.
This can lead to significant resource under-utilization for multi-GPU jobs since all of them must wait for the slowest one to complete due to the bulk synchronous programming (BSP) model used in many data-parallel workloads~\cite{paszke2017-pytorch}.
Accordingly, it is imperative for users to be aware of the impact of performance variability on their experiments, and for benchmark carpentry to propose solutions to minimize its impact.

% what can users do from carpentry perspective
Although GPU-rich systems are likely to suffer from performance variability for the foreseeable future, there are a number of steps users, maintainers, and system designers can take for existing systems to reduce its impact on gathering statistically significant results.
First, cluster operators can perform periodic performance variability benchmarking, similar to these prior works, to identify ill-performing GPUs and perform targeted maintenance on them.
Likewise, users can perform similar benchmarking before running final results to identify GPUs that behave similarly and utilize blacklisting or other scheduling approaches to attempt to schedule work on GPUs with similar performance variability profiles.
However, doing so can be time and labor intensive for clusters with thousands or more GPUs (although it is a one-time cost since a GPU's performance variability is consistent over time).
Thus, a more scalable, dynamic approach is to redesign job scheduling policies for GPU clusters to consider performance variability when making scheduling decisions.
Recent work has shown that embracing performance variability can transparently, significantly improve job completion time, makespan, and GPU utilization~\cite{JainTran2024-pal}.
Finally, since performance variability is application-specific, for new applications that have not already been profiled, we recommend either identifying other applications with similar profiles to use it as a proxy~\cite{Guerreiro-appClasses} or perform profiling when running the application on a new cluster for the first time to identify its sensitivity to performance variability.

%\input{database}