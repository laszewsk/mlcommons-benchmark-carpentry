@Misc{Reiley2016,
  author =	 {{Carol Reiley}},
  title =	 {{Deep Driving}},
  note =
                  {\url{https://www.technologyreview.com/s/602600/deep-driving/}},
  year =	 2016
}

@article{shockwave,
  title =	 {Shockwave: Fair and Efficient Cluster Scheduling for
                  Dynamic Adaptation in Machine Learning},
  author =	 {Pengfei Zheng and Rui Pan and Tarannum Khan and
                  Shivaram Venkataraman and Aditya Akella},
  year =	 2022,
  journal =	 {arXiv preprint 2210.00093},
}

@inproceedings{shen2019nexus,
  title =	 {Nexus: a GPU cluster engine for accelerating
                  DNN-based video analysis},
  author =	 {Shen, Haichen and Chen, Lequn and Jin, Yuchen and
                  Zhao, Liangyu and Kong, Bingyu and Philipose,
                  Matthai and Krishnamurthy, Arvind and Sundaram,
                  Ravi},
  booktitle =	 {Proceedings of the 27th ACM Symposium on Operating
                  Systems Principles},
  pages =	 {322--337},
  year =	 2019
}

@inproceedings{marathe2017performance,
  title =	 {Performance modeling under resource constraints
                  using deep transfer learning},
  author =	 {Marathe, Aniruddha and Anirudh, Rushil and Jain,
                  Nikhil and Bhatele, Abhinav and Thiagarajan,
                  Jayaraman and Kailkhura, Bhavya and Yeom, Jae-Seung
                  and Rountree, Barry and Gamblin, Todd},
  booktitle =	 {International Conference for High Performance
                  Computing, Networking, Storage and Analysis},
  pages =	 31,
  year =	 2017,
  organization = {ACM}
}

@article{zaharia2016apache,
  title =	 {Apache Spark: a unified engine for big data
                  processing},
  author =	 {Zaharia, Matei and Xin, Reynold S and Wendell,
                  Patrick and Das, Tathagata and Armbrust, Michael and
                  Dave, Ankur and Meng, Xiangrui and Rosen, Josh and
                  Venkataraman, Shivaram and Franklin, Michael J and
                  others},
  journal =	 {Communications of the ACM},
  volume =	 59,
  number =	 11,
  pages =	 {56--65},
  year =	 2016,
  publisher =	 {ACM New York, NY, USA}
}

@article{meng2016mllib,
  title =	 {Mllib: Machine learning in apache spark},
  author =	 {Meng, Xiangrui and Bradley, Joseph and Yavuz, Burak
                  and Sparks, Evan and Venkataraman, Shivaram and Liu,
                  Davies and Freeman, Jeremy and Tsai, DB and Amde,
                  Manish and Owen, Sean and others},
  journal =	 {The Journal of Machine Learning Research},
  volume =	 17,
  number =	 1,
  pages =	 {1235--1241},
  year =	 2016,
  publisher =	 {JMLR. org}
}

@inproceedings{rao2017experiments,
  title =	 {Experiments and Analyses of Data Transfers over
                  Wide-Area Dedicated Connections},
  author =	 {Rao, Nageswara SV and Liu, Qiang and Sen, Satyabrata
                  and Hanley, Jesse and Foster, Ian and Kettimuthu,
                  Rajkumar and Wu, Chase Q and Yun, Daqing and
                  Towsley, Don and Vardoyan, Gayane},
  booktitle =	 {26th International Conference on Computer
                  Communication and Networks},
  pages =	 {1--9},
  year =	 2017,
  organization = {IEEE}
}

@inproceedings{beckman2017skluma,
  title =	 {Skluma: A Statistical Learning Pipeline for Taming
                  Unkempt Data Repositories},
  author =	 {Beckman, Paul and Skluzacek, Tyler J and Chard, Kyle
                  and Foster, Ian},
  booktitle =	 {Proceedings of the 29th International Conference on
                  Scientific and Statistical Database Management},
  pages =	 41,
  year =	 2017,
  organization = {ACM}
}

@Misc{DLHub,
  title =	 {Building an {ALCF} Data Service: Interactive,
                  Scalable, Reproducible Data Science},
  year =	 2017,
  note =
                  {\url{https://www.slideshare.net/ianfoster/going-smart-and-deep-on-materials-at-alcf}}
}

@article{chard2017cost,
  title =	 {Cost-Aware Cloud Profiling, Prediction, and
                  Provisioning as a Service},
  author =	 {Chard, Ryan and Chard, Kyle and Wolski, Rich and
                  Madduri, Ravi and Ng, Bryan and Bubendorfer, Kris
                  and Foster, Ian},
  journal =	 {IEEE Cloud Computing},
  volume =	 4,
  number =	 4,
  pages =	 {48--59},
  year =	 2017,
  publisher =	 {IEEE}
}

@book{foster1995designing,
  title =	 {Designing and Building Parallel Programs},
  author =	 {Foster, Ian},
  year =	 1995,
  publisher =	 {Addison Wesley Publishing Company Boston}
}

@article{lecun2015deep,
  title =	 {Deep learning},
  author =	 {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal =	 {Nature},
  volume =	 521,
  number =	 7553,
  pages =	 {436--444},
  year =	 2015,
  publisher =	 {Nature Research}
}

@book{hennessy2011computer,
  title =	 {Computer Architecture: A Quantitative Approach},
  author =	 {Hennessy, John L and Patterson, David A},
  year =	 2011,
  publisher =	 {Elsevier}
}

@inproceedings{liu2017explaining,
  title =	 {Explaining wide area data transfer performance},
  author =	 {Liu, Zhengchun and Balaprakash, Prasanna and
                  Kettimuthu, Rajkumar and Foster, Ian},
  booktitle =	 {26th International Symposium on High-Performance
                  Parallel and Distributed Computing},
  pages =	 {167--178},
  year =	 2017,
  organization = {ACM}
}

@article{foster1997parallel,
  title =	 {Parallel algorithms for the spectral transform
                  method},
  author =	 {Foster, Ian T and Worley, Patrick H},
  journal =	 {SIAM Journal on Scientific Computing},
  volume =	 18,
  number =	 3,
  pages =	 {806--837},
  year =	 1997,
  publisher =	 {SIAM}
}

@article{Silver2016,
  title =	 {Mastering the game of {Go} with deep neural networks
                  and tree search},
  author =	 {Silver, David and Huang, Aja and Maddison, Chris J
                  and Guez, Arthur and Sifre, Laurent and Van Den
                  Driessche, George and Schrittwieser, Julian and
                  Antonoglou, Ioannis and Panneershelvam, Veda and
                  Lanctot, Marc and others},
  journal =	 {Nature},
  volume =	 529,
  number =	 7587,
  pages =	 {484--489},
  year =	 2016,
  publisher =	 {Nature Publishing Group}
}

@Misc{Alba2016,
  author =	 {{Alba, Davey}},
  title =	 {{Only Amazon Could Make a Checkout-Free Grocery
                  Store a Reality}},
  note =
                  {\url{https://www.wired.com/2016/12/amazon-go-grocery-store/}},
  year =	 2016
}

@article{Pedregosa2011,
  title =	 {Scikit-learn: Machine Learning in {P}ython},
  author =	 {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and
                  Michel, V.  and Thirion, B. and Grisel, O. and
                  Blondel, M. and Prettenhofer, P.  and Weiss, R. and
                  Dubourg, V. and Vanderplas, J. and Passos, A. and
                  Cournapeau, D. and Brucher, M. and Perrot, M. and
                  Duchesnay, E.},
  journal =	 {Journal of Machine Learning Research},
  volume =	 12,
  pages =	 {2825--2830},
  year =	 2011
}

@article{Meng2016,
  title =	 {{MLlib: Machine learning in apache spark}},
  author =	 {Meng, Xiangrui and Bradley, Joseph and Yavuz, Burak
                  and Sparks, Evan and Venkataraman, Shivaram and Liu,
                  Davies and Freeman, Jeremy and Tsai, DB and Amde,
                  Manish and Owen, Sean and others},
  journal =	 {Journal of Machine Learning Research},
  volume =	 17,
  number =	 34,
  pages =	 {1--7},
  year =	 2016
}

@article{Jia2014,
  Author =	 {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff
                  and Karayev, Sergey and Long, Jonathan and Girshick,
                  Ross and Guadarrama, Sergio and Darrell, Trevor},
  Journal =	 {arXiv preprint arXiv:1408.5093},
  Title =	 {Caffe: Convolutional Architecture for Fast Feature
                  Embedding},
  Year =	 2014
}

@inproceedings{Abadi2016,
  title =	 {TensorFlow: A system for large-scale machine
                  learning},
  author =	 {Abadi, Mart{\'\i}n and Barham, Paul and Chen,
                  Jianmin and Chen, Zhifeng and Davis, Andy and Dean,
                  Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and
                  Irving, Geoffrey and Isard, Michael and others},
  booktitle =	 {Proceedings of the 12th USENIX Symposium on
                  Operating Systems Design and Implementation
                  (OSDI). Savannah, Georgia, USA},
  year =	 2016
}

@inproceedings{Collobert2011,
  title =	 {Torch7: A matlab-like environment for machine
                  learning},
  author =	 {Collobert, Ronan and Kavukcuoglu, Koray and Farabet,
                  Cl{\'e}ment},
  booktitle =	 {BigLearn, NIPS Workshop},
  number =	 {EPFL-CONF-192376},
  year =	 2011
}

@article{Aliper2016,
  title =	 {Deep learning applications for predicting
                  pharmacological properties of drugs and drug
                  repurposing using transcriptomic data},
  author =	 {Aliper, Alexander and Plis, Sergey and Artemov,
                  Artem and Ulloa, Alvaro and Mamoshina, Polina and
                  Zhavoronkov, Alex},
  journal =	 {Molecular pharmaceutics},
  volume =	 13,
  number =	 7,
  pages =	 {2524--2530},
  year =	 2016,
  publisher =	 {ACS Publications}
}

@article{Codella2016,
  title =	 {Deep Learning Ensembles for Melanoma Recognition in
                  Dermoscopy Images},
  author =	 {Codella, Noel and Nguyen, Quoc-Bao and Pankanti,
                  Sharath and Gutman, David and Helba, Brian and
                  Halpern, Allan and Smith, John R},
  journal =	 {arXiv preprint arXiv:1610.04662},
  year =	 2016
}

@article{Baldi2014,
  title =	 {Searching for exotic particles in high-energy
                  physics with deep learning},
  author =	 {Baldi, Pierre and Sadowski, Peter and Whiteson,
                  Daniel},
  journal =	 {Nature communications},
  volume =	 5,
  year =	 2014,
  publisher =	 {Nature Publishing Group}
}

@article{Plis2013,
  title =	 {Deep learning for neuroimaging: a validation study},
  author =	 {Plis, Sergey M and Hjelm, Devon R and Salakhutdinov,
                  Ruslan and Calhoun, Vince D},
  journal =	 {arXiv preprint arXiv:1312.5847},
  year =	 2013
}

@inproceedings{Bergstra2010,
  title =	 {Theano: A {CPU} and {GPU} math compiler in {P}ython},
  author =	 {Bergstra, James and Breuleux, Olivier and Bastien,
                  Fr{\'e}d{\'e}ric and Lamblin, Pascal and Pascanu,
                  Razvan and Desjardins, Guillaume and Turian, Joseph
                  and Warde-Farley, David and Bengio, Yoshua},
  booktitle =	 {Proc. 9th Python in Science Conf},
  pages =	 {1--7},
  year =	 2010
}

@article{Yu2014,
  title =	 {An introduction to computational networks and the
                  computational network toolkit},
  author =	 {Yu, Dong and Eversole, Adam and Seltzer, Mike and
                  Yao, Kaisheng and Huang, Zhiheng and Guenter, Brian
                  and Kuchaiev, Oleksii and Zhang, Yu and Seide, Frank
                  and Wang, Huaming and Jasha Droppo and Geoffrey
                  Zweig and Chris Rossbach and Jon Currey and Jie Gao
                  and Avner May and Baolin Peng and Andreas Stolcke
                  and Malcolm Slaney},
  journal =	 {Microsoft Technical Report MSR-TR-2014--112},
  year =	 2014
}

@article{Chen2015,
  title =	 {{MXNet}: A flexible and efficient machine learning
                  library for heterogeneous distributed systems},
  author =	 {Chen, Tianqi and Li, Mu and Li, Yutian and Lin, Min
                  and Wang, Naiyan and Wang, Minjie and Xiao, Tianjun
                  and Xu, Bing and Zhang, Chiyuan and Zhang, Zheng},
  journal =	 {arXiv preprint arXiv:1512.01274},
  year =	 2015
}

@inproceedings{Nothaft2015,
  title =	 {Rethinking data-intensive science using scalable
                  analytics systems},
  author =	 {Nothaft, Frank Austin and Massie, Matt and Danford,
                  Timothy and Zhang, Zhao and Laserson, Uri and
                  Yeksigian, Carl and Kottalam, Jey and Ahuja, Arun
                  and Hammerbacher, Jeff and Linderman, Michael and
                  others},
  booktitle =	 {Proceedings of the 2015 ACM SIGMOD International
                  Conference on Management of Data},
  pages =	 {631--646},
  year =	 2015,
  organization = {ACM}
}

@article{Zhang2016,
  title =	 {Kira: Processing Astronomy Imagery Using Big Data
                  Technology},
  author =	 {Zhang, Zhao and Barbary, Kyle and Nothaft, Frank A
                  and Sparks, Evan R and Zahn, Oliver and Franklin,
                  Michael J and Patterson, David A and Perlmutter,
                  Saul},
  journal =	 {IEEE Transactions on Big Data},
  year =	 2016,
  publisher =	 {IEEE}
}

@inproceedings{Zhang2013,
  title =	 {Parallelizing the execution of sequential scripts},
  author =	 {Zhang, Zhao and Katz, Daniel S and Armstrong,
                  Timothy G and Wozniak, Justin M and Foster, Ian},
  booktitle =	 {Proceedings of the International Conference on High
                  Performance Computing, Networking, Storage and
                  Analysis},
  pages =	 31,
  year =	 2013,
  organization = {ACM}
}

@inproceedings{Zhang2012,
  title =	 {Design and analysis of data management in scalable
                  parallel scripting},
  author =	 {Zhang, Zhao and Katz, Daniel S and Wozniak, Justin M
                  and Espinosa, Allan and Foster, Ian},
  booktitle =	 {Proceedings of the International Conference on High
                  Performance Computing, Networking, Storage and
                  Analysis},
  pages =	 85,
  year =	 2012,
  organization = {IEEE Computer Society Press}
}

@article{Charnock2016,
  title =	 {Deep Recurrent Neural Networks for Supernovae
                  Classification},
  author =	 {Charnock, Tom and Moss, Adam},
  journal =	 {arXiv preprint arXiv:1606.07442},
  year =	 2016
}

@inproceedings{Chicco2014,
  title =	 {Deep autoencoder neural networks for gene ontology
                  annotation predictions},
  author =	 {Chicco, Davide and Sadowski, Peter and Baldi,
                  Pierre},
  booktitle =	 {Proceedings of the 5th ACM Conference on
                  Bioinformatics, Computational Biology, and Health
                  Informatics},
  pages =	 {533--540},
  year =	 2014,
  organization = {ACM}
}

@article{Choi2016,
  title =	 {Using recurrent neural network models for early
                  detection of heart failure onset},
  author =	 {Choi, Edward and Schuetz, Andy and Stewart, Walter F
                  and Sun, Jimeng},
  journal =	 {Journal of the American Medical Informatics
                  Association},
  pages =	 {ocw112},
  year =	 2016,
  publisher =	 {The Oxford University Press}
}

@inproceedings{Lena2012,
  title =	 {Deep spatio-temporal architectures and learning for
                  protein structure prediction},
  author =	 {Lena, Pietro D and Nagata, Ken and Baldi, Pierre F},
  booktitle =	 {Advances in neural information processing systems},
  pages =	 {512--520},
  year =	 2012
}

@article{Aulck2016,
  title =	 {Predicting Student Dropout in Higher Education},
  author =	 {Aulck, Lovenoor and Velagapudi, Nishant and
                  Blumenstock, Joshua and West, Jevin},
  journal =	 {arXiv preprint arXiv:1606.06364},
  year =	 2016
}

@article{Khan2016,
  title =	 {Machine Learning Across Cultures: Modeling the
                  Adoption of Financial Services for the Poor},
  author =	 {Khan, Muhammad Raza and Blumenstock, Joshua E},
  journal =	 {arXiv preprint arXiv:1606.05105},
  year =	 2016
}

@inproceedings{Coates2013,
  title =	 {Deep learning with COTS HPC systems},
  author =	 {Coates, Adam and Huval, Brody and Wang, Tao and Wu,
                  David and Catanzaro, Bryan and Andrew, Ng},
  booktitle =	 {Proceedings of The 30th International Conference on
                  Machine Learning},
  pages =	 {1337--1345},
  year =	 2013
}

@inproceedings{Dean2012,
  title =	 {Large scale distributed deep networks},
  author =	 {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and
                  Chen, Kai and Devin, Matthieu and Mao, Mark and
                  Senior, Andrew and Tucker, Paul and Yang, Ke and Le,
                  Quoc V and others},
  booktitle =	 {Advances in neural information processing systems},
  pages =	 {1223--1231},
  year =	 2012
}

@inproceedings{Cui2016,
  title =	 {{GeePS}: Scalable deep learning on distributed gpus
                  with a gpu-specialized parameter server},
  author =	 {Cui, Henggang and Zhang, Hao and Ganger, Gregory R
                  and Gibbons, Phillip B and Xing, Eric P},
  booktitle =	 {Proceedings of the Eleventh European Conference on
                  Computer Systems},
  pages =	 4,
  year =	 2016,
  organization = {ACM}
}

@article{Zhang2015,
  title =	 {Poseidon: A system architecture for efficient
                  {GPU-based} deep learning on multiple machines},
  author =	 {Zhang, Hao and Hu, Zhiting and Wei, Jinliang and
                  Xie, Pengtao and Kim, Gunhee and Ho, Qirong and
                  Xing, Eric},
  journal =	 {arXiv preprint arXiv:1512.06216},
  year =	 2015
}

@Misc{Gropp2017,
  author =	 {{NCSA}},
  title =	 {{Performance Analysis of Large Scale Deep Learning
                  Systems}},
  note =
                  {\url{https://bluewaters.ncsa.illinois.edu/science-teams?page=detail&psn=baie}},
  year =	 2017
}

@Misc{Sirignano2017,
  author =	 {{NCSA}},
  title =	 "{Distributed Learning with Neural Networks}",
  note =
                  "\url{https://bluewaters.ncsa.illinois.edu/science-teams?page=detail&psn=bahp}",
  year =	 2017
}

@Misc{Brunner2017,
  author =	 {{NCSA}},
  title =	 {{Applying Deep Learning on Time Series Astronomical
                  Data}},
  note =
                  {\url{https://bluewaters.ncsa.illinois.edu/science-teams?page=detail&psn=bacy}},
  year =	 2017
}

@Misc{Peng2017,
  author =	 {{NCSA}},
  title =	 {{Protein structure prediction using deep neural
                  networks}},
  note =
                  {\url{https://bluewaters.ncsa.illinois.edu/science-teams?page=detail&psn=baiq}},
  year =	 2017
}

@Misc{LLNL2015,
  author =	 {{LLNL}},
  title =	 {{The Livermore Brain: Massive Deep-Learning Networks
                  Enabled by High-Performance Computing}},
  note =
                  {\url{https://ldrd-annual.llnl.gov/ldrd-annual-2015/computing/brain}},
  year =	 2015
}

@Misc{Kincade2015,
  author =	 {{LBNL}},
  title =	 {{Berkeley Lab Explores Frontiers of Deep Learning
                  for Science}},
  note =
                  {\url{https://cs.lbl.gov/news-media/news/2015/berkeley-lab-explores-frontiers-of-deep-learning-for-science/}},
  year =	 2015
}

@Misc{Russel2016,
  author =	 {{Russel, John}},
  title =	 {{Enlisting Deep Learning in the War on Cancer}},
  note =
                  {\url{https://www.hpcwire.com/2016/12/07/enlisting-deep-learning-war-cancer/}},
  year =	 2016
}

@Misc{PNNL2016,
  author =	 {{PNNL}},
  title =	 {{New Deep Learning Project Launches at PNNL}},
  note =
                  {\url{http://www.pnnl.gov/science/highlights/highlight.asp?id=4443}},
  year =	 2016
}

@article{Bahrampour2015,
  title =	 {Comparative study of caffe, neon, theano, and torch
                  for deep learning},
  author =	 {Bahrampour, Soheil and Ramakrishnan, Naveen and
                  Schott, Lukas and Shah, Mohak},
  journal =	 {arXiv preprint arXiv:1511.06435},
  year =	 2015
}

@article{Simmhan05,
  title =	 {A survey of data provenance in e-science},
  author =	 {Simmhan, Yogesh L and Plale, Beth and Gannon,
                  Dennis},
  journal =	 {ACM Sigmod Record},
  volume =	 34,
  number =	 3,
  pages =	 {31--36},
  year =	 2005,
  publisher =	 {ACM}
}

@misc{NSCI,
  author =	 {{The National Strategic Computing Initiative
                  Executive Council}},
  title =	 {{National Strategic Computing Initiative Strategic
                  Plan}},
  year =	 2016,
  note =
                  {\url{https://www.whitehouse.gov/sites/whitehouse.gov/files/images/NSCI\%20Strategic\%20Plan.pdf}}
}

@inproceedings{sutskever2014sequence,
  title =	 {Sequence to sequence learning with neural networks},
  author =	 {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle =	 {Advances in Neural Information Processing Systems},
  pages =	 {3104--3112},
  year =	 2014
}

@article{ching2017opportunities,
  title =	 {Opportunities And Obstacles For Deep Learning In
                  Biology And Medicine},
  author =	 {Ching, Travers and Himmelstein, Daniel S and
                  Beaulieu-Jones, Brett K and Kalinin, Alexandr A and
                  Do, Brian T and Way, Gregory P and Ferrero, Enrico
                  and Agapow, Paul-Michael and Xie, Wei and Rosen,
                  Gail L and Benjamin J. Lengerich and Johnny Israeli
                  and Jack Lanchantin and Stephen Woloszynek and Anne
                  E. Carpenter and Avanti Shrikumar and Jinbo Xu and
                  Evan M. Cofer and David J. Harris and Dave DeCaprio
                  and Yanjun Qi and Anshul Kundaje and Yifan Peng and
                  Laura K. Wiley and Marwin H.S.  Segler and Anthony
                  Gitter and Casey S. Greene},
  journal =	 {bioRxiv},
  pages =	 142760,
  year =	 2017,
  publisher =	 {Cold Spring Harbor Labs Journals}
}

@inproceedings{balaprakash2013active,
  title =	 {Active-learning-based surrogate models for empirical
                  performance tuning},
  author =	 {Balaprakash, Prasanna and Gramacy, Robert B and
                  Wild, Stefan M},
  booktitle =	 {IEEE International Conference on Cluster Computing},
  pages =	 {1--8},
  year =	 2013,
  organization = {IEEE}
}

@inproceedings{chard2017globus,
  title =	 {Globus: Research data management as service and
                  platform},
  author =	 {Chard, Kyle and Foster, Ian and Tuecke, Steven},
  booktitle =	 {Proceedings of the Practice and Experience in
                  Advanced Research Computing 2017 on Sustainability,
                  Success and Impact},
  pages =	 26,
  year =	 2017,
  organization = {ACM}
}

@article{chard2017modern,
  title =	 {The Modern Research Data Portal: A design pattern
                  for networked, data-intensive science},
  author =	 {Chard, Kyle and Dart, Eli and Foster, Ian and
                  Shifflett, David and Tuecke, Steven and Williams,
                  Jason},
  journal =	 {PeerJ Preprints},
  year =	 2017,
  publisher =	 {PeerJ, Inc.}
}

@book{foster2017cloud,
  title =	 {Cloud Computing for Science and Engineering},
  author =	 {Foster, Ian and Gannon, Dennis B},
  year =	 2017,
  publisher =	 {MIT Press}
}

@article{foster2018research,
  title =	 {Research infrastructure for the safe analysis of
                  sensitive data},
  author =	 {Foster, Ian},
  journal =	 {The ANNALS of the American Academy of Political and
                  Social Science},
  volume =	 675,
  number =	 1,
  pages =	 {102--120},
  year =	 2018,
  publisher =	 {SAGE Publications Sage CA: Los Angeles, CA}
}

@inproceedings{ogilvie2014fast,
  title =	 {Fast automatic heuristic construction using active
                  learning},
  author =	 {Ogilvie, William F and Petoumenos, Pavlos and Wang,
                  Zheng and Leather, Hugh},
  booktitle =	 {International Workshop on Languages and Compilers
                  for Parallel Computing},
  pages =	 {146--160},
  year =	 2014,
  organization = {Springer}
}

@inproceedings{Foster02,
  title =	 {Chimera: A virtual data system for representing,
                  querying, and automating data derivation},
  author =	 {Foster, Ian and V{\"o}ckler, Jens and Wilde, Michael
                  and Zhao, Yong},
  booktitle =	 {Scientific and Statistical Database Management,
                  2002. Proceedings. 14th International Conference on},
  pages =	 {37--46},
  year =	 2002,
  organization = {IEEE}
}

@article{Oinn02,
  title =	 {Taverna: lessons in creating a workflow environment
                  for the life sciences},
  author =	 {Oinn, Tom and Greenwood, Mark and Addis, Matthew J
                  and Alpdemir, M Nedim and Ferris, Justin and Glover,
                  Kevin and Goble, Carole and Goderis, Antoon and
                  Hull, Duncan and Marvin, DJ and others},
  journal =	 {Journal of Concurrency and Computation: Practice and
                  experience},
  year =	 2002,
  publisher =	 {John Wiley \& Sons Ltd}
}

@inproceedings{Frew01,
  title =	 {Earth system science workbench: A data management
                  infrastructure for earth science products},
  author =	 {Frew, James and Bose, Rajendra},
  booktitle =	 {Scientific and Statistical Database Management,
                  2001. SSDBM 2001. Proceedings. Thirteenth
                  International Conference on},
  pages =	 {180--189},
  year =	 2001,
  organization = {IEEE}
}

@incollection{Altintas2010,
  title =	 {Understanding collaborative studies through
                  interoperable workflow provenance},
  author =	 {Altintas, Ilkay and Anand, Manish Kumar and Crawl,
                  Daniel and Bowers, Shawn and Belloum, Adam and
                  Missier, Paolo and Lud{\"a}scher, Bertram and Goble,
                  Carole A and Sloot, Peter MA},
  booktitle =	 {Provenance and Annotation of Data and Processes},
  pages =	 {42--58},
  year =	 2010,
  publisher =	 {Springer}
}

@incollection{Altintas2006,
  title =	 {Provenance collection support in the kepler
                  scientific workflow system},
  author =	 {Altintas, Ilkay and Barney, Oscar and Jaeger-Frank,
                  Efrat},
  booktitle =	 {Provenance and annotation of data},
  pages =	 {118--132},
  year =	 2006,
  publisher =	 {Springer}
}

@article{Simmhan2010,
  title =	 {Karma2: Provenance management for data-driven
                  workflows},
  author =	 {Simmhan, Yogesh L and Plale, Beth and Gannon,
                  Dennis},
  journal =	 {Web Services Research for Emerging Applications:
                  Discoveries and Trends: Discoveries and Trends},
  pages =	 317,
  year =	 2010,
  publisher =	 {IGI Global}
}

@article{Sroka2010,
  title =	 {A formal semantics for the Taverna 2 workflow model},
  author =	 {Sroka, Jacek and Hidders, Jan and Missier, Paolo and
                  Goble, Carole},
  journal =	 {Journal of Computer and System Sciences},
  volume =	 76,
  number =	 6,
  pages =	 {490--508},
  year =	 2010,
  publisher =	 {Elsevier}
}

@inproceedings{Missier2010,
  title =	 {Fine-grained and efficient lineage querying of
                  collection-based workflow provenance},
  author =	 {Missier, Paolo and Paton, Norman W and Belhajjame,
                  Khalid},
  booktitle =	 {Proceedings of the 13th International Conference on
                  Extending Database Technology},
  pages =	 {299--310},
  year =	 2010,
  organization = {ACM}
}

@article{Bowers2008,
  title =	 {Provenance in collection-oriented scientific
                  workflows},
  author =	 {Bowers, Shawn and McPhillips, Timothy M and
                  Lud{\"a}scher, Bertram},
  journal =	 {Concurrency and Computation: Practice and
                  Experience},
  volume =	 20,
  number =	 5,
  pages =	 {519--529},
  year =	 2008,
  publisher =	 {Wiley Online Library}
}

@article{bose05,
  title =	 {Lineage retrieval for scientific data processing: a
                  survey},
  author =	 {Bose, Rajendra and Frew, James},
  journal =	 {ACM Computing Surveys (CSUR)},
  volume =	 37,
  number =	 1,
  pages =	 {1--28},
  year =	 2005,
  publisher =	 {ACM}
}

@article{freire08,
  title =	 {Provenance for computational tasks: A survey},
  author =	 {Freire, Juliana and Koop, David and Santos, Emanuele
                  and Silva, Cl{\'a}udio T},
  journal =	 {Computing in Science \& Engineering},
  volume =	 10,
  number =	 3,
  pages =	 {11--21},
  year =	 2008,
  publisher =	 {AIP Publishing}
}

@Misc{Stodden2014,
  author =	 {{Victoria Stodden}},
  title =	 "{2014 : WHAT SCIENTIFIC IDEA IS READY FOR
                  RETIREMENT?}",
  note =	 "\url{https://www.edge.org/response-detail/25340}",
  year =	 2014
}

@misc{Torvalds2010,
  title =	 {Git: Fast version control system},
  author =	 {Torvalds, Linus and Hamano, Junio},
  note =	 "\url{http://git-scm.com}",
  year =	 2010
}

@article{Collins2002,
  title =	 {Version control with subversion},
  author =	 {Collins-Sussman, Ben and Fitzpatrick, Brian W and
                  Pilato, C Michael},
  journal =	 {Version Control with Subversion},
  year =	 2002
}

@inproceedings{Callahan2006,
  title =	 {Managing the evolution of dataflows with VisTrails},
  author =	 {Callahan, Steven P and Freire, Juliana and Santos,
                  Emanuele and Scheidegger, Carlos Eduardo and Silva,
                  Claudio T and Vo, Huy T},
  booktitle =	 {Data Engineering Workshops, 2006. Proceedings. 22nd
                  International Conference on},
  pages =	 {71--71},
  year =	 2006,
  organization = {IEEE}
}

@Misc{NSCI2016,
  author =	 {{The National Strategic Computing Initiative
                  Executive Council}},
  title =	 "{National Strategic Computing Initiative Strategic
                  Plan}",
  note =
                  "\url{https://www.whitehouse.gov/sites/whitehouse.gov/files/images/NSCI%20Strategic%20Plan.pdf}",
  year =	 2016
}

@misc{Deepbench2017,
  title =	 {DeepBench: Benchmarking Deep Learning operations on
                  different hardware},
  author =	 {{Baidu Research}},
  note =	 "\url{https://github.com/baidu-research/DeepBench}",
  year =	 2017
}

@misc{Narang2017-deepBench,
  title =	 {{An update to DeepBench with a focus on deep
                  learning inference}},
  author =	 {Narang, Sharan and Diamos, Greg},
  year =	 2017,
  howpublished = {\url{https://svail.github.io/DeepBench-update/}},
}

@misc{MLSL,
  title =	 {Intel(R) Machine Learning Scaling Library},
  author =	 {{Intel}},
  note =	 "\url{https://github.com/intel/MLSL}",
  year =	 2019
}

@inproceedings{you2018imagenet,
  title =	 {Imagenet training in minutes},
  author =	 {You, Yang and Zhang, Zhao and Hsieh, Cho-Jui and
                  Demmel, James and Keutzer, Kurt},
  booktitle =	 {{Proceedings of the 47th International Conference on
                  Parallel Processing}},
  pages =	 1,
  year =	 2018,
  organization = {ACM}
}

@article{Codreanu2017,
  title =	 {Scale out for large minibatch {SGD}: Residual
                  network training on {ImageNet-1K} with improved
                  accuracy and reduced time to train},
  author =	 {Codreanu, Valeriu and Podareanu, Damian and
                  Saletore, Vikram},
  journal =	 {arXiv preprint arXiv:1711.04291},
  year =	 2017
}

@article{Akiba2017,
  title =	 {{Extremely Large Minibatch SGD: Training ResNet-50
                  on ImageNet in 15 Minutes}},
  author =	 {Akiba, Takuya and Suzuki, Shuji and Fukuda, Keisuke},
  journal =	 {arXiv preprint arXiv:1711.04325},
  year =	 2017
}

@article{Shi2017,
  title =	 {Performance Modeling and Evaluation of Distributed
                  Deep Learning Frameworks on {GPUs}},
  author =	 {Shi, Shaohuai and Chu, Xiaowen},
  journal =	 {arXiv preprint arXiv:1711.05979},
  year =	 2017
}

@inproceedings{Verma2011,
  title =	 {ARIA: automatic resource inference and allocation
                  for {MapReduce} environments},
  author =	 {Verma, Abhishek and Cherkasova, Ludmila and
                  Campbell, Roy H},
  booktitle =	 {Proceedings of the 8th ACM international conference
                  on Autonomic computing},
  pages =	 {235--244},
  year =	 2011,
  organization = {ACM}
}

@article{Zheng2005,
  title =	 {Simulation-based performance prediction for large
                  parallel machines},
  author =	 {Zheng, Gengbin and Wilmarth, Terry and
                  Jagadishprasad, Praveen and Kal{\'e}, Laxmikant V},
  journal =	 {International Journal of Parallel Programming},
  volume =	 33,
  number =	 2,
  pages =	 {183--207},
  year =	 2005,
  publisher =	 {Springer}
}

@inproceedings{Balasundaram1991,
  title =	 {A static performance estimator to guide data
                  partitioning decisions},
  author =	 {Balasundaram, Vasanth and Fox, Geoffrey and Kennedy,
                  Ken and Kremer, Ulrich},
  booktitle =	 {ACM Sigplan Notices},
  volume =	 26,
  number =	 7,
  pages =	 {213--223},
  year =	 1991,
  organization = {ACM}
}

@inproceedings{Venkataraman2016,
  title =	 {Ernest: Efficient Performance Prediction for
                  Large-Scale Advanced Analytics.},
  author =	 {Venkataraman, Shivaram and Yang, Zongheng and
                  Franklin, Michael J and Recht, Benjamin and Stoica,
                  Ion},
  booktitle =	 {NSDI},
  pages =	 {363--378},
  year =	 2016
}

@inproceedings{Yadwadkar2014,
  title =	 {Wrangler: Predictable and faster jobs using fewer
                  resources},
  author =	 {Yadwadkar, Neeraja J and Ananthanarayanan, Ganesh
                  and Katz, Randy},
  booktitle =	 {Proceedings of the ACM Symposium on Cloud Computing},
  pages =	 {1--14},
  year =	 2014,
  organization = {ACM}
}

@inproceedings{Ferguson2012,
  title =	 {Jockey: guaranteed job latency in data parallel
                  clusters},
  author =	 {Ferguson, Andrew D and Bodik, Peter and Kandula,
                  Srikanth and Boutin, Eric and Fonseca, Rodrigo},
  booktitle =	 {Proceedings of the 7th ACM european conference on
                  Computer Systems},
  pages =	 {99--112},
  year =	 2012,
  organization = {ACM}
}

@article{Buyya2002,
  title =	 {Gridsim: A toolkit for the modeling and simulation
                  of distributed resource management and scheduling
                  for grid computing},
  author =	 {Buyya, Rajkumar and Murshed, Manzur},
  journal =	 {Concurrency and computation: practice and
                  experience},
  volume =	 14,
  number =	 {13-15},
  pages =	 {1175--1220},
  year =	 2002,
  publisher =	 {Wiley Online Library}
}

@inproceedings{Kurth2017,
  author =	 {Kurth, Thorsten and Zhang, Jian and Satish, Nadathur
                  and Racah, Evan and Mitliagkas, Ioannis and Patwary,
                  Md. Mostofa Ali and Malas, Tareq and Sundaram,
                  Narayanan and Bhimji, Wahid and Smorkalov, Mikhail
                  and Deslippe, Jack and Shiryaev, Mikhail and
                  Sridharan, Srinivas and Prabhat and Dubey, Pradeep},
  title =	 {Deep Learning at 15PF: Supervised and
                  Semi-supervised Classification for Scientific Data},
  booktitle =	 {Proceedings of the International Conference for High
                  Performance Computing, Networking, Storage and
                  Analysis},
  series =	 {SC '17},
  year =	 2017,
  isbn =	 {978-1-4503-5114-0},
  location =	 {Denver, Colorado},
  pages =	 {7:1--7:11},
  articleno =	 7,
  numpages =	 11,
  url =		 {http://doi.acm.org/10.1145/3126908.3126916},
  doi =		 {10.1145/3126908.3126916},
  acmid =	 3126916,
  publisher =	 {ACM},
  address =	 {New York, NY, USA},
}

@article{Katz2016,
  title =	 {Application skeletons: Construction and use in
                  eScience},
  author =	 {Katz, Daniel S and Merzky, Andre and Zhang, Zhao and
                  Jha, Shantenu},
  journal =	 {Future Generation Computer Systems},
  volume =	 59,
  pages =	 {114--124},
  year =	 2016,
  publisher =	 {Elsevier}
}

@article{Carrasquilla2017,
  title =	 {Machine learning phases of matter},
  author =	 {Carrasquilla, Juan and Melko, Roger G},
  journal =	 {Nature Physics},
  year =	 2017,
  publisher =	 {Nature Research}
}

@inproceedings{Deng2009,
  title =	 {Imagenet: A large-scale hierarchical image database},
  author =	 {Deng, Jia and Dong, Wei and Socher, Richard and Li,
                  Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle =	 {Computer Vision and Pattern Recognition, 2009. CVPR
                  2009. IEEE Conference on},
  pages =	 {248--255},
  year =	 2009,
  organization = {IEEE}
}

@article{Thakur2005,
  title =	 {Optimization of collective communication operations
                  in {MPICH}},
  author =	 {Thakur, Rajeev and Rabenseifner, Rolf and Gropp,
                  William},
  journal =	 {The International Journal of High Performance
                  Computing Applications},
  volume =	 19,
  number =	 1,
  pages =	 {49--66},
  year =	 2005,
  publisher =	 {Sage Publications Sage CA: Thousand Oaks, CA}
}

@article{Chan2007,
  title =	 {Collective communication: theory, practice, and
                  experience},
  author =	 {Chan, Ernie and Heimlich, Marcel and Purkayastha,
                  Avi and Van De Geijn, Robert},
  journal =	 {Concurrency and Computation: Practice and
                  Experience},
  volume =	 19,
  number =	 13,
  pages =	 {1749--1783},
  year =	 2007,
  publisher =	 {Wiley Online Library}
}

@inproceedings{Zhang2017,
  author =	 {Zhang, Zhao and Sparks, Evan R. and Franklin,
                  Michael J.},
  title =	 {Diagnosing Machine Learning Pipelines with
                  Fine-grained Lineage},
  booktitle =	 {Proceedings of the 26th International Symposium on
                  High-Performance Parallel and Distributed Computing},
  series =	 {HPDC '17},
  year =	 2017,
  isbn =	 {978-1-4503-4699-3},
  location =	 {Washington, DC, USA},
  pages =	 {143--153},
  numpages =	 11,
  url =		 {http://doi.acm.org/10.1145/3078597.3078603},
  doi =		 {10.1145/3078597.3078603},
  acmid =	 3078603,
  publisher =	 {ACM},
  address =	 {New York, NY, USA},
  keywords =	 {diagnostics, fine-grained lineage, machine learning}
}

@inproceedings{Zhang2017-2,
  author =	 {Zhang, Zhao and Xu, Weijia and Gaffney, Niall and
                  Stanzione, Daniel},
  title =	 {Early Results of Deep Learning on the {Stampede2}
                  Supercomputer},
  booktitle =	 {The Intel Xeon Phi User's Group (IXPUG) US Annual
                  Meeting},
  doi =		 {10.13140/RG.2.2.36806.78404},
  year =	 2017
}

@article{Dongarra1992,
  title =	 {Performance of various computers using standard
                  linear equations software},
  author =	 {Dongarra, Jack J},
  journal =	 {ACM SIGARCH Computer Architecture News},
  volume =	 20,
  number =	 3,
  pages =	 {22--44},
  year =	 1992,
  publisher =	 {ACM}
}

@Misc{Lambdalabs2018,
  author =	 {{Lambda Labs}},
  title =	 "{Titan RTX TensorFlow Benchmarks}",
  note =
                  "\url{https://lambdalabs.com/blog/titan-rtx-tensorflow-benchmarks/}",
  year =	 2018
}

@article{istrate2018tapas,
  title =	 {TAPAS: Train-less Accuracy Predictor for
                  Architecture Search},
  author =	 {Istrate, Roxana and Scheidegger, Florian and
                  Mariani, Giovanni and Nikolopoulos, D and Bekas,
                  Costas and Malossi, A Cristiano I},
  journal =	 {arXiv preprint arXiv:1806.00250},
  year =	 2018
}

@inproceedings{gupta2015deep,
  title =	 {Deep learning with limited numerical precision},
  author =	 {Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan,
                  Kailash and Narayanan, Pritish},
  booktitle =	 {International Conference on Machine Learning},
  pages =	 {1737--1746},
  year =	 2015
}

@article{ottman1996gene,
  title =	 {Gene--environment interaction: definitions and study
                  design},
  author =	 {Ottman, Ruth},
  journal =	 {Preventive medicine},
  volume =	 25,
  number =	 6,
  pages =	 {764--770},
  year =	 1996,
  publisher =	 {Elsevier}
}

@article{chard2018dlhub,
  title =	 {DLHub: Model and Data Serving for Science},
  author =	 {Chard, Ryan and Li, Zhuozhao and Chard, Kyle and
                  Ward, Logan and Babuji, Yadu and Woodard, Anna and
                  Tuecke, Steve and Blaiszik, Ben and Franklin,
                  Michael J and Foster, Ian},
  journal =	 {arXiv preprint arXiv:1811.11213},
  year =	 2018
}

@inproceedings{lee2001algorithms,
  title =	 {Algorithms for non-negative matrix factorization},
  author =	 {Lee, Daniel D and Seung, H Sebastian},
  booktitle =	 {Advances in neural information processing systems},
  pages =	 {556--562},
  year =	 2001
}

@inproceedings{jain2013low,
  title =	 {Low-rank matrix completion using alternating
                  minimization},
  author =	 {Jain, Prateek and Netrapalli, Praneeth and Sanghavi,
                  Sujay},
  booktitle =	 {Proceedings of the forty-fifth annual ACM symposium
                  on Theory of computing},
  pages =	 {665--674},
  year =	 2013,
  organization = {ACM}
}

@inproceedings{krizhevsky2012imagenet,
  title =	 {Imagenet classification with deep convolutional
                  neural networks},
  author =	 {Krizhevsky, Alex and Sutskever, Ilya and Hinton,
                  Geoffrey E},
  booktitle =	 {Advances in neural information processing systems},
  pages =	 {1097--1105},
  year =	 2012
}

@inproceedings{he2016deep,
  title =	 {Deep residual learning for image recognition},
  author =	 {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and
                  Sun, Jian},
  booktitle =	 {Proceedings of the IEEE conference on computer
                  vision and pattern recognition},
  pages =	 {770--778},
  year =	 2016
}

@inproceedings{huang2017densely,
  title =	 {Densely connected convolutional networks.},
  author =	 {Huang, Gao and Liu, Zhuang and Van Der Maaten,
                  Laurens and Weinberger, Kilian Q},
  booktitle =	 {CVPR},
  volume =	 1,
  number =	 2,
  pages =	 3,
  year =	 2017
}

@article{sergeev2018horovod,
  Author =	 {Alexander Sergeev and Mike Del Balso},
  Journal =	 {arXiv preprint arXiv:1802.05799},
  Title =	 {Horovod: Fast and easy distributed deep learning in
                  {TensorFlow}},
  Year =	 2018
}

@article{li2022amp,
  title =	 {AMP: Automatically Finding Model Parallel Strategies
                  with Heterogeneity Awareness},
  author =	 {Li, Dacheng and Wang, Hongyi and Xing, Eric and
                  Zhang, Hao},
  journal =	 {arXiv preprint arXiv:2210.07297},
  year =	 2022
}

@article{zheng2022alpa,
  title =	 {Alpa: Automating Inter-and Intra-Operator
                  Parallelism for Distributed Deep Learning},
  author =	 {Zheng, Lianmin and Li, Zhuohan and Zhang, Hao and
                  Zhuang, Yonghao and Chen, Zhifeng and Huang, Yanping
                  and Wang, Yida and Xu, Yuanzhong and Zhuo, Danyang
                  and Gonzalez, Joseph E and others},
  journal =	 {arXiv preprint arXiv:2201.12023},
  year =	 2022
}

@inproceedings{venkataraman2016ernest,
  title =	 {Ernest: efficient performance prediction for
                  large-scale advanced analytics},
  author =	 {Venkataraman, Shivaram and Yang, Zongheng and
                  Franklin, Michael and Recht, Benjamin and Stoica,
                  Ion},
  booktitle =	 {Proceedings of the 13th Usenix Conference on
                  Networked Systems Design and Implementation},
  pages =	 {363--378},
  year =	 2016,
  organization = {USENIX Association}
}

@inproceedings{sinha2022notall,
  title =	 {{Not All GPUs Are Created Equal: Characterizing
                  Variability in Large-Scale, Accelerator-Rich
                  Systems}},
  author =	 {Sinha, Prasoon and Guliani, Akhil and Jain, Rutwik
                  and Tran, Brandon and Sinclair, Matthew D and
                  Venkataraman, Shivaram},
  booktitle =	 {{Proceedings of the International Conference on High
                  Performance Computing, Networking, Storage and
                  Analysis}},
  pages =	 {1--15},
  year =	 2022,
  series =	 {SC},
}

@inproceedings{gonzalez2012powergraph,
  title =	 {{PowerGraph: Distributed Graph-parallel Computation
                  on Natural Graphs}},
  author =	 {Gonzalez, J and Low, Yucheng and Gu, Haijie and
                  Bickson, Danny and Guestrin, Carlos},
  booktitle =	 {{Proceedings of the 10th USENIX Conference on
                  Operating Systems Design and Implementation}},
  series =	 {OSDI},
  year =	 2012
}



@misc{pypi,
  author =	 {{Python Software Foundation}},
  title =	 {{PyPI}: Python package index},
  note =	 {\url{https://pypi.org}},
  year =	 2021,
}

@misc{sciml,
  author =	 {SciML},
  title =	 {{SciML}: Scientific machine learning benchmark
                  suite},
  note =
                  {\url{https://github.com/stfc-sciml/sciml-benchmarks/}}
}

@article{lin2017deep,
  title =	 {Deep gradient compression: Reducing the
                  communication bandwidth for distributed training},
  author =	 {Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu
                  and Dally, William J},
  journal =	 {arXiv preprint arXiv:1712.01887},
  year =	 2017
}

@article{ginsburg2018large,
  title =	 {Large Batch Training of Convolutional Networks with
                  Layer-wise Adaptive Rate Scaling},
  author =	 {Ginsburg, Boris and Gitman, Igor and You, Yang},
  year =	 2018
}

@article{smith2017don,
  title =	 {Don't decay the learning rate, increase the batch
                  size},
  author =	 {Smith, Samuel L and Kindermans, Pieter-Jan and Ying,
                  Chris and Le, Quoc V},
  journal =	 {arXiv preprint arXiv:1711.00489},
  year =	 2017
}

@article{krizhevsky2014one,
  title =	 {One weird trick for parallelizing convolutional
                  neural networks},
  author =	 {Krizhevsky, Alex},
  journal =	 {arXiv preprint arXiv:1404.5997},
  year =	 2014
}

@article{bottou2018optimization,
  title =	 {Optimization methods for large-scale machine
                  learning},
  author =	 {Bottou, L{\'e}on and Curtis, Frank E and Nocedal,
                  Jorge},
  journal =	 {SIAM Review},
  volume =	 60,
  number =	 2,
  pages =	 {223--311},
  year =	 2018,
  publisher =	 {SIAM}
}

@article{goyal2017accurate,
  title =	 {Accurate, large minibatch SGD: training imagenet in
                  1 hour},
  author =	 {Goyal, Priya and Doll{\'a}r, Piotr and Girshick,
                  Ross and Noordhuis, Pieter and Wesolowski, Lukasz
                  and Kyrola, Aapo and Tulloch, Andrew and Jia,
                  Yangqing and He, Kaiming},
  journal =	 {arXiv preprint arXiv:1706.02677},
  year =	 2017
}

@article{zhang2018fanstore,
  title =	 {{FanStore}: Enabling Efficient and Scalable I/O for
                  Distributed Deep Learning},
  author =	 {Zhang, Zhao and Huang, Lei and Manor, Uri and Fang,
                  Linjing and Merlo, Gabriele and Michoski, Craig and
                  Cazes, John and Gaffney, Niall},
  journal =	 {arXiv preprint arXiv:1809.10799},
  year =	 2018
}

@misc{Deep500,
  title =	 {{Deep500}},
  note =	 {\url{https://www.deep500.org/}}
}

@misc{ONNX,
  title =	 {{ONNX}},
  note =	 {\url{https://onnx.ai/}}
}

@inproceedings{Kannan2016HPA,
  author =	 {Kannan, Ramakrishnan and Ballard, Grey and Park,
                  Haesun},
  title =	 {A High-performance Parallel Algorithm for
                  Nonnegative Matrix Factorization},
  booktitle =	 {Proceedings of the 21st ACM SIGPLAN Symposium on
                  Principles and Practice of Parallel Programming},
  series =	 {PPoPP '16},
  year =	 2016,
  isbn =	 {978-1-4503-4092-2},
  location =	 {Barcelona, Spain},
  pages =	 {9:1--9:11},
  articleno =	 9,
  numpages =	 11,
  url =		 {http://doi.acm.org/10.1145/2851141.2851152},
  doi =		 {10.1145/2851141.2851152},
  acmid =	 2851152,
  publisher =	 {ACM},
  address =	 {New York, NY, USA},
}

@inproceedings{zhang2013mtc,
  title =	 {MTC Envelope: Defining the capability of large scale
                  computers in the context of parallel scripting
                  applications},
  author =	 {Zhang, Zhao and Katz, Daniel S and Wilde, Michael
                  and Wozniak, Justin M and Foster, Ian},
  booktitle =	 {Proceedings of the 22nd international symposium on
                  High-performance parallel and distributed computing},
  pages =	 {37--48},
  year =	 2013,
  organization = {ACM}
}

@inproceedings{you2019large,
  author =	 {You, Yang and Hseu, Jonathan and Ying, Chris and
                  Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  title =	 {Large-Batch Training for LSTM and Beyond},
  year =	 2019,
  isbn =	 9781450362290,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/3295500.3356137},
  doi =		 {10.1145/3295500.3356137},
  booktitle =	 {Proceedings of the International Conference for High
                  Performance Computing, Networking, Storage and
                  Analysis},
  articleno =	 {Article 9},
  numpages =	 16,
  keywords =	 {neural networks, distributed computing, large-batch
                  training},
  location =	 {Denver, Colorado},
  series =	 {SC ’19}
}

@incollection{NEURIPS2019_9015,
  title =	 {PyTorch: An Imperative Style, High-Performance Deep
                  Learning Library},
  author =	 {Paszke, Adam and Gross, Sam and Massa, Francisco and
                  Lerer, Adam and Bradbury, James and Chanan, Gregory
                  and Killeen, Trevor and Lin, Zeming and Gimelshein,
                  Natalia and Antiga, Luca and Desmaison, Alban and
                  Kopf, Andreas and Yang, Edward and DeVito, Zachary
                  and Raison, Martin and Tejani, Alykhan and
                  Chilamkurthy, Sasank and Steiner, Benoit and Fang,
                  Lu and Bai, Junjie and Chintala, Soumith},
  booktitle =	 {Advances in Neural Information Processing Systems
                  32},
  pages =	 {8024--8035},
  year =	 2019,
  publisher =	 {Curran Associates, Inc.},
  url =
                  {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@inproceedings{martens2015optimizing,
  title =	 {Optimizing neural networks with kronecker-factored
                  approximate curvature},
  author =	 {Martens, James and Grosse, Roger},
  booktitle =	 {International conference on machine learning},
  pages =	 {2408--2417},
  year =	 2015
}

@InProceedings{Osawa_2019_CVPR,
  author =	 {Osawa, Kazuki and Tsuji, Yohei and Ueno, Yuichiro
                  and Naruse, Akira and Yokota, Rio and Matsuoka,
                  Satoshi},
  title =	 {Large-Scale Distributed Second-Order Optimization
                  Using Kronecker-Factored Approximate Curvature for
                  Deep Convolutional Neural Networks},
  booktitle =	 {The IEEE Conference on Computer Vision and Pattern
                  Recognition (CVPR)},
  month =	 {June},
  year =	 2019
}

@misc{ma2019inefficiency,
  title =	 {Inefficiency of K-FAC for Large Batch Size Training},
  author =	 {Linjian Ma and Gabe Montague and Jiayu Ye and Zhewei
                  Yao and Amir Gholami and Kurt Keutzer and Michael
                  W. Mahoney},
  year =	 2019,
  eprint =	 {1903.06237},
  archivePrefix ={arXiv},
  primaryClass = {cs.LG}
}

@inproceedings{zhang2020compress,
  title =	 {Efficient I/O for Neural Network Training with
                  Compressed Data},
  author =	 {Zhang, Zhao and Huang, Lei and Pauloski, J. Gregory
                  and Foster, Ian T.},
  booktitle =	 {34th International Symposium on Parallel and
                  Distributed Processing},
  year =	 2020,
  organization = {IEEE}
}

@inproceedings{recht2011hogwild,
  title =	 {Hogwild: A lock-free approach to parallelizing
                  stochastic gradient descent},
  author =	 {Recht, Benjamin and Re, Christopher and Wright,
                  Stephen and Niu, Feng},
  booktitle =	 {Advances in neural information processing systems},
  pages =	 {693--701},
  year =	 2011
}

@inproceedings{martens2018kronecker,
  title =	 {Kronecker-factored curvature approximations for
                  recurrent neural networks},
  author =	 {Martens, James and Ba, Jimmy and Johnson, Matt},
  booktitle =	 {International Conference on Learning
                  Representations},
  year =	 2018
}

@article{gossiping-jin2016scale,
  title =	 {How to scale distributed deep learning?},
  author =	 {Jin, Peter H and Yuan, Qiaochu and Iandola, Forrest
                  and Keutzer, Kurt},
  journal =	 {arXiv preprint arXiv:1611.04581},
  year =	 2016
}

@inproceedings{zhang2015deep,
  title =	 {Deep learning with elastic averaging SGD},
  author =	 {Zhang, Sixin and Choromanska, Anna E and LeCun,
                  Yann},
  booktitle =	 {Advances in Neural Information Processing Systems},
  pages =	 {685--693},
  year =	 2015
}

@inproceedings{momentum-mitliagkas2016asynchrony,
  title =	 {Asynchrony begets momentum, with an application to
                  deep learning},
  author =	 {Mitliagkas, Ioannis and Zhang, Ce and Hadjis, Stefan
                  and R{\'e}, Christopher},
  booktitle =	 {Communication, Control, and Computing (Allerton),
                  2016 54th Annual Allerton Conference on},
  pages =	 {997--1004},
  year =	 2016,
  organization = {IEEE}
}

@inproceedings{li2014scaling,
  title =	 {Scaling distributed machine learning with the
                  parameter server},
  author =	 {Li, Mu and Andersen, David G and Park, Jun Woo and
                  Smola, Alexander J and Ahmed, Amr and Josifovski,
                  Vanja and Long, James and Shekita, Eugene J and Su,
                  Bor-Yiing},
  booktitle =	 {11th $\{$USENIX$\}$ Symposium on Operating Systems
                  Design and Implementation ($\{$OSDI$\}$ 14)},
  pages =	 {583--598},
  year =	 2014
}

@inproceedings{alistarh2018convergence,
  title =	 {The convergence of stochastic gradient descent in
                  asynchronous shared memory},
  author =	 {Alistarh, Dan and De Sa, Christopher and
                  Konstantinov, Nikola},
  booktitle =	 {Proceedings of the 2018 ACM Symposium on Principles
                  of Distributed Computing},
  pages =	 {169--178},
  year =	 2018,
  organization = {ACM}
}

@article{mikami2018massively,
  title =	 {Massively Distributed SGD: ImageNet/ResNet-50
                  Training in a Flash},
  author =	 {Mikami, Hiroaki and Suganuma, Hisahiro and Tanaka,
                  Yoshiki and Kageyama, Yuichi and others},
  journal =	 {arXiv preprint arXiv:1811.05233},
  year =	 2018
}

@article{ying2018image,
  title =	 {Image classification at supercomputer scale},
  author =	 {Ying, Chris and Kumar, Sameer and Chen, Dehao and
                  Wang, Tao and Cheng, Youlong},
  journal =	 {arXiv preprint arXiv:1811.06992},
  year =	 2018
}

@article {Fang740548,
  author =	 {Fang, Linjing and Monroe, Fred and Novak, Sammy
                  Weiser and Kirk, Lyndsey and Schiavon, Cara R. and
                  Yu, Seungyoon B. and Zhang, Tong and Wu, Melissa and
                  Kastner, Kyle and Kubota, Yoshiyuki and Zhang, Zhao
                  and Pekkurnaz, Gulcin and Mendenhall, John and
                  Harris, Kristen and Howard, Jeremy and Manor, Uri},
  title =	 {Deep Learning-Based Point-Scanning Super-Resolution
                  Imaging},
  elocation-id = 740548,
  year =	 2019,
  doi =		 {10.1101/740548},
  publisher =	 {Cold Spring Harbor Laboratory},
  URL =
                  {https://www.biorxiv.org/content/early/2019/10/24/740548},
  eprint =
                  {https://www.biorxiv.org/content/early/2019/10/24/740548.full.pdf},
  journal =	 {bioRxiv}
}

@article{kanitpanyacharoen2013comparative,
  title =	 {A comparative study of X-ray tomographic microscopy
                  on shales at different synchrotron facilities: ALS,
                  APS and SLS},
  author =	 {Kanitpanyacharoen, Waruntorn and Parkinson, Dilworth
                  Y and De Carlo, Francesco and Marone, Federica and
                  Stampanoni, Marco and Mokso, Rajmund and MacDowell,
                  Alastair and Wenk, H-R},
  journal =	 {Journal of synchrotron radiation},
  volume =	 20,
  number =	 1,
  pages =	 {172--180},
  year =	 2013,
  publisher =	 {International Union of Crystallography}
}

@techreport{singh2017varied,
  title =	 {Varied volume fractions of borosilicate glass
                  spheres with diameter gaussian distributed from
                  38-45 micronsen cased in a polypropylene matrix},
  author =	 {Singh, Somya and Stannard, Tyler J and Singh,
                  Sudhanshu S and Singaravelu, Arun SS and Xiao,
                  Xianghui and Chawla, Nikhilesh},
  year =	 2017,
  institution =	 {Argonne National Lab.(ANL), Argonne, IL (United
                  States)}
}

@misc{T8/YLCK5A_2019,
  author =	 {Fang, Linjing and Monroe, Fred and Novak, Sammy and
                  Kirk, Lyndsey and Schiavon, Cara and Seungyoon, Yu
                  and Zhang, Tong and Wu, Melissa and Kastner, Kyle
                  and Kubota, Yoshiyuki and Zhang, Zhao and Pekkurnaz,
                  Gulcin and Mendenhall, John and Harris, Kristen and
                  Howard, Jeremy and Manor, Uri},
  publisher =	 {Texas Data Repository Dataverse},
  title =	 "{Training, Testing, and Validation Data for ``Deep
                  Learning-Based Point-Scanning Super-Resolution
                  Imaging''}",
  year =	 2019,
  version =	 {V1},
  doi =		 {10.18738/T8/YLCK5A},
  url =		 {https://doi.org/10.18738/T8/YLCK5A}
}

@INPROCEEDINGS{dong2019scaling,
  author =	 {W. {Dong} and M. {Keceli} and R. {Vescovi} and
                  H. {Li} and C. {Adams} and E. {Jennings} and
                  S. {Flender} and T. {Uram} and V. {Vishwanath} and
                  N. {Ferrier} and N. {Kasthuri} and P. {Littlewood}},
  booktitle =	 {2019 IEEE/ACM Third Workshop on Deep Learning on
                  Supercomputers (DLS)},
  title =	 {Scaling Distributed Training of Flood-Filling
                  Networks on HPC Infrastructure for Brain Mapping},
  year =	 2019,
  pages =	 {52-61},
  doi =		 {10.1109/DLS49591.2019.00012},
  ISSN =	 {null},
  month =	 {Nov},
}

@article{takemura2015synaptic,
  title =	 {Synaptic circuits and their variations within
                  different columns in the visual system of
                  Drosophila},
  author =	 {Takemura, Shin-ya and Xu, C Shan and Lu, Zhiyuan and
                  Rivlin, Patricia K and Parag, Toufiq and Olbris,
                  Donald J and Plaza, Stephen and Zhao, Ting and Katz,
                  William T and Umayam, Lowell and others},
  journal =	 {Proceedings of the National Academy of Sciences},
  volume =	 112,
  number =	 44,
  pages =	 {13711--13716},
  year =	 2015,
  publisher =	 {National Acad Sciences}
}

@inproceedings{lee2019deepdrivemd,
  title =	 {DeepDriveMD: Deep-Learning Driven Adaptive Molecular
                  Simulations for Protein Folding},
  author =	 {Lee, Hyungro and Turilli, Matteo and Jha, Shantenu
                  and Bhowmik, Debsindhu and Ma, Heng and Ramanathan,
                  Arvind},
  booktitle =	 {2019 IEEE/ACM Third Workshop on Deep Learning on
                  Supercomputers (DLS)},
  pages =	 {12--19},
  year =	 2019,
  organization = {IEEE}
}

@inproceedings{he2017mask,
  title =	 {Mask {R-CNN}},
  author =	 {He, Kaiming and Gkioxari, Georgia and Doll{\'a}r,
                  Piotr and Girshick, Ross},
  booktitle =	 {IEEE International Conference on Computer Vision},
  pages =	 {2961--2969},
  year =	 2017
}

@article{hannun2014deep,
  title =	 {Deep speech: Scaling up end-to-end speech
                  recognition},
  author =	 {Hannun, Awni and Case, Carl and Casper, Jared and
                  Catanzaro, Bryan and Diamos, Greg and Elsen, Erich
                  and Prenger, Ryan and Satheesh, Sanjeev and
                  Sengupta, Shubho and Coates, Adam and others},
  journal =	 {arXiv preprint arXiv:1412.5567},
  year =	 2014
}

@inproceedings{devlin2018bert,
  author =	 {Jacob Devlin and Ming{-}Wei Chang and Kenton Lee and
                  Kristina Toutanova},
  title =	 {{BERT: Pre-training of Deep Bidirectional
                  Transformers for Language Understanding}},
  booktitle =	 {{Proceedings of the 2019 Conference of the North
                  American Chapter of the Association for
                  Computational Linguistics: Human Language
                  Technologies}},
  series =	 {NAACL-HLT},
  pages =	 {4171--4186},
  publisher =	 {Association for Computational Linguistics},
  year =	 2019,
  url =		 {https://doi.org/10.18653/v1/n19-1423},
  doi =		 {10.18653/v1/n19-1423},
  timestamp =	 {Fri, 06 Aug 2021 00:41:31 +0200},
  biburl =	 {https://dblp.org/rec/conf/naacl/DevlinCLT19.bib},
  bibsource =	 {dblp computer science bibliography,
                  https://dblp.org},
}

@inproceedings{lin2014microsoft,
  title =	 {Microsoft coco: Common objects in context},
  author =	 {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge
                  and Hays, James and Perona, Pietro and Ramanan, Deva
                  and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle =	 {European conference on computer vision},
  pages =	 {740--755},
  year =	 2014,
  organization = {Springer}
}

@inproceedings{cieri2004fisher,
  title =	 {The Fisher Corpus: a Resource for the Next
                  Generations of Speech-to-Text.},
  author =	 {Cieri, Christopher and Miller, David and Walker,
                  Kevin},
  booktitle =	 {LREC},
  volume =	 4,
  pages =	 {69--71},
  year =	 2004
}

@inproceedings{panayotov2015librispeech,
  title =	 {Librispeech: an ASR corpus based on public domain
                  audio books},
  author =	 {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel
                  and Khudanpur, Sanjeev},
  booktitle =	 {2015 IEEE International Conference on Acoustics,
                  Speech and Signal Processing (ICASSP)},
  pages =	 {5206--5210},
  year =	 2015,
  organization = {IEEE}
}

@inproceedings{zhu2015aligning,
  title =	 {Aligning books and movies: Towards story-like visual
                  explanations by watching movies and reading books},
  author =	 {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and
                  Salakhutdinov, Ruslan and Urtasun, Raquel and
                  Torralba, Antonio and Fidler, Sanja},
  booktitle =	 {Proceedings of the IEEE international conference on
                  computer vision},
  pages =	 {19--27},
  year =	 2015
}

@Misc{wiki,
  author =	 {{Wikipedia}},
  title =	 "{{Wikipedia Corpus}}",
  note =	 "\url{https://www.english-corpora.org/wiki/}",
}

@Misc{mlperf0.6,
  author =	 {{MLPerf}},
  title =	 "{{MLPerf Results v0.6}}",
  note =	 "\url{https://mlperf.org/training-results-0-6}",
}

@article{you2019reducing,
  title =	 {Reducing BERT Pre-Training Time from 3 Days to 76
                  Minutes},
  author =	 {You, Yang and Li, Jing and Hseu, Jonathan and Song,
                  Xiaodan and Demmel, James and Hsieh, Cho-Jui},
  journal =	 {arXiv preprint arXiv:1904.00962},
  year =	 2019
}

@Misc{tfkfac,
  author =	 {{TensorFlow}},
  title =	 "{{TensorFlow K-FAC}}",
  note =	 "\url{https://github.com/tensorflow/kfac}",
}

@article{mccandlish2018empirical,
  title =	 {An empirical model of large-batch training},
  author =	 {McCandlish, Sam and Kaplan, Jared and Amodei, Dario
                  and Team, OpenAI Dota},
  journal =	 {arXiv preprint arXiv:1812.06162},
  year =	 2018
}

@inproceedings{parkhi2015deep,
  title =	 {Deep face recognition.},
  author =	 {Parkhi, Omkar M and Vedaldi, Andrea and Zisserman,
                  Andrew and others},
  booktitle =	 {bmvc},
  volume =	 1,
  number =	 3,
  pages =	 6,
  year =	 2015
}

@inproceedings{babuji2019parsl,
  title =	 {Parsl: Pervasive parallel programming in python},
  author =	 {Babuji, Yadu and Woodard, Anna and Li, Zhuozhao and
                  Katz, Daniel S and Clifford, Ben and Kumar, Rohan
                  and Lacinski, Lukasz and Chard, Ryan and Wozniak,
                  Justin M and Foster, Ian and others},
  booktitle =	 {Proceedings of the 28th International Symposium on
                  High-Performance Parallel and Distributed Computing},
  pages =	 {25--36},
  year =	 2019,
  organization = {ACM}
}

@inproceedings{kurihana2019cloud,
  title =	 {Cloud Characterization With Deep Learning {II}},
  author =	 {Kurihana, Takuya and Foster, Ian and Moyer,
                  Elisabeth J and Willett, Rebecca and Maire, Michael
                  and Jenkins, Sydney and Koenig, Kathryn and Werman,
                  Ruby},
  booktitle =	 {AGU Fall Meeting 2019},
  year =	 2019,
  organization = {AGU}
}

@inproceedings{abeykoon2019scientific,
  title =	 {Scientific image restoration anywhere},
  author =	 {Abeykoon, Vibhatha and Liu, Zhengchun and
                  Kettimuthu, Rajkumar and Fox, Geoffrey and Foster,
                  Ian},
  booktitle =	 {2019 IEEE/ACM 1st Annual Workshop on Large-scale
                  Experiment-in-the-Loop Computing (XLOOP)},
  pages =	 {8--13},
  year =	 2019,
  organization = {IEEE}
}

@inproceedings{liu2019deep,
  title =	 {Deep learning accelerated light source experiments},
  author =	 {Liu, Zhengchun and Bicer, Tekin and Kettimuthu,
                  Rajkumar and Foster, Ian},
  booktitle =	 {IEEE/ACM Third Workshop on Deep Learning on
                  Supercomputers},
  pages =	 {20--28},
  year =	 2019,
  organization = {IEEE}
}

@inproceedings{montella2019stormseeker,
  title =	 {Storm{S}eeker: A Machine-Learning-Based
                  {M}editerranean Storm Tracer},
  author =	 {Montella, Raffaele and Di Luccio, Diana and
                  Ciaramella, Angelo and Foster, Ian},
  booktitle =	 {International Conference on Internet and Distributed
                  Computing Systems},
  pages =	 {444--456},
  year =	 2019,
  organization = {Springer}
}

@inproceedings{jha2019irnet,
  title =	 {{IRNet}: A general purpose deep residual regression
                  framework for materials discovery},
  author =	 {Jha, Dipendra and Ward, Logan and Yang, Zijiang and
                  Wolverton, Christopher and Foster, Ian and Liao,
                  Wei-keng and Choudhary, Alok and Agrawal, Ankit},
  booktitle =	 {25th ACM SIGKDD International Conference on
                  Knowledge Discovery \& Data Mining},
  pages =	 {2385--2393},
  year =	 2019
}

@article{liu2019tomogan,
  title =	 {{TomoGAN}: Low-Dose X-Ray Tomography with Generative
                  Adversarial Networks},
  author =	 {Liu, Zhengchun and Bicer, Tekin and Kettimuthu,
                  Rajkumar and Gursoy, Doga and De Carlo, Francesco
                  and Foster, Ian},
  journal =	 {arXiv preprint arXiv:1902.07582},
  year =	 2019
}

@article{wozniak2018scaling,
  title =	 {Scaling deep learning for cancer with advanced
                  workflow storage integration},
  author =	 {Wozniak, Justin M and Davis, Philip E and Shu, Tong
                  and Ozik, Jonathan and Collier, Nicholson and
                  Parashar, Manish and Foster, Ian and Brettin, Thomas
                  and Stevens, Rick},
  journal =	 {Proc. Machine Learning in High Performance Computing
                  Environments (MLHPC) at SC},
  year =	 2018
}

@article{ward2019machine,
  title =	 {Machine Learning Prediction of Accurate Atomization
                  Energies of Organic Molecules from Low-Fidelity
                  Quantum Chemical Calculations},
  author =	 {Ward, Logan and Blaiszik, Ben and Foster, Ian and
                  Assary, Rajeev S and Narayanan, Badri and Curtiss,
                  Larry},
  journal =	 {arXiv preprint arXiv:1906.03233},
  year =	 2019
}

@article{chan2019machine,
  title =	 {Machine Learning Classical Interatomic Potentials
                  for Molecular Dynamics from First-Principles
                  Training Data},
  author =	 {Chan, Henry and Narayanan, Badri and Cherukara,
                  Mathew J and Sen, Fatih G and Sasikumar, Kiran and
                  Gray, Stephen K and Chan, Maria KY and
                  Sankaranarayanan, Subramanian KRS},
  journal =	 {The Journal of Physical Chemistry C},
  volume =	 123,
  number =	 12,
  pages =	 {6941--6957},
  year =	 2019,
  publisher =	 {ACS Publications}
}

@article{jackson2019electronic,
  title =	 {Electronic structure at coarse-grained resolutions
                  from supervised machine learning},
  author =	 {Jackson, Nicholas E and Bowen, Alec S and Antony,
                  Lucas W and Webb, Michael A and Vishwanath,
                  Venkatram and de Pablo, Juan J},
  journal =	 {Science advances},
  volume =	 5,
  number =	 3,
  pages =	 {eaav1190},
  year =	 2019,
  publisher =	 {American Association for the Advancement of Science}
}

@inproceedings{bergstra2011algorithms,
  title =	 {Algorithms for hyper-parameter optimization},
  author =	 {Bergstra, James S and Bardenet, R{\'e}mi and Bengio,
                  Yoshua and K{\'e}gl, Bal{\'a}zs},
  booktitle =	 {Advances in neural information processing systems},
  pages =	 {2546--2554},
  year =	 2011
}

@inproceedings{balaprakash2018deephyper,
  title =	 {{DeepHyper}: Asynchronous hyperparameter search for
                  deep neural networks},
  author =	 {Balaprakash, Prasanna and Salim, Michael and Uram,
                  Thomas and Vishwanath, Venkat and Wild, Stefan},
  booktitle =	 {IEEE 25th International Conference on High
                  Performance Computing},
  pages =	 {42--51},
  year =	 2018,
  organization = {IEEE}
}

@inproceedings{ICAC2014,
  title =	 {$\{$PCP$\}$: A Generalized Approach to Optimizing
                  Performance Under Power Constraints through Resource
                  Management},
  author =	 {Hoffmann, Henry and Maggio, Martina},
  booktitle =	 {11th International Conference on Autonomic Computing
                  ($\{$ICAC$\}$ 14)},
  pages =	 {241--247},
  year =	 2014
}

@article{ASPLOS2016,
  title =	 {Maximizing performance under a power cap: A
                  comparison of hardware, software, and hybrid
                  techniques},
  author =	 {Zhang, Huazhe and Hoffmann, Henry},
  journal =	 {ACM SIGPLAN Notices},
  volume =	 51,
  number =	 4,
  pages =	 {545--559},
  year =	 2016,
  publisher =	 {ACM New York, NY, USA}
}

@inproceedings{ICPP2018,
  title =	 {Performance \& energy tradeoffs for dependent
                  distributed applications under system-wide power
                  caps},
  author =	 {Zhang, Huazhe and Hoffmann, Henry},
  booktitle =	 {Proceedings of the 47th International Conference on
                  Parallel Processing},
  pages =	 {1--11},
  year =	 2018
}

@inproceedings{SC2019,
  title =	 {PoDD: power-capping dependent distributed
                  applications},
  author =	 {Zhang, Huazhe and Hoffmann, Henry},
  booktitle =	 {Proceedings of the International Conference for High
                  Performance Computing, Networking, Storage and
                  Analysis},
  pages =	 {1--23},
  year =	 2019
}

@inproceedings{IPDPS2020,
  title =	 {SeeSAw: Optimizing Performance of In-Situ Analytics
                  Applications under Power Constraints},
  author =	 {Marincic, Ivana and Vishwanath, Venkatram and
                  Hoffmann, Henry},
  booktitle =	 {to appear in 2020 IEEE 34th International Symposium
                  on Parallel and Distributed Processing},
  organization = {IEEE}
}

@inproceedings{STACS2018,
  title =	 {Approximation Algorithms for Scheduling with
                  Resource and Precedence Constraints},
  author =	 {Demirci, G{\"o}kalp and Hoffmann, Henry and Kim,
                  David HK},
  booktitle =	 {35th Symposium on Theoretical Aspects of Computer
                  Science (STACS 2018)},
  year =	 2018,
  organization = {Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik}
}

@inproceedings{SC2018,
  title =	 {A divide and conquer algorithm for DAG scheduling
                  under power constraints},
  author =	 {Demirci, G{\"o}kalp and Marincic, Ivana and
                  Hoffmann, Henry},
  booktitle =	 {SC18: International Conference for High Performance
                  Computing, Networking, Storage and Analysis},
  pages =	 {466--477},
  year =	 2018,
  organization = {IEEE}
}

@inproceedings{racingpacing.cpsna15,
  title =	 {Racing and pacing to idle: Theoretical and empirical
                  analysis of energy optimization heuristics},
  author =	 {Kim, David HK and Imes, Connor and Hoffmann, Henry},
  booktitle =	 {2015 IEEE 3rd international conference on
                  cyber-physical systems, networks, and applications},
  pages =	 {78--85},
  year =	 2015,
  organization = {IEEE}
}

@inproceedings{poet.rtas15,
  title =	 {{POET}: A portable approach to minimizing energy
                  under soft real-time constraints},
  author =	 {Imes, Connor and Kim, David HK and Maggio, Martina
                  and Hoffmann, Henry},
  booktitle =	 {21st IEEE Real-Time and Embedded Technology and
                  Applications Symposium},
  pages =	 {75--86},
  year =	 2015,
  organization = {IEEE}
}

@inproceedings{automulti.fse15,
  title =	 {Automated multi-objective control for self-adaptive
                  software design},
  author =	 {Filieri, Antonio and Hoffmann, Henry and Maggio,
                  Martina},
  booktitle =	 {10th Joint Meeting on Foundations of Software
                  Engineering},
  pages =	 {13--24},
  year =	 2015
}

@inproceedings{jouleguard.sosp15,
  title =	 {JouleGuard: energy guarantees for approximate
                  applications},
  author =	 {Hoffmann, Henry},
  booktitle =	 {25th Symposium on Operating Systems Principles},
  pages =	 {198--214},
  year =	 2015
}

@inproceedings{multiphaseopt.isca19,
  title =	 {Generative and multi-phase learning for computer
                  systems optimization},
  author =	 {Ding, Yi and Mishra, Nikita and Hoffmann, Henry},
  booktitle =	 {46th International Symposium on Computer
                  Architecture},
  pages =	 {39--52},
  year =	 2019
}

@inproceedings{multiplegoals.fse17,
  title =	 {Automated control of multiple software goals using
                  multiple actuators},
  author =	 {Maggio, Martina and Papadopoulos, Alessandro
                  Vittorio and Filieri, Antonio and Hoffmann, Henry},
  booktitle =	 {11th Joint Meeting on Foundations of Software
                  Engineering},
  pages =	 {373--384},
  year =	 2017
}

@article{rajpurkar2016squad,
  title =	 {Squad: 100,000+ questions for machine comprehension
                  of text},
  author =	 {Rajpurkar, Pranav and Zhang, Jian and Lopyrev,
                  Konstantin and Liang, Percy},
  journal =	 {arXiv preprint arXiv:1606.05250},
  year =	 2016
}

@article{pauloski2020convolutional,
  title =	 {Convolutional Neural Network Training with
                  Distributed {K-FAC}},
  author =	 {Pauloski, J Gregory and Zhang, Zhao and Huang, Lei
                  and Xu, Weijia and Foster, Ian T},
  journal =	 {International Conference for High Performance
                  Computing, Networking, Storage and Analysis},
  year =	 2020
}

@article{kates2019predicting,
  title =	 {Predicting disruptive instabilities in controlled
                  fusion plasmas through deep learning},
  author =	 {Kates-Harbeck, Julian and Svyatkovskiy, Alexey and
                  Tang, William},
  journal =	 {Nature},
  volume =	 568,
  number =	 7753,
  pages =	 {526--531},
  year =	 2019,
  publisher =	 {Nature Publishing Group}
}

@inproceedings{stanzione2020frontera,
  title =	 {Frontera: The Evolution of Leadership Computing at
                  the {National Science Foundation}},
  author =	 {Stanzione, Dan and West, John and Evans, R Todd and
                  Minyard, Tommy and Ghattas, Omar and Panda,
                  Dhabaleswar K},
  booktitle =	 {Practice and Experience in Advanced Research
                  Computing},
  pages =	 {106--111},
  year =	 2020
}

@article{vogel2016superfast,
  title =	 {Superfast divide-and-conquer method and perturbation
                  analysis for structured eigenvalue solutions},
  author =	 {Vogel, James and Xia, Jianlin and Cauley, Stephen
                  and Balakrishnan, Venkataramanan},
  journal =	 {SIAM Journal on Scientific Computing},
  volume =	 38,
  number =	 3,
  pages =	 {A1358--A1382},
  year =	 2016,
  publisher =	 {SIAM}
}

@inproceedings{dean20201,
  title =	 {The Deep Learning Revolution and Its Implications
                  for Computer Architecture and Chip Design},
  author =	 {Dean, Jeffrey},
  booktitle =	 {IEEE International Solid-State Circuits Conference},
  pages =	 {8--14},
  year =	 2020,
  organization = {IEEE}
}

@inproceedings{ronneberger2015u,
  title =	 {U-net: Convolutional networks for biomedical image
                  segmentation},
  author =	 {Ronneberger, Olaf and Fischer, Philipp and Brox,
                  Thomas},
  booktitle =	 {International Conference on Medical image computing
                  and computer-assisted intervention},
  pages =	 {234--241},
  year =	 2015,
  organization = {Springer}
}

@inproceedings{moritz2018ray,
  title =	 {Ray: A distributed framework for emerging $\{$AI$\}$
                  applications},
  author =	 {Moritz, Philipp and Nishihara, Robert and Wang,
                  Stephanie and Tumanov, Alexey and Liaw, Richard and
                  Liang, Eric and Elibol, Melih and Yang, Zongheng and
                  Paul, William and Jordan, Michael I and others},
  booktitle =	 {13th $\{$USENIX$\}$ Symposium on Operating Systems
                  Design and Implementation ($\{$OSDI$\}$ 18)},
  pages =	 {561--577},
  year =	 2018
}

@inproceedings{jia2020pushing,
  title =	 {Pushing the limit of molecular dynamics with ab
                  initio accuracy to 100 million atoms with machine
                  learning},
  author =	 {Jia, Weile and Wang, Han and Chen, Mohan and Lu,
                  Denghui and Lin, Lin and Car, Roberto and Weinan, E
                  and Zhang, Linfeng},
  booktitle =	 {SC20: International Conference for High Performance
                  Computing, Networking, Storage and Analysis},
  pages =	 {1--14},
  year =	 2020,
  organization = {IEEE}
}

@article{senior2020improved,
  title =	 {Improved protein structure prediction using
                  potentials from deep learning},
  author =	 {Senior, Andrew W and Evans, Richard and Jumper, John
                  and Kirkpatrick, James and Sifre, Laurent and Green,
                  Tim and Qin, Chongli and {\v{Z}}{\'\i}dek, Augustin
                  and Nelson, Alexander WR and Bridgland, Alex and
                  others},
  journal =	 {Nature},
  volume =	 577,
  number =	 7792,
  pages =	 {706--710},
  year =	 2020,
  publisher =	 {Nature Publishing Group}
}

@article{casalino2021ai,
  title =	 {AI-driven multiscale simulations illuminate
                  mechanisms of SARS-CoV-2 spike dynamics},
  author =	 {Casalino, Lorenzo and Dommer, Abigail C and Gaieb,
                  Zied and Barros, Emilia P and Sztain, Terra and Ahn,
                  Surl-Hee and Trifan, Anda and Brace, Alexander and
                  Bogetti, Anthony T and Clyde, Austin and others},
  journal =	 {{The International Journal of High Performance
                  Computing Applications}},
  pages =	 10943420211006452,
  publisher =	 {SAGE Publications Sage UK: London, England},
  year =	 2021,
}

@article{fan2021predicting,
  title =	 {Predicting orientation-dependent plastic
                  susceptibility from static structure in amorphous
                  solids via deep learning},
  author =	 {Fan, Zhao and Ma, Evan},
  journal =	 {Nature communications},
  volume =	 12,
  number =	 1,
  pages =	 {1--13},
  year =	 2021,
  publisher =	 {Nature Publishing Group}
}

@article{jumper2021highly,
  title =	 {Highly accurate protein structure prediction with
                  AlphaFold},
  author =	 {Jumper, John and Evans, Richard and Pritzel,
                  Alexander and Green, Tim and Figurnov, Michael and
                  Ronneberger, Olaf and Tunyasuvunakool, Kathryn and
                  Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and
                  Potapenko, Anna and others},
  journal =	 {Nature},
  volume =	 596,
  number =	 7873,
  pages =	 {583--589},
  year =	 2021,
  publisher =	 {Nature Publishing Group}
}

@Misc{top500,
  author =	 {{top500.org}},
  title =	 "{{TOP500 List}}",
  note =
                  "\url{https://www.top500.org/lists/top500/list/2021/11/}",
  year =	 2021,
}

@Misc{tesla,
  author =	 {{Tesla}},
  title =	 "{{Ahead of ‘Dojo,’ Tesla Reveals Its Massive
                  Precursor Supercomputer}}",
  note =
                  "\url{https://www.hpcwire.com/2021/06/22/ahead-of-dojo-tesla-reveals-its-massive-precursor-supercomputer/}",
  year =	 2021,
}

@ARTICLE{TalpesSarma2023-teslaDojo,
  author =	 {Talpes, Emil and Sarma, Debjit Das and Williams,
                  Doug and Arora, Sahil and Kunjan, Thomas and
                  Floering, Benjamin and Jalote, Ankit and Hsiong,
                  Christopher and Poorna, Chandrasekhar and Samant,
                  Vaidehi and Sicilia, John and Nivarti, Anantha Kumar
                  and Ramachandran, Raghuvir and Fischer, Tim and
                  Herzberg, Ben and McGee, Bill and Venkataramanan,
                  Ganesh and Banon, Pete},
  journal =	 {IEEE Micro},
  title =	 {{The Microarchitecture of DOJO, Tesla’s Exa-Scale
                  Computer}},
  year =	 2023,
  volume =	 43,
  number =	 03,
  ISSN =	 {1937-4143},
  pages =	 {31-39},
  abstract =	 { The Tesla-built DOJO system is a scalable solution
                  targeted towards machine learning training
                  applications. It is based on the D1 custom compute
                  chip which packs together 354 independent
                  processors, resulting in 362 TFLOPS of compute and
                  440 MB of internal static random-access memory
                  storage. While maintaining full programmability,
                  DOJO emphasizes distribution of resources and an
                  extremely high bandwidth interconnect, allowing it
                  to scale from small systems all the way to exaFLOP
                  supercomputers. },
  keywords =	 {Computer
                  architecture;Training;Microarchitecture;Machine
                  learning;Bandwidth;Instruction sets;Computational
                  modeling},
  doi =		 {10.1109/MM.2023.3258906},
  url =
                  {https://doi.ieeecomputersociety.org/10.1109/MM.2023.3258906},
  publisher =	 {IEEE Computer Society},
  address =	 {Los Alamitos, CA, USA},
  month =	 may
}

@inproceedings{pauloski2021kaisa,
  title =	 {KAISA: an adaptive second-order optimizer framework
                  for deep neural networks},
  author =	 {Pauloski, J Gregory and Huang, Qi and Huang, Lei and
                  Venkataraman, Shivaram and Chard, Kyle and Foster,
                  Ian and Zhang, Zhao},
  booktitle =	 {Proceedings of the International Conference for High
                  Performance Computing, Networking, Storage and
                  Analysis},
  pages =	 {1--14},
  year =	 2021
}


@article{fang2021deep,
  title =	 {Deep learning-based point-scanning super-resolution
                  imaging},
  author =	 {Fang, Linjing and Monroe, Fred and Novak, Sammy
                  Weiser and Kirk, Lyndsey and Schiavon, Cara R and
                  Seungyoon, B Yu and Zhang, Tong and Wu, Melissa and
                  Kastner, Kyle and Latif, Alaa Abdel and others},
  journal =	 {Nature Methods},
  volume =	 18,
  number =	 4,
  pages =	 {406--416},
  year =	 2021,
  publisher =	 {Nature Publishing Group}
}

@article{ivezic2019lsst,
  title =	 {LSST: from science drivers to reference design and
                  anticipated data products},
  author =	 {Ivezi{\'c}, {\v{Z}}eljko and Kahn, Steven M and
                  Tyson, J Anthony and Abel, Bob and Acosta, Emily and
                  Allsman, Robyn and Alonso, David and AlSayyad, Yusra
                  and Anderson, Scott F and Andrew, John and others},
  journal =	 {The Astrophysical Journal},
  volume =	 873,
  number =	 2,
  pages =	 111,
  year =	 2019,
  publisher =	 {IOP Publishing}
}

@inproceedings{pu2016fairride,
  title =	 {Fairride: Near-optimal, fair cache sharing},
  author =	 {Pu, Qifan and Li, Haoyuan and Zaharia, Matei and
                  Ghodsi, Ali and Stoica, Ion},
  booktitle =	 {13th {USENIX} Symposium on Networked Systems Design
                  and Implementation ({NSDI} 16)},
  pages =	 {393--406},
  year =	 2016
}

@inproceedings{mahajan2020themis,
  title =	 {Themis: Fair and efficient {GPU} cluster scheduling},
  author =	 {Mahajan, Kshiteej and Balasubramanian, Arjun and
                  Singhvi, Arjun and Venkataraman, Shivaram and
                  Akella, Aditya and Phanishayee, Amar and Chawla,
                  Shuchi},
  booktitle =	 {17th {USENIX} Symposium on Networked Systems Design
                  and Implementation ({NSDI} 20)},
  pages =	 {289--304},
  year =	 2020
}

@misc{gpt-neox,
  author =	 {Andonian, Alex and Anthony, Quentin and Biderman,
                  Stella and Black, Sid and Gali, Preetham and Gao,
                  Leo and Hallahan, Eric and Levy-Kramer, Josh and
                  Leahy, Connor and Nestler, Lucas and Parker, Kip and
                  Pieler, Michael and Purohit, Shivanshu and Songz,
                  Tri and Wang, Phil and Weinbach, Samuel},
  title =	 {{GPT-NeoX: Large Scale Autoregressive Language
                  Modeling in PyTorch}},
  url =		 {http://github.com/eleutherai/gpt-neox},
  year =	 2021
}

@inproceedings{gowda-etal-2021-many,
  title =	 "Many-to-{E}nglish Machine Translation Tools, Data,
                  and Pretrained Models",
  author =	 "Gowda, Thamme and Zhang, Zhao and Mattmann, Chris
                  and May, Jonathan",
  booktitle =	 "Proceedings of the 59th Annual Meeting of the
                  Association for Computational Linguistics and the
                  11th International Joint Conference on Natural
                  Language Processing: System Demonstrations",
  month =	 aug,
  year =	 2021,
  address =	 "Online",
  publisher =	 "Association for Computational Linguistics",
  url =		 "https://aclanthology.org/2021.acl-demo.37",
  doi =		 "10.18653/v1/2021.acl-demo.37",
  pages =	 "306--316",
}

@misc{spectre-classifier,
  author =	 {Zhang, Zhao and Gaffney, Niall},
  title =	 {A Spectrum based Supernovae Classifier},
  url =		 {https://github.com/zhaozhang/spectra},
  year =	 2020,
}

@inproceedings{smith1999using,
  title =	 {Using run-time predictions to estimate queue wait
                  times and improve scheduler performance},
  author =	 {Smith, Warren and Taylor, Valerie and Foster, Ian},
  booktitle =	 {Workshop on Job scheduling strategies for Parallel
                  Processing},
  pages =	 {202--219},
  year =	 1999,
  organization = {Springer}
}

@inproceedings{nurmi2006evaluation,
  title =	 {Evaluation of a workflow scheduler using integrated
                  performance modelling and batch queue wait time
                  prediction},
  author =	 {Nurmi, Daniel and Mandal, Anirban and Brevik, John
                  and Koelbel, Chuck and Wolski, Rich and Kennedy,
                  Ken},
  booktitle =	 {SC'06: Proceedings of the 2006 ACM/IEEE conference
                  on Supercomputing},
  pages =	 {29--29},
  year =	 2006,
  organization = {IEEE}
}

@inproceedings{nurmi2007qbets,
  title =	 {QBETS: Queue bounds estimation from time series},
  author =	 {Nurmi, Daniel and Brevik, John and Wolski, Rich},
  booktitle =	 {Workshop on Job Scheduling Strategies for Parallel
                  Processing},
  pages =	 {76--101},
  year =	 2007,
  organization = {Springer}
}

@inproceedings{sonmez2009trace,
  title =	 {Trace-based evaluation of job runtime and queue wait
                  time predictions in grids},
  author =	 {Sonmez, Ozan and Yigitbasi, Nezih and Iosup,
                  Alexandru and Epema, Dick},
  booktitle =	 {Proceedings of the 18th ACM international symposium
                  on High performance distributed computing},
  pages =	 {111--120},
  year =	 2009
}

@string {ASPLOS = "Proceedings of the International Conference on Architectural Support for Programming Languages and Operation Systems (ASPLOS)"}
@string {ISCA = "Proceedings of the International Symposium on Computer Architecture (ISCA)"}
@string {MICRO = "Proceedings of the International Symposium on Microarchitecture (MICRO)"}
@string {HPCA = "Proceedings of the International Symposium on High-Performance Computer Architecture (HPCA)"}
@string {ICCD = "Proceedings of the International Conference on Computer Design (ICCD)"}
@string {PACT = "Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT)"}
@string {UTCS = "Department of Computer Sciences, The University of Texas at Austin"}       
@string {UTECE = "Department of Electrical and Computer Engineering, The University of Texas at Austin"}        
@string {PPoPP = "Proceedings of the Symposium on Principles and Practice of Parallel Programming (PPOPP)"}
@string {ISSCC = "Proceedings of the International Solid State Circuits Conference (ISSCC)"}
@string {SOSP = "Proceedings of the ACM Symposium on Operating System Principles (SOSP)"}
@string {JSSC = "IEEE Journal of Solid-State Circuits"}
@string {SC = "Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis (SC)"}
@string {ICS = "Proceedings of the ACM international conference on Supercomputing"}       
@string {SPAA = "Proceedings of the ACM Symposium on Parallel Algorithms and Architectures (SPAA)"}
@string {IEEETC = "IEEE Transactions on Computers"}
@string {ACMTC = "ACM Transactions on Computer Systems"}
@string {CACM = "Communications of the ACM"}
@string {PLDI = "Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)"}
@string {DAC = "Proceedings of the Design Automation Conference (DAC)"}
@string {IEDM = "Proceedings of the International Electron Devices Meeting (IEDM)"}
@string {CGO = "International Symposium on Code Generation and Optimization (CGO)"}      
@string {IISWC = "Proceedings of the International Symposium on Workload Characterization (IISWC)"}
@string {ICPP = "Proceedings of the International Conference on Parallel Processing (ICPP)"}  
@string {HIPC = "Proceedings of the International Conference on High Performance Computing (HiPC)"}
@string {ISPASS = "Proceedings of the International Symposium on Performance Analysis of Systems and Software (ISPASS)"}       
@string {IPDPS = "Proceedings of the  International Parallel and Distributed Processing Symposium (IPDPS)"}
@string {OOPSLA = "Proceedings of the Object-Oriented Programming, Systems, Languages and Applications (OOPSLA)"}
@string {SOCC = "Proceedings of the Symposium on Cloud Computing (SOCC)"}
@string {VLDB = "The International Journal on Very Large Data Bases (VLDB)"}
@string {SIGMOD = "Proceedings of the International Conference on Management of Data (SIGMOD)"}
@string {SIGOPS = "ACM SIGOPS Operating Systems Review"}
@string {FPGA = "Proceedings of the International Symposium on Field-Programmable Gate Arrays"}
@string {OSDI = "Proceedings of the International Conference on Operating Systems Design and Implementation"}
@string {POPL = "Proceedings of the ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages"}

@MISC{BERTforGoogleSearch,
  author =	 {Nayak, Pandu},
  title =	 {{Understanding searches better than ever before}},
  howpublished =
                  {"\url{https://blog.google/products/search/search-language-understanding-bert/}"},
  year =	 2019,
}

@inbook{Rumelhart88RNN,
  author =	 {Rumelhart, David E. and Hinton, Geoffrey E. and
                  Williams, Ronald J.},
  title =	 {{Learning Representations by Back-Propagating
                  Errors}},
  year =	 1988,
  isbn =	 0262010976,
  publisher =	 {MIT Press},
  address =	 {Cambridge, MA, USA},
  booktitle =	 {{Neurocomputing: Foundations of Research}},
  pages =	 {696–699},
  numpages =	 4
}

@MISC{MLGrowth,
  author =	 {Gholami, Amir},
  title =	 {{Memory Footprint and FLOPs for SOTA Models in
                  CV/NLP/Speech}},
  howpublished =
                  {"\url{https://github.com/amirgholami/ai_and_memory_wall}"},
  year =	 2021,
}

@misc{jia2018data,
  title =	 {Beyond Data and Model Parallelism for Deep Neural
                  Networks},
  author =	 {Zhihao Jia and Matei Zaharia and Alex Aiken},
  year =	 2018,
  eprint =	 {1807.05358},
  archivePrefix ={arXiv},
  primaryClass = {cs.DC}
}

@inproceedings{DevlinChang18-bert,
  author =	 {Jacob Devlin and Ming{-}Wei Chang and Kenton Lee and
                  Kristina Toutanova},
  editor =	 {Jill Burstein and Christy Doran and Thamar Solorio},
  title =	 {{BERT: Pre-training of Deep Bidirectional
                  Transformers for Language Understanding}},
  booktitle =	 {{Proceedings of the 2019 Conference of the North
                  American Chapter of the Association for
                  Computational Linguistics: Human Language
                  Technologies}},
  series =	 {NAACL-HLT},
  pages =	 {4171--4186},
  publisher =	 {Association for Computational Linguistics},
  year =	 2019,
  url =		 {https://doi.org/10.18653/v1/n19-1423},
  doi =		 {10.18653/v1/n19-1423},
  timestamp =	 {Fri, 06 Aug 2021 00:41:31 +0200},
  biburl =	 {https://dblp.org/rec/conf/naacl/DevlinCLT19.bib},
  bibsource =	 {dblp computer science bibliography,
                  https://dblp.org}
}

@inproceedings{AmodeiAnubhai2016-deepSpeech2,
  author =	 {Dario Amodei and Rishita Anubhai and Eric Battenberg
                  and Carl Case and Jared Casper and Bryan Catanzaro
                  and Jingdong Chen and Mike Chrzanowski and Adam
                  Coates and Greg Diamos and Erich Elsen and Jesse
                  Engel and Linxi Fan and Christopher Fougner and Awni
                  Y. Hannun and Billy Jun and Tony Han and Patrick
                  LeGresley and Xiangang Li and Libby Lin and Sharan
                  Narang and Andrew Y. Ng and Sherjil Ozair and Ryan
                  Prenger and Sheng Qian and Jonathan Raiman and
                  Sanjeev Satheesh and David Seetapun and Shubho
                  Sengupta and Chong Wang and Yi Wang and Zhiqian Wang
                  and Bo Xiao and Yan Xie and Dani Yogatama and Jun
                  Zhan and Zhenyao Zhu},
  title =	 {{Deep Speech 2 : End-to-End Speech Recognition in
                  English and Mandarin}},
  booktitle =	 {{Proceedings of the 33nd International Conference on
                  Machine Learning}},
  series =	 {{ICML}},
  pages =	 {173--182},
  year =	 2016,
}

@article{WuSchuster16-seq2seq,
  author =	 {Yonghui Wu and Mike Schuster and Zhifeng Chen and
                  Quoc V. Le and Mohammad Norouzi and Wolfgang
                  Macherey and Maxim Krikun and Yuan Cao and Qin Gao
                  and Klaus Macherey and Jeff Klingner and Apurva Shah
                  and Melvin Johnson and Xiaobing Liu and Lukasz
                  Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku
                  Kudo and Hideto Kazawa and Keith Stevens and George
                  Kurian and Nishant Patil and Wei Wang and Cliff
                  Young and Jason Smith and Jason Riesa and Alex
                  Rudnick and Oriol Vinyals and Greg Corrado and
                  Macduff Hughes and Jeffrey Dean},
  title =	 {{Google's Neural Machine Translation System:
                  Bridging the Gap between Human and Machine
                  Translation}},
  journal =	 {CoRR},
  volume =	 {abs/1609.08144},
  year =	 2016,
  url =		 {http://arxiv.org/abs/1609.08144},
  archivePrefix ={arXiv},
  eprint =	 {1609.08144},
}

@inproceedings{PatiAga20-seqPoints,
  author =	 {Pati, Suchita and Aga, Shaizeen and Sinclair,
                  Matthew D. and Jayasena, Nuwan},
  title =	 {{SeqPoint: Identifying Representative Iterations of
                  Sequence-based Neural Networks}},
  booktitle =	 {{IEEE International Symposium on Performance
                  Analysis of Systems and Software}},
  series =	 {ISPASS},
  year =	 2020,
  month =	 {August},
  publisher =	 {IEEE Computer Society},
  address =	 {Washington, DC, USA},
  pages =	 {69-80},
  doi =		 {10.1109/ISPASS48437.2020.00017}
}

@article{MattsonReddi2020-mlPerf,
  title =	 {{MLPerf: An Industry Standard Benchmark Suite for
                  Machine Learning Performance}},
  author =	 {Mattson, Peter and Reddi, Vijay Janapa and Cheng,
                  Christine and Coleman, Cody and Diamos, Greg and
                  Kanter, David and Micikevicius, Paulius and
                  Patterson, David and Schmuelling, Guenther and Tang,
                  Hanlin and others},
  journal =	 {IEEE Micro},
  volume =	 40,
  number =	 2,
  pages =	 {8--16},
  year =	 2020,
  publisher =	 {IEEE},
}

@MISC{Tensile,
  author =	 {{AMD}},
  title =	 {{AMD's tool for creating a benchmark-driven backend
                  library for GEMMs}},
  howpublished =
                  {"\url{https://github.com/ROCmSoftwarePlatform/Tensile/}"},
  year =	 2020,
}

@MISC{NV-DL-Perf,
  author =	 {{NVIDIA}},
  title =	 {NVIDIA Deep Learning Performance},
  howpublished =
                  {"\url{https://docs.nvidia.com/deeplearning/performance/index.html}"},
  year =	 2019,
}

@article{HochreiterSchmidhuber1997-lstm,
  author =	 {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
  title =	 {{Long Short-Term Memory}},
  journal =	 {{Neural Computation}},
  issue_date =	 {November 15, 1997},
  volume =	 9,
  number =	 8,
  month =	 nov,
  year =	 1997,
  issn =	 {0899-7667},
  pages =	 {1735--1780},
  numpages =	 46,
  url =		 {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
  doi =		 {10.1162/neco.1997.9.8.1735},
  acmid =	 1246450,
  publisher =	 {MIT Press},
  address =	 {Cambridge, MA, USA},
}

@INPROCEEDINGS{ChoMerrienboer14-gru,
  author =	 {Cho, Kyunghyun and van Merri{\"{e}}nboer, Bart and
                  G{\"{u}}l{\c c}ehre, {\c C}ağlar and Bahdanau,
                  Dzmitry and Bougares, Fethi and Schwenk, Holger and
                  Bengio, Yoshua},
  month =	 oct,
  title =	 {{Learning Phrase Representations using RNN
                  Encoder--Decoder for Statistical Machine
                  Translation}},
  booktitle =	 {{Proceedings of the 2014 Conference on Empirical
                  Methods in Natural Language Processing}},
  series =	 {EMNLP},
  year =	 2014,
  pages =	 {1724--1734},
  publisher =	 {Association for Computational Linguistics},
  address =	 {Doha, Qatar},
  url =		 {http://www.aclweb.org/anthology/D14-1179}
}

%Concurrency in GPU

@inproceedings{adriaens2012case,
  title =	 {{The Case for GPGPU Spatial Multitasking}},
  author =	 {Adriaens, Jacob T and Compton, Katherine and Kim,
                  Nam Sung and Schulte, Michael J},
  booktitle =	 {{IEEE International Symposium on High-Performance
                  Comp Architecture}},
  pages =	 {1--12},
  year =	 2012,
  organization = {IEEE},
  series =	 {HPCA},
  publisher =	 {IEEE Computer Society},
  address =	 {Washington, DC, USA},
}

@inproceedings{jiao2015improving,
  title =	 {{Improving GPGPU energy-efficiency through
                  concurrent kernel execution and DVFS}},
  author =	 {Jiao, Qing and Lu, Mian and Huynh, Huynh Phung and
                  Mitra, Tulika},
  booktitle =	 {{2015 IEEE/ACM International Symposium on Code
                  Generation and Optimization}},
  series =	 {CGO},
  pages =	 {1--11},
  year =	 2015,
  organization = {IEEE},
}

@inproceedings{pai2013improving,
  author =	 {Pai, Sreepathi and Thazhuthaveetil, Matthew J. and
                  Govindarajan, R.},
  title =	 {{Improving GPGPU Concurrency with Elastic Kernels}},
  year =	 2013,
  isbn =	 9781450318709,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/2451116.2451160},
  doi =		 {10.1145/2451116.2451160},
  abstract =	 {Each new generation of GPUs vastly increases the
                  resources available to GPGPU programs.  GPU
                  programming models (like CUDA) were designed to
                  scale to use these resources.  However, we find that
                  CUDA programs actually do not scale to utilize all
                  available resources, with over 30\% of resources
                  going unused on average for programs of the Parboil2
                  suite that we used in our work. Current GPUs
                  therefore allow concurrent execution of kernels to
                  improve utilization. In this work, we study
                  concurrent execution of GPU kernels using
                  multiprogram workloads on current NVIDIA Fermi
                  GPUs. On two-program workloads from the Parboil2
                  benchmark suite we find concurrent execution is
                  often no better than serialized execution. We
                  identify that the lack of control over resource
                  allocation to kernels is a major serialization
                  bottleneck. We propose transformations that convert
                  CUDA kernels into elastic kernels which permit
                  fine-grained control over their resource usage. We
                  then propose several elastic-kernel aware
                  concurrency policies that offer significantly better
                  performance and concurrency compared to the current
                  CUDA policy. We evaluate our proposals on real
                  hardware using multiprogrammed workloads constructed
                  from benchmarks in the Parboil 2 suite. On average,
                  our proposals increase system throughput (STP) by
                  1.21x and improve the average normalized turnaround
                  time (ANTT) by 3.73x for two-program workloads when
                  compared to the current CUDA concurrency
                  implementation.},
  booktitle =	 {{Proceedings of the Eighteenth International
                  Conference on Architectural Support for Programming
                  Languages and Operating Systems}},
  pages =	 {407–418},
  numpages =	 12,
  keywords =	 {concurrent kernels, cuda, gpgpu},
  location =	 {Houston, Texas, USA},
  series =	 {ASPLOS '13}
}

@MISC{amd-kernel-object,
  author =	 {{AMD}},
  title =	 {{AMD HSA Code Object Format}},
  howpublished =
                  {"\url{https://rocmdocs.amd.com/en/latest/ROCm_Compiler_SDK/ROCm-Codeobj-format.html}"},
  year =	 2021,
}

@inproceedings{puthoor2018oversubscribed,
  author =	 {Puthoor, Sooraj and Tang, Xulong and Gross, Joseph
                  and Beckmann, Bradford M.},
  title =	 {{Oversubscribed Command Queues in GPUs}},
  booktitle =	 {{Proceedings of the 11th Workshop on General Purpose
                  GPUs}},
  series =	 {GPGPU-11},
  year =	 2018,
  isbn =	 {978-1-4503-5647-3},
  location =	 {Vienna, Austria},
  pages =	 {50--60},
  numpages =	 11,
  url =		 {http://doi.acm.org/10.1145/3180270.3180271},
  doi =		 {10.1145/3180270.3180271},
  acmid =	 3180271,
  publisher =	 {ACM},
  address =	 {New York, NY, USA},
}

@MISC{amd-cdna-arch,
  author =	 {{AMD}},
  title =	 {{AMD CDNA Architecture}},
  howpublished =
                  {"\url{https://www.amd.com/system/files/documents/amd-cdna-whitepaper.pdf}"},
  year =	 2020,
}

% mi210

@misc{amd-cdna2,
  author =	 {{AMD}},
  title =	 {{Introducing AMD CDNA 2 Architecture}},
  howpublished =
                  {\url{https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna2-white-paper.pdf}},
  year =	 2022,
}

@MISC{threadripper,
  author =	 {{AMD}},
  title =	 {{AMD Ryzen\texttrademark\ Threadripper 2950X
                  Processor}},
  howpublished =
                  {"\url{https://www.amd.com/en/products/cpu/amd-ryzen-threadripper-2950x}"},
  year =	 2019,
}

@MISC{mi100,
  author =	 {{AMD}},
  title =	 {{AMD Instinct\texttrademark\ MI100 Accelerator}},
  howpublished =
                  {"\url{https://www.amd.com/en/products/server-accelerators/instinct-mi100}"},
  year =	 2020,
}

@MISC{hip,
  author =	 {{AMD}},
  title =	 {{HIP : C++ Heterogeneous-Compute Interface for
                  Portability}},
  howpublished =
                  {"\url{https://gpuopen.com/compute-product/hip-convert-cuda-to-portable-c-code/}"},
  year =	 2019,
}

@MISC{hiptensorflow,
  author =	 {{AMD}},
  title =	 {{ROCm/HIP enabled Tensorflow}},
  howpublished = {"\url{https://github.com/soyers/hiptensorflow}"},
  year =	 2019,
}

@MISC{rocm,
  author =	 {{AMD}},
  title =	 {{ROCm, a New Era in Open GPU Computing}},
  howpublished = {"\url{https://rocm.github.io/}"},
  year =	 2019,
}

@misc{khan2019miopen,
  title =	 {{MIOpen: An Open Source Library For Deep Learning
                  Primitives}},
  author =	 {Jehandad Khan and Paul Fultz and Artem Tamazov and
                  Daniel Lowell and Chao Liu and Michael Melesse and
                  Murali Nandhimandalam and Kamil Nasyrov and Ilya
                  Perminov and Tejash Shah and Vasilii Filippov and
                  Jing Zhang and Jing Zhou and Bragadeesh Natarajan
                  and Mayank Daga},
  year =	 2019,
  eprint =	 {1910.00078},
  archivePrefix ={arXiv},
  primaryClass = {cs.LG},
}

@misc{miopen-git,
  author =	 {{AMD}},
  title =	 {{AMD's Machine Intelligence Library}},
  howpublished =
                  {"\url{https://github.com/ROCmSoftwarePlatform/MIOpen}"},
  year =	 2019,
}

@MISC{rocblas,
  author =	 {{AMD}},
  title =	 {{AMD's BLAS Library}},
  howpublished =
                  {"\url{https://github.com/ROCmSoftwarePlatform/rocBLAS}"},
  year =	 2019,
}

@MISC{rocprof-profiler,
  author =	 {{AMD}},
  title =	 {{AMD ROCm Profiler}},
  howpublished =
                  {"\url{https://rocmdocs.amd.com/en/latest/ROCm\_Tools/ROCm-Tools.html}"},
  year =	 2019,
}

@MISC{hbm2,
  author =	 {{JEDEC}},
  title =	 {{High Bandwidth Memory DRAM (HBM1, HBM2)}},
  howpublished =
                  {"\url{https://www.jedec.org/standards-documents/docs/jesd235a}"},
  year =	 2019,
}

@MISC{multi-instance-1,
  author =	 {{TIRIAS Research}},
  title =	 {{Why Your AI infrastructure Needs Both Training and
                  Inference}},
  howpublished = {"https://www.ibm.com/downloads/cas/QM4BYOPP"},
  year =	 2019,
}

@MISC{multi-instance-2,
  author =	 {{NVIDIA}},
  title =	 {{Easily Deploy Deep Learning Models in Production}},
  howpublished =
                  {"https://www.kdnuggets.com/2019/08/nvidia-deploy-deep-learning-models-production.html"},
  year =	 2019,
}

@MISC{multi-instance-3,
  author =	 {{NVIDIA}},
  title =	 {{Ride the Fast Lane to AI Productivity with
                  Multi-Instance GPUs}},
  howpublished =
                  {"https://blogs.nvidia.com/blog/2020/05/14/multi-instance-gpus/"},
  year =	 2020,
}

@misc{TPU2,
  author =	 {Alcorn, Paul},
  title =	 {{Hot Chips 2017: A Closer Look At Google's TPU v2}},
  howpublished =
                  {http://www.tomshardware.com/news/tpu-v2-google-machine-learning,35370.html},
  month =	 "September",
  year =	 2017,
}

@ARTICLE{NorriePatil2021-tpuV2V3,
  author =	 {Norrie, Thomas and Patil, Nishant and Yoon, Doe Hyun
                  and Kurian, George and Li, Sheng and Laudon, James
                  and Young, Cliff and Jouppi, Norman and Patterson,
                  David},
  journal =	 {IEEE Micro},
  title =	 {{The Design Process for Google's Training Chips:
                  TPUv2 and TPUv3}},
  year =	 2021,
  volume =	 41,
  number =	 2,
  pages =	 {56-63},
  keywords =	 {Training;Internet;Random access
                  memory;Hardware;Software engineering;Feeds},
  doi =		 {10.1109/MM.2021.3058217}
}

@inproceedings{JouppiYoung2017-tpu,
  author =	 {Jouppi, Norman P. and Young, Cliff and Patil,
                  Nishant and Patterson, David and Agrawal, Gaurav and
                  Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh
                  and Boden, Nan and Borchers, Al and Boyle, Rick and
                  Cantin, Pierre-luc and Chao, Clifford and Clark,
                  Chris and Coriell, Jeremy and Daley, Mike and Dau,
                  Matt and Dean, Jeffrey and Gelb, Ben and
                  Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and
                  Gulland, William and Hagmann, Robert and Ho,
                  C. Richard and Hogberg, Doug and Hu, John and Hundt,
                  Robert and Hurt, Dan and Ibarz, Julian and Jaffey,
                  Aaron and Jaworski, Alek and Kaplan, Alexander and
                  Khaitan, Harshit and Killebrew, Daniel and Koch,
                  Andy and Kumar, Naveen and Lacy, Steve and Laudon,
                  James and Law, James and Le, Diemthu and Leary,
                  Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin,
                  Alan and MacKean, Gordon and Maggiore, Adriana and
                  Mahony, Maire and Miller, Kieran and Nagarajan,
                  Rahul and Narayanaswami, Ravi and Ni, Ray and Nix,
                  Kathy and Norrie, Thomas and Omernick, Mark and
                  Penukonda, Narayana and Phelps, Andy and Ross,
                  Jonathan and Ross, Matt and Salek, Amir and
                  Samadiani, Emad and Severn, Chris and Sizikov,
                  Gregory and Snelham, Matthew and Souter, Jed and
                  Steinberg, Dan and Swing, Andy and Tan, Mercedes and
                  Thorson, Gregory and Tian, Bo and Toma, Horia and
                  Tuttle, Erick and Vasudevan, Vijay and Walter,
                  Richard and Wang, Walter and Wilcox, Eric and Yoon,
                  Doe Hyun},
  title =	 {{In-Datacenter Performance Analysis of a Tensor
                  Processing Unit}},
  booktitle =	 {{Proceedings of the 44th Annual International
                  Symposium on Computer Architecture}},
  series =	 {ISCA},
  year =	 2017,
  isbn =	 {978-1-4503-4892-8},
  location =	 {Toronto, ON, Canada},
  pages =	 {1--12},
  numpages =	 12,
  url =		 {http://doi.acm.org/10.1145/3079856.3080246},
  doi =		 {10.1145/3079856.3080246},
  acmid =	 3080246,
  publisher =	 {ACM},
  address =	 {New York, NY, USA},
  keywords =	 {CNN, DNN, GPU, LSTM, MLP, RNN, TPU, TensorFlow,
                  accelerator, deep learning, domain-specific
                  architecture, neural network},
}

@inproceedings{JouppiYoon2021-tpuv4,
  title =	 {{Ten Lessons from Three Generations Shaped Google's
                  TPUv4i}},
  author =	 {Jouppi, Norman P. and Yoon, Doe Hyun and Ashcraft,
                  Matthew and Gottscho, Mark and Jablin, Thomas B. and
                  Kurian, George and Laudon, James and Li, Sheng and
                  Ma, Peter and Ma, Xiaoyu and Patil, Nishant and
                  Prasad, Sushma and Young, Clifford and Zhou, Zongwei
                  and Patterson, David},
  booktitle =	 {{Proceedings of the 48th Annual International
                  Symposium on Computer Architecture}},
  series =	 {ISCA},
  year =	 2021,
}

@INPROCEEDINGS{BakhodaYuan2009-gpgpuSim,
  author =	 {Bakhoda, Ali and Yuan, George L. and Fung, Wilson
                  W. L. and Wong, Henry and Aamodt, Tor M.},
  booktitle =	 {{2009 IEEE International Symposium on Performance
                  Analysis of Systems and Software}},
  series =	 {ISPASS},
  title =	 {{Analyzing CUDA workloads using a detailed GPU
                  simulator}},
  year =	 2009,
  pages =	 {163-174},
  keywords =	 {cache storage;computer graphic equipment;instruction
                  sets;multiprocessing
                  systems;multi-threading;parallel architectures;CUDA
                  workload;GPU simulator;graphic processing
                  unit;flexible programming model;CUDA
                  programming;microarchitecture performance
                  simulator;parallel thread execution;virtual
                  instruction set;GPU hardware;high-end graphics
                  card;microarchitecture design;interconnect
                  topology;caches;memory controller;parallel workload
                  distribution;memory request coalescing
                  hardware;Analytical models;Yarn;Graphics;Parallel
                  processing;Microarchitecture;Hardware;Process
                  design;Concurrent computing;Parallel
                  programming;Computational modeling},
  doi =		 {10.1109/ISPASS.2009.4919648},
  ISSN =	 {null},
  month =	 {April},
}

@article{KhairyJain2018-voltaGPGPUSim,
  author =	 {Mahmoud Khairy and Akshay Jain and Tor M. Aamodt and
                  Timothy G. Rogers},
  title =	 {Exploring Modern {GPU} Memory System Design
                  Challenges through Accurate Modeling},
  journal =	 {CoRR},
  volume =	 {abs/1810.07269},
  year =	 2018,
  url =		 {http://arxiv.org/abs/1810.07269},
  archivePrefix ={arXiv},
  eprint =	 {1810.07269},
  timestamp =	 {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl =
                  {https://dblp.org/rec/journals/corr/abs-1810-07269.bib},
  bibsource =	 {dblp computer science bibliography,
                  https://dblp.org}
}

@article{RaihanGoli2018-gpgpusimTC,
  author =	 {Md Aamir Raihan and Negar Goli and Tor M. Aamodt},
  title =	 {{Modeling Deep Learning Accelerator Enabled GPUs}},
  journal =	 {CoRR},
  volume =	 {abs/1811.08309},
  year =	 2018,
  url =		 {http://arxiv.org/abs/1811.08309},
  archivePrefix ={arXiv},
  eprint =	 {1811.08309},
  timestamp =	 {Mon, 26 Nov 2018 12:52:45 +0100},
  biburl =
                  {https://dblp.org/rec/journals/corr/abs-1811-08309.bib},
  bibsource =	 {dblp computer science bibliography,
                  https://dblp.org}
}

@INPROCEEDINGS{KhairyShen2021-accelSim,
  author =	 {Khairy, Mahmoud and Shen, Zhesheng and Aamodt, Tor
                  M. and Rogers, Timothy G.},
  booktitle =	 {{2020 ACM/IEEE 47th Annual International Symposium
                  on Computer Architecture}},
  series =	 {ISCA},
  title =	 {{Accel-Sim: An Extensible Simulation Framework for
                  Validated GPU Modeling}},
  year =	 2020,
  pages =	 {473-486},
  doi =		 {10.1109/ISCA45697.2020.00047}
}

@misc{volta,
  title =	 {{{NVIDIA Tesla V100 GPU Architecture The World's
                  Most Advanced Data Center GPU}}},
  author =	 {{NVIDIA}},
  organization = {{NVIDIA}},
  year =	 2017,
  howpublished =
                  {{\url{http://www.nvidia.com/object/volta-architecture-whitepaper.html}
                  }}
}

@inproceedings{VaswaniShazeer17-attention,
  author =	 {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki
                  and Uszkoreit, Jakob and Jones, Llion and Gomez,
                  Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  title =	 {{Attention Is All You Need}},
  isbn =	 9781510860964,
  year =	 2017,
  publisher =	 {Curran Associates Inc.},
  address =	 {Red Hook, NY, USA},
  abstract =	 {The dominant sequence transduction models are based
                  on complex recurrent or convolutional neural
                  networks that include an encoder and a decoder. The
                  best performing models also connect the encoder and
                  decoder through an attention mechanism. We propose a
                  new simple network architecture, the Transformer,
                  based solely on attention mechanisms, dispensing
                  with recurrence and convolutions
                  entirely. Experiments on two machine translation
                  tasks show these models to be superior in quality
                  while being more parallelizable and requiring
                  significantly less time to train. Our model achieves
                  28.4 BLEU on the WMT 2014 English-to-German
                  translation task, improving over the existing best
                  results, including ensembles, by over 2 BLEU. On the
                  WMT 2014 English-to-French translation task, our
                  model establishes a new single-model
                  state-of-the-art BLEU score of 41.0 after training
                  for 3.5 days on eight GPUs, a small fraction of the
                  training costs of the best models from the
                  literature.},
  booktitle =	 {{Proceedings of the 31st International Conference on
                  Neural Information Processing Systems}},
  pages =	 {6000–6010},
  numpages =	 11,
  location =	 {Long Beach, California, USA},
  series =	 {NeurIPS},
}

@inproceedings{HestnessArdalani2019-dlChalls,
  author =	 {Hestness, Joel and Ardalani, Newsha and Diamos,
                  Gregory},
  title =	 {{Beyond Human-Level Accuracy: Computational
                  Challenges in Deep Learning}},
  year =	 2019,
  isbn =	 9781450362252,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/3293883.3295710},
  doi =		 {10.1145/3293883.3295710},
  booktitle =	 {{Proceedings of the 24th Symposium on Principles and
                  Practice of Parallel Programming}},
  pages =	 {1–14},
  numpages =	 14,
  keywords =	 {deep learning, model parallelism, neural networks,
                  compute requirements, data parallelism, compute
                  graph},
  location =	 {Washington, District of Columbia},
  series =	 {PPoPP ’19},
}

@misc{IvanovDryden2020-transformersDataMove,
  title =	 {{Data Movement Is All You Need: A Case Study on
                  Optimizing Transformers}},
  author =	 {Andrei Ivanov and Nikoli Dryden and Tal Ben-Nun and
                  Shigang Li and Torsten Hoefler},
  year =	 2020,
  eprint =	 {2007.00072},
  archivePrefix ={arXiv},
  primaryClass = {cs.LG}
}

@INPROCEEDINGS{ZadehPoulos2019-dlTime,
  author =	 {Zadeh, Ali Hadi and Poulos, Zissis and Moshovos,
                  Andreas},
  booktitle =	 {{IEEE International Symposium on Workload
                  Characterization}},
  series =	 {IISWC},
  title =	 {{Deep Learning Language Modeling Workloads: Where
                  Time Goes on Graphics Processors}},
  year =	 2019,
  pages =	 {131-142},
  organization = {IEEE},
  publisher =	 {IEEE Computer Society},
  address =	 {Washington, DC, USA},
}

@misc{ShoeybiPatwary2019-megatronlm,
  title =	 {{Megatron-LM: Training Multi-Billion Parameter
                  Language Models Using Model Parallelism}},
  author =	 {Mohammad Shoeybi and Mostofa Patwary and Raul Puri
                  and Patrick LeGresley and Jared Casper and Bryan
                  Catanzaro},
  year =	 2019,
  eprint =	 {1909.08053},
  archivePrefix ={arXiv},
  primaryClass = {cs.CL}
}

@inproceedings{LanChen2019-albert,
  author =	 {Lan, Zhenzhong and Chen, Mingda and Goodman,
                  Sebastian and Gimpel, Kevin and Sharma, Piyush and
                  Soricut, Radu},
  title =	 {{ALBERT: A Lite BERT for Self-supervised Learning of
                  Language Representations}},
  booktitle =	 {{Proceedings of the Seventh International Conference
                  on Learning Representation}},
  series =	 {ICLR},
  year =	 2019,
  numpages =	 17,
  publisher =	 {OpenReview.net},
}

@inproceedings{ZhuXia2020-bertNMT,
  author =	 {Zhu, Jinhua and Xia, Yingce and Wu, Lijun and He, Di
                  amd Qin, Tao and Zhou, Wengang and Li, Houqiang and
                  Liu, Tieyan},
  title =	 {{Incorporating BERT into Neural Machine
                  Translation}},
  year =	 2020,
  series =	 {ICLR},
  booktitle =	 {{Proceedings of the Eighth International Conference
                  on Learning Representation}},
}

@inproceedings{BahdanauCho2015-nmt,
  title =	 {{Neural Machine Translation by Jointly Learning to
                  Align and Translate}},
  author =	 {Bahdanau, Dzmitry and Cho, KyungHyun and Bengio,
                  Yoshua},
  year =	 2015,
  series =	 {ICLR},
  booktitle =	 {{Proceedings of the Third International Conference
                  on Learning Representation}},
}

@inproceedings{LuongPham2015-attentionNMT,
  title =	 "Effective Approaches to Attention-based Neural
                  Machine Translation",
  author =	 "Luong, Thang and Pham, Hieu and Manning, Christopher
                  D.",
  booktitle =	 "Proceedings of the 2015 Conference on Empirical
                  Methods in Natural Language Processing",
  series =	 {EMNLP},
  month =	 sep,
  year =	 2015,
  address =	 "Lisbon, Portugal",
  publisher =	 "Association for Computational Linguistics",
  url =		 "https://www.aclweb.org/anthology/D15-1166",
  doi =		 "10.18653/v1/D15-1166",
  pages =	 "1412--1421",
}

@inproceedings{ShenZhou2018-bidirAttention,
  title =	 {{Bi-Directional Block Self-Attention for Fast and
                  Memory-Efficient Sequence Modeling}},
  author =	 {Tao Shen and Tianyi Zhou and Guodong Long and Jing
                  Jiang and Chengqi Zhang},
  year =	 2018,
  booktitle =	 {{Proceedings of the Sixth International Conference
                  on Learning Representation}},
  series =	 {ICLR},
}

@inproceedings{kitaev2020reformer,
  title =	 {{Reformer: The Efficient Transformer}},
  author =	 {Nikita Kitaev and Łukasz Kaiser and Anselm Levskaya},
  year =	 2020,
  booktitle =	 {{Proceedings of the Eighth International Conference
                  on Learning Representation}},
  series =	 {ICLR},
}

@misc{correia2019adaptively,
  title =	 {{Adaptively Sparse Transformers}},
  author =	 {Gonçalo M. Correia and Vlad Niculae and André
                  F. T. Martins},
  year =	 2019,
  booktitle =	 {{Proceedings of the 2019 Conference on Empirical
                  Methods in Natural Language Processing and the 9th
                  International Joint Conference on Natural Language
                  Processing}},
  series =	 {EMNLP-IJCNP},
}

@misc{gpt3-github,
  title =	 {{GPT-3: Language Models are Few-Shot Learners}},
  howpublished = {\url{https://github.com/openai/gpt-3}},
  author =	 {OpenAI},
  year =	 2020,
}

@misc{gpt2-github,
  title =	 {{gpt-2 Github}},
  author =	 {OpenAI},
  howpublished = {\url{https://github.com/openai/gpt-2}},
  year =	 2019,
}

@misc{megatron-github,
  author =	 {NVIDIA},
  title =	 {{Megatron-LM Github}},
  year =	 2018,
  howpublished = {\url{https://github.com/NVIDIA/Megatron-LM}},
}

@misc{ZhuPhanishayee2020-daydream,
  title =	 {{Daydream: Accurately Estimating the Efficacy of
                  Optimizations for DNN Training}},
  author =	 {Hongyu Zhu and Amar Phanishayee and Gennady
                  Pekhimenko},
  year =	 2020,
  eprint =	 {2006.03318},
  archivePrefix ={arXiv},
  primaryClass = {cs.DC}
}

@inproceedings{VillaStephenson2019-nvbit,
  author =	 {Villa, Oreste and Stephenson, Mark and Nellans,
                  David and Keckler, Stephen W.},
  title =	 {{NVBit: A Dynamic Binary Instrumentation Framework
                  for NVIDIA GPUs}},
  year =	 2019,
  isbn =	 9781450369381,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/3352460.3358307},
  doi =		 {10.1145/3352460.3358307},
  booktitle =	 {{Proceedings of the 52nd Annual IEEE/ACM
                  International Symposium on Microarchitecture}},
  pages =	 {372–383},
  numpages =	 12,
  keywords =	 {Dynamic binary instrumentation, GPGPU, GPU
                  computing, CUDA},
  location =	 {Columbus, OH, USA},
  series =	 {MICRO ’52}
}

@inproceedings{NarayananHarlap2019-pipedream,
  author =	 {Narayanan, Deepak and Harlap, Aaron and Phanishayee,
                  Amar and Seshadri, Vivek and Devanur, Nikhil R. and
                  Ganger, Gregory R. and Gibbons, Phillip B. and
                  Zaharia, Matei},
  title =	 {{PipeDream: Generalized Pipeline Parallelism for DNN
                  Training}},
  year =	 2019,
  isbn =	 9781450368735,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/3341301.3359646},
  doi =		 {10.1145/3341301.3359646},
  booktitle =	 {{Proceedings of the 27th ACM Symposium on Operating
                  Systems Principles}},
  pages =	 {1–15},
  numpages =	 15,
  location =	 {Huntsville, Ontario, Canada},
  series =	 {SOSP ’19}
}

@misc{LiZheng2020-horizFus,
  title =	 {{Automatic Horizontal Fusion for GPU Kernels}},
  author =	 {Ao Li and Bojian Zheng and Gennady Pekhimenko and
                  Fan Long},
  year =	 2020,
  eprint =	 {2007.01277},
  archivePrefix ={arXiv},
  primaryClass = {cs.DC}
}

@inproceedings{SivathanuChugh2019-astra,
  author =	 {Sivathanu, Muthian and Chugh, Tapan and Singapuram,
                  Sanjay S. and Zhou, Lidong},
  title =	 {{Astra: Exploiting Predictability to Optimize Deep
                  Learning}},
  year =	 2019,
  isbn =	 9781450362405,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/3297858.3304072},
  doi =		 {10.1145/3297858.3304072},
  booktitle =	 {{Proceedings of the Twenty-Fourth International
                  Conference on Architectural Support for Programming
                  Languages and Operating Systems}},
  pages =	 {909–923},
  numpages =	 15,
  keywords =	 {domain-specific compiler, deep learning, adaptation},
  location =	 {Providence, RI, USA},
  series =	 {ASPLOS ’19}
}

@inproceedings{SpringerWauligmann2017-fuseGPULang,
  author =	 {Springer, Matthias and Wauligmann, Peter and
                  Masuhara, Hidehiko},
  title =	 {{Modular Array-Based GPU Computing in a
                  Dynamically-Typed Language}},
  year =	 2017,
  isbn =	 9781450350693,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/3091966.3091974},
  doi =		 {10.1145/3091966.3091974},
  booktitle =	 {{Proceedings of the 4th ACM SIGPLAN International
                  Workshop on Libraries, Languages, and Compilers for
                  Array Programming}},
  pages =	 {48–55},
  numpages =	 8,
  keywords =	 {kernel fusion, Ruby, GPGPU, CUDA},
  location =	 {Barcelona, Spain},
  series =	 {ARRAY 2017}
}

@article{FilipovivcMadzin2015-blasCUDAFusion,
  author =	 {Filipovi\v{c}, Ji\v{r}\'{\i} and Madzin,
                  Mat\'{u}\v{s} and Fousek, Jan and Matyska,
                  Ludundefinedk},
  title =	 {{Optimizing CUDA Code by Kernel Fusion: Application
                  on BLAS}},
  year =	 2015,
  issue_date =	 {October 2015},
  publisher =	 {Kluwer Academic Publishers},
  address =	 {USA},
  volume =	 71,
  number =	 10,
  issn =	 {0920-8542},
  url =		 {https://doi.org/10.1007/s11227-015-1483-z},
  doi =		 {10.1007/s11227-015-1483-z},
  journal =	 {{The Journal of Supercomputing}},
  month =	 oct,
  pages =	 {3934–3957},
  numpages =	 24,
  keywords =	 {BLAS, GPGPU, Kernel fusion, Code generation, CUDA}
}

@article{FousekFilipovivc2011-fuseGPUMap,
  author =	 {Fousek, Jan and Filipovi\v{c}, Ji\v{r}i and Madzin,
                  Matu\v{s}},
  title =	 {{Automatic Fusions of CUDA-GPU Kernels for Parallel
                  Map}},
  year =	 2011,
  issue_date =	 {September 2011},
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  volume =	 39,
  number =	 4,
  issn =	 {0163-5964},
  url =		 {https://doi.org/10.1145/2082156.2082183},
  doi =		 {10.1145/2082156.2082183},
  journal =	 {SIGARCH Comput. Archit. News},
  month =	 dec,
  pages =	 {98–99},
  numpages =	 2
}

@inproceedings{WangLin2010-gpuKernelFusion,
  author =	 {Wang, Guibin and Lin, YiSong and Yi, Wei},
  title =	 {{Kernel Fusion: An Effective Method for Better Power
                  Efficiency on Multithreaded GPU}},
  year =	 2010,
  isbn =	 9780769543314,
  publisher =	 {IEEE Computer Society},
  address =	 {USA},
  url =		 {https://doi.org/10.1109/GreenCom-CPSCom.2010.102},
  doi =		 {10.1109/GreenCom-CPSCom.2010.102},
  booktitle =	 {{Proceedings of the 2010 IEEE/ACM Int’l Conference
                  on Green Computing and Communications \& Int’l
                  Conference on Cyber, Physical and Social Computing}},
  pages =	 {344–350},
  numpages =	 7,
  keywords =	 {Kernel Fusion, Power Efficiency, GPGPU, Power
                  Optimization},
  series =	 {GREENCOM-CPSCOM ’10}
}

@misc{cuDNN,
  title =	 {{NVIDIA cuDNN: GPU Accelerated Deep Learning}},
  author =	 {NVIDIA},
  year =	 2018,
  howpublished = {\url{https://developer.nvidia.com/cudnn}},
}

@article{Appleyard2016-optRNNGPU,
  author =	 {Jeremy Appleyard and Tom{\'{a}}s Kocisk{\'{y}} and
                  Phil Blunsom},
  title =	 {{Optimizing Performance of Recurrent Neural Networks
                  on GPUs}},
  journal =	 {{CoRR}},
  volume =	 {abs/1604.01946},
  year =	 2016,
  url =		 {http://arxiv.org/abs/1604.01946},
}

@misc{MehtaGhazvininejad2020-delight,
  title =	 {{DeLighT: Very Deep and Light-weight Transformer}},
  author =	 {Sachin Mehta and Marjan Ghazvininejad and Srinivasan
                  Iyer and Luke Zettlemoyer and Hannaneh Hajishirzi},
  year =	 2020,
  eprint =	 {2008.00623},
  archivePrefix ={arXiv},
  primaryClass = {cs.LG}
}

@inproceedings{MehtaKoncel2019-define,
  title =	 {{DeFINE: Deep Factorized Input Token Embeddings for
                  Neural Sequence Modeling}},
  author =	 {Mehta, Sachin and Koncel-Kedziorski, Rik and
                  Rastegari, Mohammad and Hajishirzi, Hannaneh},
  booktitle =	 {{International Conference on Learning
                  Representations}},
  series =	 {ICLR},
  year =	 2019
}

@misc{delight-github,
  title =	 {{DeLighT: Very Deep and Light-weight Transformers}},
  author =	 {Mehta, Sachin},
  howpublished = {\url{https://github.com/sacmehta/delight}},
  year =	 2020,
}

@misc{ampere,
  title =	 {{NVIDIA Ampere Architecture In-Depth}},
  author =	 {Krashinsky, Ronny and Giroux, Olivier and Jones,
                  Stephen and Stam, Nick and Ramaswamy, Sridhar},
  year =	 2020,
  month =	 {Month},
  howpublished =
                  {\url{https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/}},
}

@inproceedings{ChenMoreau2018-tvm,
  author =	 {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng
                  and Zheng, Lianmin and Yan, Eddie and Cowan, Meghan
                  and Shen, Haichen and Wang, Leyuan and Hu, Yuwei and
                  Ceze, Luis and Guestrin, Carlos and Krishnamurthy,
                  Arvind},
  title =	 {{TVM: An Automated End-to-End Optimizing Compiler
                  for Deep Learning}},
  year =	 2018,
  isbn =	 9781931971478,
  publisher =	 {USENIX Association},
  address =	 {USA},
  booktitle =	 {{Proceedings of the 13th USENIX Conference on
                  Operating Systems Design and Implementation}},
  pages =	 {579–594},
  numpages =	 16,
  location =	 {Carlsbad, CA, USA},
  series =	 {OSDI’18}
}

@misc{YangDai2019-xlnet,
  title =	 {{XLNet: Generalized Autoregressive Pretraining for
                  Language Understanding}},
  author =	 {Zhilin Yang and Zihang Dai and Yiming Yang and Jaime
                  Carbonell and Ruslan Salakhutdinov and Quoc V. Le},
  year =	 2019,
  eprint =	 {1906.08237},
  archivePrefix ={arXiv},
  primaryClass = {cs.CL}
}

@misc{xlnet-github,
  title =	 {{XLNet Github}},
  howpublished = {\url{https://github.com/zihangdai/xlnet}},
  year =	 2019,
  author =	 {Dai, Zihang},
}

@misc{LiuOtt2019-roberta,
  title =	 {{RoBERTa: A Robustly Optimized BERT Pretraining
                  Approach}},
  author =	 {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei
                  Du and Mandar Joshi and Danqi Chen and Omer Levy and
                  Mike Lewis and Luke Zettlemoyer and Veselin
                  Stoyanov},
  year =	 2019,
  eprint =	 {1907.11692},
  archivePrefix ={arXiv},
  primaryClass = {cs.CL}
}

@misc{multiWeekGPUTraining,
  title =	 {{AI and Compute}},
  month =	 {May},
  howpublished = {\url{https://openai.com/blog/ai-and-compute/}},
  author =	 {Amodei, Dario and Hernandez, Danny and Sastry,
                  Girish and Clark, Jack and Brockman, Greg and
                  Sutskever, Ilya},
  year =	 2018,
}

@inproceedings{HuangCheng2019-gpipe,
  author =	 {Yanping Huang and Youlong Cheng and Ankur Bapna and
                  Orhan Firat and Mia Xu Chen and Dehao Chen and
                  HyoukJoong Lee and Jiquan Ngiam and Quoc V. Le and
                  Yonghui Wu and Zhifeng Chen},
  title =	 {{GPipe: Efficient Training of Giant Neural Networks
                  using Pipeline Parallelism}},
  booktitle =	 {{Proceedings of the 33rd International Conference on
                  Neural Information Processing Systems}},
  series =	 {{NeurIPS}},
  year =	 2019,
}

@misc{LinLi2020-bertMultiNode,
  title =	 {{Multi-node BERT-pretraining: Cost-efficient
                  Approach}},
  author =	 {Jiahuang Lin and Xin Li and Gennady Pekhimenko},
  year =	 2020,
  eprint =	 {2008.00177},
  archivePrefix ={arXiv},
  primaryClass = {cs.LG}
}

@article{kalamkar2019bfloat16,
  author =	 {Dhiraj D. Kalamkar and Dheevatsa Mudigere and Naveen
                  Mellempudi and Dipankar Das and Kunal Banerjee and
                  Sasikanth Avancha and Dharma Teja Vooturi and
                  Nataraj Jammalamadaka and Jianyu Huang and Hector
                  Yuen and Jiyan Yang and Jongsoo Park and Alexander
                  Heinecke and Evangelos Georganas and Sudarshan
                  Srinivasan and Abhisek Kundu and Misha Smelyanskiy
                  and Bharat Kaul and Pradeep Dubey},
  title =	 {A Study of {BFLOAT16} for Deep Learning Training},
  journal =	 {CoRR},
  volume =	 {abs/1905.12322},
  year =	 2019,
}

@misc{YangZhang2019-pipemare,
  title =	 {{PipeMare: Asynchronous Pipeline Parallel DNN
                  Training}},
  author =	 {Bowen Yang and Jian Zhang and Jonathan Li and
                  Christopher Ré and Christopher R. Aberger and
                  Christopher De Sa},
  year =	 2019,
  eprint =	 {1910.05124},
  archivePrefix ={arXiv},
  primaryClass = {cs.DC}
}

@inproceedings{QinyiHe2020-prague,
  author =	 {Luo, Qinyi and He, Jiaao and Zhuo, Youwei and Qian,
                  Xuehai},
  title =	 {{Prague: High-Performance Heterogeneity-Aware
                  Asynchronous Decentralized Training}},
  year =	 2020,
  isbn =	 9781450371025,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/3373376.3378499},
  doi =		 {10.1145/3373376.3378499},
  booktitle =	 {{Proceedings of the Twenty-Fifth International
                  Conference on Architectural Support for Programming
                  Languages and Operating Systems}},
  pages =	 {401–416},
  numpages =	 16,
  keywords =	 {heterogeneity, deep learning, decentralized
                  training, machine learning},
  location =	 {Lausanne, Switzerland},
  series =	 {ASPLOS ’20}
}

@INPROCEEDINGS{HamJung2020-a3,
  author =	 {Ham, Tae Jun and Jung, Sung Jun and Kim, Seonghak
                  and Park, Yeonhong and Oh, Young H. and Song, Yoon
                  Ho and Park, Junghoon and Lee, Sanghee and Park,
                  Kyoung and Lee, Jae W. and Jeong, Deog-Kyoon},
  booktitle =	 {{26th IEEE International Symposium on High
                  Performance Computer Architecture}},
  title =	 {{A3: Accelerating Attention Mechanisms in Neural
                  Networks with Approximation}},
  year =	 2020,
  series =	 {HPCA},
  month =	 {Feb},
}

@misc{RaihanAamodt2020-swat,
  title =	 {{Sparse Weight Activation Training}},
  author =	 {Md Aamir Raihan and Tor M. Aamodt},
  year =	 2020,
  eprint =	 {2001.01969},
  archivePrefix ={arXiv},
  primaryClass = {cs.LG}
}

@article{PatiAga2021-demystifying,
  title =	 {{Demystifying BERT: Implications for Accelerator
                  Design}},
  author =	 {Suchita Pati and Shaizeen Aga and Nuwan Jayasena and
                  Matthew D. Sinclair},
  year =	 2021,
  eprint =	 {2104.08335},
  journal =	 {CoRR},
  volume =	 {abs/2104.08335},
  archivePrefix ={arXiv},
  primaryClass = {cs.AR}
}

@INPROCEEDINGS{PatiAga2022-demystifying,
  title =	 {{Demystifying BERT: System Design Implications}},
  author =	 {Suchita Pati and Shaizeen Aga and Nuwan Jayasena and
                  Matthew D. Sinclair},
  year =	 2021,
  booktitle =	 {{IEEE International Symposium on Workload
                  Characterization}},
  series =	 {{IISWC}},
}

@INPROCEEDINGS{BruceAkram2021-gem5art,
  author =	 {Bruce, Bobby R. and Akram, Ayaz and Nguyen, Hoa and
                  Roarty, Kyle and Samani, Mahyar and Fariborz, Marjan
                  and Reddy, Trivikram and Sinclair, Matthew D. and
                  Lowe-Power, Jason},
  booktitle =	 {{IEEE International Symposium on Performance
                  Analysis of Systems and Software}},
  series =	 {ISPASS},
  title =	 {{Enabling Reproducible and Agile Full-System
                  Simulation}},
  year =	 2021,
}

@inproceedings{KuperPati2021-gpuUtil,
  title =	 {{Improving GPU Utilization in ML Workloads Through
                  Finer-Grained Synchronization}},
  author =	 {Kuper, Reese and Pati, Suchita and Sinclair, Matthew
                  D.},
  booktitle =	 {{3rd Young Architects Workshop}},
  series =	 {YArch},
  month =	 {April},
  year =	 2021,
}

@INPROCEEDINGS{LeBeanePotter2016-taskQueue,
  author =	 {LeBeane, Michael and Potter, Brandon and Pan,
                  Abhisek and Dutu, Alexandru and Agarwala, Vinay and
                  Lee, Wonchan and Majeti, Deepak and Ghimire, Bibek
                  and Tassell, Eric Van and Wasmundt, Samuel and
                  Benton, Brad and Breternitz, Mauricio and Chu,
                  Michael L. and Thottethodi, Mithuna and John, Lizy
                  K. and Reinhardt, Steven K.},
  booktitle =	 {{Proceedings of the International Conference for
                  High Performance Computing, Networking, Storage and
                  Analysis}},
  series =	 {SC},
  title =	 {{Extended Task Queuing: Active Messages for
                  Heterogeneous Systems}},
  year =	 2016,
  pages =	 {933-944},
  doi =		 {10.1109/SC.2016.79}
}

@inproceedings{LeBeaneHamidouche2018-cpNet,
  author =	 {LeBeane, Michael and Hamidouche, Khaled and Benton,
                  Brad and Breternitz, Mauricio and Reinhardt, Steven
                  K. and John, Lizy K.},
  title =	 {{ComP-Net: Command Processor Networking for
                  Efficient Intra-Kernel Communications on GPUs}},
  year =	 2018,
  isbn =	 9781450359863,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/3243176.3243179},
  doi =		 {10.1145/3243176.3243179},
  abstract =	 {Current state-of-the-art in GPU networking advocates
                  a host-centric model that reduces performance and
                  increases code complexity. Recently, researchers
                  have explored several techniques for networking
                  within a GPU kernel itself. These approaches,
                  however, suffer from high latency, waste energy on
                  the host, and are not scalable with larger/more GPUs
                  on a node. In this work, we introduce Command
                  Processor Networking (ComP-Net), which leverages the
                  availability of scalar cores integrated on the GPU
                  itself to provide high-performance intra-kernel
                  networking. ComP-Net enables efficient
                  synchronization between the Command Processors and
                  Compute Units on the GPU through a line locking
                  scheme implemented in the GPU's shared last-level
                  cache. We illustrate that ComP-Net can improve
                  application performance by up to 20\% and provide up
                  to 50\% reduction in energy consumption
                  vs. competing networking techniques across a Jacobi
                  stencil, allreduce collective, and machine learning
                  applications.},
  booktitle =	 {{Proceedings of the 27th International Conference on
                  Parallel Architectures and Compilation Techniques}},
  articleno =	 29,
  numpages =	 13,
  keywords =	 {GPUs, programming models, RDMA networks},
  location =	 {Limassol, Cyprus},
  series =	 {PACT '18}
}

@misc{nvidia-stream,
  author =	 {NVIDIA},
  title =	 {NVIDIA, CUDA Stream Management},
  year =	 2018,
  url =
                  {http://developer.download.nvidia.com/compute/cuda/2_3/toolkit/docs/online/group__CUDART__STREAM.html},
  urldate =	 {2019-06-01}
}

@misc{nvidia-stream2,
  author =	 {Justin Luitjens},
  title =	 {CUDA Streams: Best Practices and Common Pitfalls},
  year =	 2014,
  series =	 {GPU Technology Conference},
}

@misc{amd-hip,
  author =	 {AMD},
  title =	 {{HIP: Heterogeneous-computing Interface for
                  Portability}},
  year =	 2018,
  url =		 {https://github.com/ROCm-Developer-Tools/HIP/},
  urldate =	 {2019-06-01}
}

@misc{amd-white,
  author =	 {AMD},
  title =	 {{AMD’s Asynchronous Shaders White Paper}},
  year =	 2012,
  url =
                  {https://developer.amd.com/wordpress/media/2012/10/Asynchronous-Shaders-White-Paper-FINAL.pdf},
  urldate =	 {2019-06-01}
}

@inproceedings{amd-dag,
  author =	 {Puthoor, Sooraj and Aji, Ashwin M. and Che, Shuai
                  and Daga, Mayank and Wu, Wei and Beckmann, Bradford
                  M. and Rodgers, Gregory},
  title =	 {{Implementing Directed Acyclic Graphs with the
                  Heterogeneous System Architecture}},
  booktitle =	 {{Proceedings of the 9th Annual Workshop on General
                  Purpose Processing Using Graphics Processing Unit}},
  series =	 {GPGPU '16},
  year =	 2016,
  isbn =	 {978-1-4503-4195-0},
  location =	 {Barcelona, Spain},
  pages =	 {53--62},
  numpages =	 10,
  url =		 {http://doi.acm.org/10.1145/2884045.2884052},
  doi =		 {10.1145/2884045.2884052},
  acmid =	 2884052,
  publisher =	 {ACM},
  address =	 {New York, NY, USA},
  keywords =	 {APU, directed acyclic graph (DAG), fine-grain task
                  management, heterogeneous system architecture (HSA)},
}

@misc{nvidia-hyperq,
  title =	 {{CUDA HyperQ Example}},
  author =	 {NVIDIA},
  year =	 2013,
  howpublished =
                  {\url{http://developer.download.nvidia.com/compute/DevZone/C/html_x64/6_Advanced/simpleHyperQ/doc/HyperQ.pdf}},
}

@article{nvidia_tesla,
  title =	 {{{NVIDIA Tesla}: A Unified Graphics and Computing
                  Architecture}},
  author =	 {Lindholm, E. and Nickolls, J. and Oberman, S. and
                  Montrym, J.},
  journal =	 {IEEE Micro},
  year =	 2008,
  month =	 {March-April},
  volume =	 28,
  number =	 2,
  pages =	 {39-55},
}

@inproceedings{RoartySinclair2020-gem5GPU,
  title =	 {{Modeling Modern GPU Applications in gem5}},
  author =	 {Roarty, Kyle and Sinclair, Matthew D.},
  year =	 2020,
  month =	 {June},
  booktitle =	 {{3rd gem5 Users' Workshop}},
}

@article{LowePowerAhmad2020-gem520,
  title =	 {The gem5 Simulator: Version 20.0+},
  author =	 {Jason Lowe-Power and Abdul Mutaal Ahmad and Ayaz
                  Akram and Mohammad Alian and Rico Amslinger and
                  Matteo Andreozzi and Adrià Armejach and Nils
                  Asmussen and Srikant Bharadwaj and Gabe Black and
                  Gedare Bloom and Bobby R. Bruce and Daniel Rodrigues
                  Carvalho and Jeronimo Castrillon and Lizhong Chen
                  and Nicolas Derumigny and Stephan Diestelhorst and
                  Wendy Elsasser and Marjan Fariborz and Amin
                  Farmahini-Farahani and Pouya Fotouhi and Ryan
                  Gambord and Jayneel Gandhi and Dibakar Gope and
                  Thomas Grass and Bagus Hanindhito and Andreas
                  Hansson and Swapnil Haria and Austin Harris and
                  Timothy Hayes and Adrian Herrera and Matthew
                  Horsnell and Syed Ali Raza Jafri and Radhika Jagtap
                  and Hanhwi Jang and Reiley Jeyapaul and Timothy
                  M. Jones and Matthias Jung and Subash Kannoth and
                  Hamidreza Khaleghzadeh and Yuetsu Kodama and Tushar
                  Krishna and Tommaso Marinelli and Christian Menard
                  and Andrea Mondelli and Tiago Mück and Omar Naji and
                  Krishnendra Nathella and Hoa Nguyen and Nikos
                  Nikoleris and Lena E. Olson and Marc Orr and Binh
                  Pham and Pablo Prieto and Trivikram Reddy and Alec
                  Roelke and Mahyar Samani and Andreas Sandberg and
                  Javier Setoain and Boris Shingarov and Matthew
                  D. Sinclair and Tuan Ta and Rahul Thakur and Giacomo
                  Travaglini and Michael Upton and Nilay Vaish and
                  Ilias Vougioukas and Zhengrong Wang and Norbert Wehn
                  and Christian Weis and David A. Wood and Hongil Yoon
                  and Éder F. Zulian},
  year =	 2020,
  journal =	 {CoRR},
  volume =	 {abs/2007.03152},
  eprint =	 {2007.03152},
  archivePrefix ={arXiv},
  primaryClass = {cs.AR}
}

@inproceedings{BrownMann2020-gpt3,
  author =	 {Brown, Tom and Mann, Benjamin and Ryder, Nick and
                  Subbiah, Melanie and Kaplan, Jared D and Dhariwal,
                  Prafulla and Neelakantan, Arvind and Shyam, Pranav
                  and Sastry, Girish and Askell, Amanda and Agarwal,
                  Sandhini and Herbert-Voss, Ariel and Krueger,
                  Gretchen and Henighan, Tom and Child, Rewon and
                  Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey
                  and Winter, Clemens and Hesse, Chris and Chen, Mark
                  and Sigler, Eric and Litwin, Mateusz and Gray, Scott
                  and Chess, Benjamin and Clark, Jack and Berner,
                  Christopher and McCandlish, Sam and Radford, Alec
                  and Sutskever, Ilya and Amodei, Dario},
  booktitle =	 {{Advances in Neural Information Processing Systems}},
  editor =	 {H. Larochelle and M. Ranzato and R. Hadsell and
                  M. F. Balcan and H. Lin},
  pages =	 {1877--1901},
  publisher =	 {Curran Associates, Inc.},
  address =	 {Red Hook, NY, USA},
  title =	 {{Language Models are Few-Shot Learners}},
  url =
                  {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  volume =	 33,
  year =	 2020,
  series =	 {NeurIPS},
}

@article{RadfordWu2019-gpt2,
  title =	 {Language Models are Unsupervised Multitask Learners},
  author =	 {Radford, Alec and Wu, Jeff and Child, Rewon and
                  Luan, David and Amodei, Dario and Sutskever, Ilya},
  year =	 2019,
  journal =	 {OpenAI Blog},
  volume =	 1,
  number =	 8,
}

@article{Microsoft2020-tnlg,
  title =	 {Turing-NLG: A 17-billion-parameter language model by
                  Microsoft},
  author =	 {Microsoft},
  year =	 2020,
  journal =	 {Microsoft Research Blog},
  url =
                  {https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/},
  volume =	 1,
  number =	 8,
}

@article{BanburyReddi2021-tinyMLPerf,
  author =	 {Colby R. Banbury and Vijay Janapa Reddi and Peter
                  Torelli and Jeremy Holleman and Nat Jeffries and
                  Csaba Kir{\'{a}}ly and Pietro Montino and David
                  Kanter and Sebastian Ahmed and Danilo Pau and Urmish
                  Thakker and Antonio Torrini and Pete Warden and Jay
                  Cordaro and Giuseppe Di Guglielmo and Javier
                  M. Duarte and Stephen Gibellini and Videet Parekh
                  and Honson Tran and Nhan Tran and Wenxu Niu and
                  Xuesong Xu},
  title =	 {MLPerf Tiny Benchmark},
  journal =	 {CoRR},
  volume =	 {abs/2106.07597},
  year =	 2021,
  url =		 {https://arxiv.org/abs/2106.07597},
  archivePrefix ={arXiv},
  eprint =	 {2106.07597},
  timestamp =	 {Wed, 16 Jun 2021 10:42:19 +0200},
  biburl =
                  {https://dblp.org/rec/journals/corr/abs-2106-07597.bib},
  bibsource =	 {dblp computer science bibliography,
                  https://dblp.org}
}

@ARTICLE{ReddiCheng2021-mlPerfVision,
  author =	 {Reddi, Vijay Janapa and Cheng, Christine and Kanter,
                  David and Mattson, Peter and Schmuelling, Guenther
                  and Wu, Carole-Jean},
  journal =	 {IEEE Micro},
  title =	 {{The Vision Behind MLPerf: Understanding AI
                  Inference Performance}},
  year =	 2021,
  volume =	 41,
  number =	 3,
  pages =	 {10-18},
  doi =		 {10.1109/MfM.2021.3066343}
}

% - L. Ke, et  al.  RecNMP:  Accelerating  Personalized  Recommendation with Near-Memory Processing. In ISCA, pp. 790-803, 2020.
% - LAX https://pages.cs.wisc.edu/~sinclair/papers/yeh-lax-hpca21.pdf
% - Batch-Maker P. Gao, et  al.  Low  latency  RNN  inference  with  cellular  batching, inEuroSys, pp. 1-15, 2018.
% - Prema
% - Successor of Prema from Minsoo: LazyBatching

@inproceedings{GaoYu2018-batchmaker,
  author =	 {Gao, Pin and Yu, Lingfan and Wu, Yongwei and Li,
                  Jinyang},
  title =	 {Low Latency RNN Inference with Cellular Batching},
  booktitle =	 {{Proceedings of the Thirteenth EuroSys Conference}},
  series =	 {EuroSys '18},
  year =	 2018,
  isbn =	 {978-1-4503-5584-1},
  location =	 {Porto, Portugal},
  pages =	 {31:1--31:15},
  articleno =	 31,
  numpages =	 15,
  url =		 {http://doi.acm.org/10.1145/3190508.3190541},
  doi =		 {10.1145/3190508.3190541},
  acmid =	 3190541,
  publisher =	 {ACM},
  address =	 {New York, NY, USA},
  keywords =	 {batching, dataflow graph, inference, recurrent
                  neural network},
}

@INPROCEEDINGS{YehSinclair2021-lax,
  author =	 {Yeh, Tsung Tai and Sinclair, Matthew D. and
                  Beckmann, Bradford M. and Rogers, Timothy G.},
  booktitle =	 {{27th IEEE International Symposium on High
                  Performance Computer Architecture}},
  series =	 {HPCA},
  title =	 {{Deadline-Aware Offloading for High-Throughput
                  Accelerators}},
  year =	 2021,
  pages =	 {479-492},
  doi =		 {10.1109/HPCA51647.2021.00048}
}

@INPROCEEDINGS{ChoiRhu2020-prema,
  author =	 {Choi, Yujeong and Rhu, Minsoo},
  booktitle =	 {{26th IEEE International Symposium on High
                  Performance Computer Architecture}},
  series =	 {HPCA},
  title =	 {{PREMA: A Predictive Multi-Task Scheduling Algorithm
                  For Preemptible Neural Processing Units}},
  year =	 2020,
  pages =	 {220-233},
  doi =		 {10.1109/HPCA47549.2020.00027}
}

@INPROCEEDINGS{ChoiKim2021-lazyBatching,
  author =	 {Y. Choi and Y. Kim and M. Rhu},
  booktitle =	 {{27th IEEE International Symposium on High
                  Performance Computer Architecture}},
  series =	 {HPCA},
  title =	 {{Lazy Batching: An SLA-aware Batching System for
                  Cloud Machine Learning Inference}},
  year =	 2021,
  pages =	 {493-506},
  keywords =	 {training;quality of service;machine
                  learning;computer architecture;throughput;time
                  factors},
  doi =		 {10.1109/HPCA51647.2021.00049},
  url =
                  {https://doi.ieeecomputersociety.org/10.1109/HPCA51647.2021.00049},
  publisher =	 {IEEE Computer Society},
  address =	 {Los Alamitos, CA, USA},
  month =	 {mar}
}

@inproceedings{KeGupta2020-recNMP,
  author =	 {Ke, Liu and Gupta, Udit and Cho, Benjamin Youngjae
                  and Brooks, David and Chandra, Vikas and Diril, Utku
                  and Firoozshahian, Amin and Hazelwood, Kim and Jia,
                  Bill and Lee, Hsien-Hsin S. and Li, Meng and Maher,
                  Bert and Mudigere, Dheevatsa and Naumov, Maxim and
                  Schatz, Martin and Smelyanskiy, Mikhail and Wang,
                  Xiaodong and Reagen, Brandon and Wu, Carole-Jean and
                  Hempstead, Mark and Zhang, Xuan},
  title =	 {{RecNMP: Accelerating Personalized Recommendation
                  with near-Memory Processing}},
  year =	 2020,
  isbn =	 9781728146614,
  publisher =	 {IEEE Press},
  url =		 {https://doi.org/10.1109/ISCA45697.2020.00070},
  abstract =	 {Personalized recommendation systems leverage deep
                  learning models and account for the majority of data
                  center AI cycles. Their performance is dominated by
                  memory-bound sparse embedding operations with unique
                  irregular memory access patterns that pose a
                  fundamental challenge to accelerate. This paper
                  proposes a lightweight, commodity DRAM compliant,
                  near-memory processing solution to accelerate
                  personalized recommendation inference. The in-depth
                  characterization of production-grade recommendation
                  models shows that embedding operations with high
                  model-, operator- and data-level parallelism lead to
                  memory bandwidth saturation, limiting recommendation
                  inference performance.  We propose RecNMP which
                  provides a scalable solution to improve system
                  throughput, supporting a broad range of sparse
                  embedding models. RecNMP is specifically tailored to
                  production environments with heavy co-location of
                  operators on a single server.  Several
                  hardware/software co-optimization techniques such as
                  memory-side caching, table-aware packet scheduling,
                  and hot entry profiling are studied, providing up to
                  9.8x memory latency speedup over a highly-optimized
                  baseline. Overall, RecNMP offers 4.2x throughput
                  improvement and 45.8\% memory energy savings.},
  booktitle =	 {{Proceedings of the ACM/IEEE 47th Annual
                  International Symposium on Computer Architecture}},
  series =	 {ISCA},
  pages =	 {790–803},
  numpages =	 14
}

@article{MattsonCheng2019-mlperfTrain,
  author =	 {Peter Mattson and Christine Cheng and Cody Coleman
                  and Greg Diamos and Paulius Micikevicius and David
                  A. Patterson and Hanlin Tang and Gu{-}Yeon Wei and
                  Peter Bailis and Victor Bittorf and David Brooks and
                  Dehao Chen and Debojyoti Dutta and Udit Gupta and
                  Kim M. Hazelwood and Andrew Hock and Xinyuan Huang
                  and Bill Jia and Daniel Kang and David Kanter and
                  Naveen Kumar and Jeffery Liao and Guokai Ma and
                  Deepak Narayanan and Tayo Oguntebi and Gennady
                  Pekhimenko and Lillian Pentecost and Vijay Janapa
                  Reddi and Taylor Robie and Tom St. John and
                  Carole{-}Jean Wu and Lingjie Xu and Cliff Young and
                  Matei Zaharia},
  title =	 {{MLPerf Training Benchmark}},
  journal =	 {CoRR},
  volume =	 {abs/1910.01500},
  year =	 2019,
  url =		 {http://arxiv.org/abs/1910.01500},
  archivePrefix ={arXiv},
  eprint =	 {1910.01500},
  timestamp =	 {Mon, 04 Nov 2019 08:16:51 +0100},
  biburl =
                  {https://dblp.org/rec/journals/corr/abs-1910-01500.bib},
  bibsource =	 {dblp computer science bibliography,
                  https://dblp.org},
  numpages =	 14,
}

@misc{pytorch-amp,
  title =	 {{Pytorch Automatic Mixed Precision Package}},
  author =	 {PyTorch},
  year =	 2019,
  howpublished = {\url{https://pytorch.org/docs/stable/amp.html}},
}

@misc{sklearn-multiclass-log-reg,
  title =	 {{Sklearn Multi-class Logistic Regression}},
  author =	 {Sklearn},
  year =	 2019,
  howpublished =
                  {\url{https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression}},
}

@article{hosmer1997comparison,
  title =	 {{A Comparison of Goodness-of-fit Tests for the
                  Logistic Regression Model}},
  author =	 {Hosmer, David W and Hosmer, Trina and Le Cessie,
                  Saskia and Lemeshow, Stanley},
  journal =	 {Statistics in medicine},
  volume =	 16,
  number =	 9,
  pages =	 {965--980},
  year =	 1997,
  publisher =	 {Wiley Online Library}
}

@article{carroll1993robustness,
  title =	 {{On Robustness in the Logistic Regression Model}},
  author =	 {Carroll, Raymond J and Pederson, Shane},
  journal =	 {Journal of the Royal Statistical Society: Series B
                  (Methodological)},
  volume =	 55,
  number =	 3,
  pages =	 {693--706},
  year =	 1993,
  publisher =	 {Wiley Online Library}
}

@misc{he2018streaming,
  title =	 {Streaming End-to-end Speech Recognition For Mobile
                  Devices},
  author =	 {Yanzhang He and Tara N. Sainath and Rohit
                  Prabhavalkar and Ian McGraw and Raziel Alvarez and
                  Ding Zhao and David Rybach and Anjuli Kannan and
                  Yonghui Wu and Ruoming Pang and Qiao Liang and
                  Deepti Bhatia and Yuan Shangguan and Bo Li and Golan
                  Pundak and Khe Chai Sim and Tom Bagby and Shuo-yiin
                  Chang and Kanishka Rao and Alexander Gruenstein},
  year =	 2018,
  eprint =	 {1811.06621},
  archivePrefix ={arXiv},
  primaryClass = {cs.CL}
}

@INPROCEEDINGS{Reddi2020mlperf-Infer,
  author =	 {Reddi, Vijay Janapa and Cheng, Christine and Kanter,
                  David and Mattson, Peter and Schmuelling, Guenther
                  and Wu, Carole-Jean and Anderson, Brian and Breughe,
                  Maximilien and Charlebois, Mark and Chou, William
                  and Chukka, Ramesh and Coleman, Cody and Davis, Sam
                  and Deng, Pan and Diamos, Greg and Duke, Jared and
                  Fick, Dave and Gardner, J. Scott and Hubara, Itay
                  and Idgunji, Sachin and Jablin, Thomas B. and Jiao,
                  Jeff and John, Tom St. and Kanwar, Pankaj and Lee,
                  David and Liao, Jeffery and Lokhmotov, Anton and
                  Massa, Francisco and Meng, Peng and Micikevicius,
                  Paulius and Osborne, Colin and Pekhimenko, Gennady
                  and Rajan, Arun Tejusve Raghunath and Sequeira,
                  Dilip and Sirasao, Ashish and Sun, Fei and Tang,
                  Hanlin and Thomson, Michael and Wei, Frank and Wu,
                  Ephrem and Xu, Lingjie and Yamada, Koichi and Yu,
                  Bing and Yuan, George and Zhong, Aaron and Zhang,
                  Peizhao and Zhou, Yuchen},
  booktitle =	 {{2020 ACM/IEEE 47th Annual International Symposium
                  on Computer Architecture}},
  series =	 {ISCA},
  title =	 {{MLPerf Inference Benchmark}},
  year =	 2020,
  pages =	 {446-459},
  doi =		 {10.1109/ISCA45697.2020.00045}
}

@article{riscv-story,
  title =	 {{NVIDIA RISC-V Story}},
  author =	 {NVIDIA},
  year =	 2016,
  journal =	 {{4th RISC-V Workshop}},
  url =
                  {https://riscv.org/wp-content/uploads/2016/07/Tue1100_Nvidia_RISCV_Story_V2.pdf},
}

@inproceedings{kotra2021increasing,
  title =	 {{Increasing GPU Translation Reach by Leveraging
                  Under-Utilized On-Chip Resources}},
  author =	 {Kotra, Jagadish B and LeBeane, Michael and Kandemir,
                  Mahmut T and Loh, Gabriel H},
  booktitle =	 {{54th Annual IEEE/ACM International Symposium on
                  Microarchitecture}},
  series =	 {MICRO},
  pages =	 {1169--1181},
  year =	 2021
}

@misc{tensorflow2015-whitepaper,
  title =	 {{TensorFlow}: Large-Scale Machine Learning on
                  Heterogeneous Systems},
  url =		 {http://tensorflow.org/},
  note =	 {Software available from tensorflow.org},
  author =	 { Mart\'{\i}n~Abadi and Ashish~Agarwal and
                  Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and
                  Craig~Citro and Greg~S.~Corrado and Andy~Davis and
                  Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat
                  and Ian~Goodfellow and Andrew~Harp and
                  Geoffrey~Irving and Michael~Isard and Yangqing Jia
                  and Rafal~Jozefowicz and Lukasz~Kaiser and
                  Manjunath~Kudlur and Josh~Levenberg and Dan~Man\'{e}
                  and Rajat~Monga and Sherry~Moore and Derek~Murray
                  and Chris~Olah and Mike~Schuster and Jonathon~Shlens
                  and Benoit~Steiner and Ilya~Sutskever and
                  Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke
                  and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and
                  Oriol~Vinyals and Pete~Warden and Martin~Wattenberg
                  and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
  year =	 2015,
}

@inproceedings{paszke2017-pytorch,
  title =	 {{Automatic differentiation in PyTorch}},
  author =	 {Paszke, Adam and Gross, Sam and Chintala, Soumith
                  and Chanan, Gregory and Yang, Edward and DeVito,
                  Zachary and Lin, Zeming and Desmaison, Alban and
                  Antiga, Luca and Lerer, Adam},
  booktitle =	 {NIPS-W},
  year =	 2017
}

@inproceedings{dwf,
  author =	 {Fung, Wilson W. L. and Sham, Ivan and Yuan, George
                  and Aamodt, Tor M.},
  title =	 {{Dynamic Warp Formation and Scheduling for Efficient
                  {GPU} Control Flow}},
  booktitle =	 MICRO,
  pages =	 {407--420},
  year =	 2007
}

@inproceedings{cruise,
  author =	 {Jaleel, Aamer and Najaf-abadi, Hashem H. and
                  Subramaniam, Samantika and Steely, Simon C. and
                  Emer, Joel},
  title =	 {{{CRUISE}: Cache Replacement and Utility-Aware
                  Scheduling}},
  booktitle =	 {{Proceedings of the International Conference on
                  Architectural Support for Programming Languages and
                  Operation Systems}},
  series =	 {ASPLOS},
  year =	 2012,
  pages =	 {249--260},
}

@misc{ccws_posted,
  author =	 {Rogers, Timothy G.},
  title =	 {{{CCWS Simulation Infrastructure}}},
  note =	 {Accessed July 6, 2015},
  organization = {{University of British Columbia}},
  year =	 2013,
  howpublished = { \url{http://www.ece.ubc.ca/~tgrogers/ccws.html} }
}

@article{ccws_toppicks,
  author =	 {Rogers, Timothy G. and O'Connor, Mike and Aamodt,
                  Tor M.},
  title =	 {{Cache-Conscious Thread Scheduling for Massively
                  Multithreaded Processors}},
  journal =	 {IEEE Micro, Special Issue: Micro's Top Picks from
                  2012 Computer Architecture Conferences},
  year =	 2013,
}

@article{tgrogers_cacm,
  author =	 {Rogers, Timothy G. and O'Connor, Mike and Aamodt,
                  Tor M.},
  title =	 {{Learning Your Limit: Managing Massively
                  Multithreaded Caches Through Scheduling}},
  journal =	 {Communications of the ACM},
  year =	 2014,
  month =	 {December},
}

@inproceedings{cawa,
  author =	 {Lee, Shin-Ying and Arunkumar, Akhil and Wu,
                  Carole-Jean},
  title =	 {{CAWA: Coordinated Warp Scheduling and Cache
                  Prioritization for Critical Warp Acceleration of
                  GPGPU Workloads}},
  booktitle =	 ISCA,
  year =	 2015,
  pages =	 {515--527},
}

@inproceedings{gpu-trace-schedule,
  author =	 {Jablin, James A. and Jablin, Thomas B. and Mutlu,
                  Onur and Herlihy, Maurice},
  title =	 {{Warp-aware Trace Scheduling for GPUs}},
  booktitle =	 PACT,
  year =	 2014,
  pages =	 {163--174},
}

@inproceedings{gputhread,
  author =	 {Lakshminarayana, Nagesh B. and Kim, Hyesoon},
  title =	 {{Effect of Instruction Fetch and Memory Scheduling
                  on GPU Performance}},
  booktitle =	 {{{Workshop on Language, Compiler, and Architecture
                  Support for GPGPU}}},
  year =	 2010,
}

@INPROCEEDINGS{RogersOConnor2013-daws,
  author =	 {Rogers, Timothy G. and O'Connor, Mike and Aamodt,
                  Tor M.},
  booktitle =	 {{In 46th Annual IEEE/ACM International Symposium on
                  Microarchitecture}},
  series =	 {MICRO},
  title =	 {{Divergence-Aware Warp Scheduling}},
  year =	 2013,
  pages =	 {99-110}
}

@inproceedings{stall-aware,
  author =	 {Yu, Yulong and Xiao, Weijun and He, Xubin and Guo,
                  He and Wang, Yuxin and Chen, Xin},
  title =	 {{A Stall-Aware Warp Scheduling for Dynamically
                  Optimizing Thread-level Parallelism in GPGPUs}},
  booktitle =	 ICS,
  year =	 2015,
  pages =	 {15--24},
}

@inproceedings{simd_sched_gra,
  author =	 {Hao, Benjamin and Pearson, David},
  title =	 {{Instruction Scheduling and Global Register
                  Allocation for SIMD Multiprocessors}},
  booktitle =	 {{2nd Int'l Workshop on Parallel Algorithms for
                  Irregularly Structured Problems}},
  year =	 1995,
  pages =	 {81--86},
}

@inproceedings{owl,
  author =	 {Jog, Adwait and Kayiran, Onur and Chidambaram
                  Nachiappan, Nachiappan and Mishra, Asit K. and
                  Kandemir, Mahmut T. and Mutlu, Onur and Iyer,
                  Ravishankar and Das, Chita R.},
  title =	 {{OWL: Cooperative Thread Array Aware Scheduling
                  Techniques for Improving GPGPU Performance}},
  booktitle =	 {{Proceedings of the International Conference on
                  Architectural Support for Programming Languages and
                  Operation Systems}},
  series =	 {ASPLOS},
  year =	 2013,
}

@inproceedings{micro_2_lvl,
  author =	 {Narasiman, Veynu and Shebanow, Michael and Lee,
                  Chang Joo and Miftakhutdinov, Rustam and Mutlu, Onur
                  and Patt, Yale N.},
  title =	 "{Improving GPU Performance via Large Warps and
                  Two-Level Warp Scheduling}",
  booktitle =	 MICRO,
  year =	 2011,
  month =	 {December},
  pages =	 {308--317}
}

@inproceedings{pats,
  author =	 {Xu, Qiumin and Annavaram, Murali},
  title =	 {{PATS: Pattern Aware Scheduling and Power Gating for
                  GPGPUs}},
  booktitle =	 PACT,
  year =	 2014,
  pages =	 {225--236},
}

@INPROCEEDINGS{LiuYang2015-saws,
  author =	 {J. {Liu} and J. {Yang} and R. {Melhem}},
  booktitle =	 {{Proceedings of the 48th Annual IEEE/ACM
                  International Symposium on Microarchitecture}},
  series =	 {MICRO},
  title =	 {{SAWS: Synchronization aware GPGPU warp scheduling
                  for multiple independent warp schedulers}},
  year =	 2015,
  pages =	 {383-394},
}

@inproceedings{ChenYang2017-prophet,
  author =	 {Chen, Quan and Yang, Hailong and Guo, Minyi and
                  Kannan, Ram Srivatsa and Mars, Jason and Tang,
                  Lingjia},
  title =	 {{Prophet: Precise QoS Prediction on Non-Preemptive
                  Accelerators to Improve Utilization in
                  Warehouse-Scale Computers}},
  booktitle =	 {{Proceedings of the Twenty-Second International
                  Conference on Architectural Support for Programming
                  Languages and Operating Systems}},
  series =	 {ASPLOS '17},
  year =	 2017,
  isbn =	 {978-1-4503-4465-4},
  location =	 {Xi'an, China},
  pages =	 {17--32},
  numpages =	 16,
  url =		 {http://doi.acm.org/10.1145/3037697.3037700},
  doi =		 {10.1145/3037697.3037700},
  acmid =	 3037700,
  publisher =	 {ACM},
  address =	 {New York, NY, USA},
  keywords =	 {non-preemptive accelerators, quality-of-service
                  prediction, warehouse-scale computers},
}

@inproceedings{ChenYang2016-baymax,
  author =	 {Chen, Quan and Yang, Hailong and Mars, Jason and
                  Tang, Lingjia},
  title =	 {{Baymax: QoS Awareness and Increased Utilization for
                  Non-Preemptive Accelerators in Warehouse Scale
                  Computers}},
  booktitle =	 {{Proceedings of the Twenty-First International
                  Conference on Architectural Support for Programming
                  Languages and Operating Systems}},
  series =	 {ASPLOS '16},
  year =	 2016,
  isbn =	 {978-1-4503-4091-5},
  location =	 {Atlanta, Georgia, USA},
  pages =	 {681--696},
  numpages =	 16,
  url =		 {http://doi.acm.org/10.1145/2872362.2872368},
  doi =		 {10.1145/2872362.2872368},
  acmid =	 2872368,
  publisher =	 {ACM},
  address =	 {New York, NY, USA},
  keywords =	 {non-preemptive accelerators, quality of service,
                  scheduling, warehouse scale computers},
}

@inproceedings{HolmesMawhirter2019-grnn,
  author =	 {Holmes, Connor and Mawhirter, Daniel and He, Yuxiong
                  and Yan, Feng and Wu, Bo},
  title =	 {GRNN: Low-Latency and Scalable RNN Inference on
                  GPUs},
  booktitle =	 {Proceedings of the Fourteenth EuroSys Conference
                  2019},
  series =	 {EuroSys '19},
  year =	 2019,
  isbn =	 {978-1-4503-6281-8},
  location =	 {Dresden, Germany},
  pages =	 {41:1--41:16},
  articleno =	 41,
  numpages =	 16,
  url =		 {http://doi.acm.org/10.1145/3302424.3303949},
  doi =		 {10.1145/3302424.3303949},
  acmid =	 3303949,
  publisher =	 {ACM},
  address =	 {New York, NY, USA},
  keywords =	 {GPUs, deep learning inference, recurrent neural
                  networks},
}

@INPROCEEDINGS{gpusync,
  author =	 {G. A. Elliott and B. C. Ward and J. H. Anderson},
  booktitle =	 {2013 IEEE 34th Real-Time Systems Symposium},
  title =	 {GPUSync: A Framework for Real-Time GPU Management},
  year =	 2013,
  pages =	 {33-44},
  keywords =	 {graphics processing units;processor
                  scheduling;real-time systems;GPUSync;real-time GPU
                  management;graphics processing unit
                  management;multiGPU multicore real-time
                  systems;static priority CPU scheduling;dynamic
                  priority CPU scheduling;GPU allocation;migration
                  cost predictors;GPU-related interrupt;worker
                  threads;sporadic task model;GPU drivers;Graphics
                  processing units;Engines;Real-time systems;Resource
                  management;Memory
                  management;Kernel;Protocols;GPGPU;real time
                  systems;schedulability;operating systems},
  doi =		 {10.1109/RTSS.2013.12},
  ISSN =	 {1052-8725},
  month =	 {Dec},
}

@inproceedings {KatoLakshmanan2011-timeGraph,
  author =	 {Kato, Shinpei and Lakshmanan, Karthik and Rajkumar,
                  Ragunathan and Ishikawa, Yutaka},
  title =	 {{TimeGraph: GPU Scheduling for Real-Time
                  Multi-Tasking Environments}},
  booktitle =	 {{USENIX Annual Technical Conference}},
  series =	 {USENIX ATC},
  year =	 2011,
  address =	 {Portland, OR},
  url =
                  {https://www.usenix.org/conference/usenixatc11/timegraph-gpu-scheduling-real-time-multi-tasking-environments},
  publisher =	 {{USENIX} Association},
  month =	 jun,
}

@INPROCEEDINGS{GutierrezBeckmann2018-gem5GPU,
  author =	 {Gutierrez, Anthony and Beckmann, Bradford M. and
                  Dutu, Alexandru and Gross, Joseph and LeBeane,
                  Michael and Kalamatianos, John and Kayiran, Onur and
                  Poremba, Matthew and Potter, Brandon and Puthoor,
                  Sooraj and Sinclair, Matthew D. and Wyse, Michael
                  and Yin, Jieming and Zhang, Xianwei and Jain, Akshay
                  and Rogers, Timothy},
  booktitle =	 {{24th IEEE International Symposium on High
                  Performance Computer Architecture}},
  series =	 {HPCA},
  title =	 {{Lost in Abstraction: Pitfalls of Analyzing GPUs at
                  the Intermediate Language Level}},
  year =	 2018,
  pages =	 {608-619},
  keywords =	 {embedded systems;graphics processing
                  units;instruction sets;microprocessor
                  chips;optimising compilers;parallel
                  architectures;program compilers;intermediate
                  language level;target GPU hardware;GPU
                  microarchitecture simulators;GPU microarchitecture
                  models;dynamic instruction count;Kernel;Graphics
                  processing units;Registers;Hardware;Computer
                  architecture;Microarchitecture;Runtime;ABI;GPU;Intermediate
                  Language;Intermediate Representation;ISA;Simulation},
  doi =		 {10.1109/HPCA.2018.00058},
  ISSN =	 {2378-203X},
  month =	 {Feb},
}

@misc{ba2016layer,
  title =	 {{Layer Normalization}},
  author =	 {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton,
                  Geoffrey E.},
  year =	 2016,
  eprint =	 {1607.06450},
  archivePrefix ={arXiv},
  primaryClass = {stat.ML}
}

@inproceedings{FowersOvtcharov2018-brainwave,
  author =	 {Fowers, Jeremy and Ovtcharov, Kalin and Papamichael,
                  Michael and Massengill, Todd and Liu, Ming and Lo,
                  Daniel and Alkalay, Shlomi and Haselman, Michael and
                  Adams, Logan and Ghandi, Mahdi and Heil, Stephen and
                  Patel, Prerak and Sapek, Adam and Weisz, Gabriel and
                  Woods, Lisa and Lanka, Sitaram and Reinhardt, Steven
                  K. and Caulfield, Adrian M. and Chung, Eric S. and
                  Burger, Doug},
  title =	 {{A Configurable Cloud-scale DNN Processor for
                  Real-time AI}},
  booktitle =	 {{Proceedings of the 45th Annual International
                  Symposium on Computer Architecture}},
  series =	 {{ISCA}},
  year =	 2018,
  isbn =	 {978-1-5386-5984-7},
  location =	 {Los Angeles, California},
  pages =	 {1--14},
  numpages =	 14,
  url =		 {https://doi.org/10.1109/ISCA.2018.00012},
  doi =		 {10.1109/ISCA.2018.00012},
  acmid =	 3276541,
  publisher =	 {IEEE Press},
  address =	 {Piscataway, NJ, USA},
  keywords =	 {accelerator architectures, field programmable gate
                  arrays, neural network hardware},
}

@inproceedings{YuLukefahr17-scalpel,
  author =	 {Yu, Jiecao and Lukefahr, Andrew and Palframan, David
                  and Dasika, Ganesh and Das, Reetuparna and Mahlke,
                  Scott},
  title =	 {{Scalpel: Customizing DNN Pruning to the Underlying
                  Hardware Parallelism}},
  booktitle =	 {{Proceedings of the 44th Annual International
                  Symposium on Computer Architecture}},
  series =	 {ISCA '17},
  year =	 2017,
  isbn =	 {978-1-4503-4892-8},
  location =	 {Toronto, ON, Canada},
  pages =	 {548--560},
  numpages =	 13,
  url =		 {http://doi.acm.org/10.1145/3079856.3080215},
  doi =		 {10.1145/3079856.3080215},
  acmid =	 3080215,
  publisher =	 {ACM},
  address =	 {New York, NY, USA},
  keywords =	 {hardware parallelism, multiple data, neural network
                  pruning, single instruction},
}

@article{mao2017exploring,
  title =	 {{Exploring the Regularity of Sparse Structure in
                  Convolutional Neural Networks}},
  author =	 {Mao, Huizi and Han, Song and Pool, Jeff and Li,
                  Wenshuo and Liu, Xingyu and Wang, Yu and Dally,
                  William J},
  journal =	 {arXiv preprint arXiv:1705.08922},
  year =	 2017
}

@inproceedings{HanKang2017-ese,
  author =	 {Han, Song and Kang, Junlong and Mao, Huizi and Hu,
                  Yiming and Li, Xin and Li, Yubin and Xie, Dongliang
                  and Luo, Hong and Yao, Song and Wang, Yu and Yang,
                  Huazhong and Dally, William (Bill) J.},
  title =	 {{ESE: Efficient Speech Recognition Engine with
                  Sparse LSTM on FPGA}},
  booktitle =	 {Proceedings of the 2017 ACM/SIGDA International
                  Symposium on Field-Programmable Gate Arrays},
  series =	 {FPGA '17},
  year =	 2017
}

@inproceedings{Zhu2018-sparsePRNN,
  author =	 {Feiwen Zhu and Jeff Pool and Michael Andersch and
                  Jeremy Appleyard and Fung Xie},
  title =	 {{Sparse Persistent RNNs: Squeezing Large Recurrent
                  Networks On-Chip}},
  booktitle =	 {{Proceedings of 6th International Conference on
                  Learning Representations}},
  series =	 {{ICLR}},
  year =	 2018,
}

@article{Narang2017-sparseRNNs,
  author =	 {Sharan Narang and Gregory F. Diamos and Shubho
                  Sengupta and Erich Elsen},
  title =	 {{Exploring Sparsity in Recurrent Neural Networks}},
  journal =	 {CoRR},
  volume =	 {abs/1704.05119},
  year =	 2017,
  url =		 {http://arxiv.org/abs/1704.05119},
}

@inproceedings{Parashar17-scnn,
  author =	 {Parashar, Angshuman and Rhu, Minsoo and Mukkara,
                  Anurag and Puglielli, Antonio and Venkatesan,
                  Rangharajan and Khailany, Brucek and Emer, Joel and
                  Keckler, Stephen W. and Dally, William J.},
  title =	 {{SCNN: An Accelerator for Compressed-sparse
                  Convolutional Neural Networks}},
  booktitle =	 {{Proceedings of the 44th Annual International
                  Symposium on Computer Architecture}},
  series =	 {ISCA},
  year =	 2017,
  isbn =	 {978-1-4503-4892-8},
  location =	 {Toronto, ON, Canada},
  pages =	 {27--40},
  numpages =	 14,
  url =		 {http://doi.acm.org/10.1145/3079856.3080254},
  doi =		 {10.1145/3079856.3080254},
  acmid =	 3080254,
  publisher =	 {ACM},
  address =	 {New York, NY, USA},
  keywords =	 {Convolutional neural networks, accelerator
                  architecture},
}

@inproceedings{wagstaff2001constrained,
  title =	 {{Constrained K-means Clustering with Background
                  Knowledge}},
  author =	 {Wagstaff, Kiri and Cardie, Claire and Rogers, Seth
                  and Schr{\"o}dl, Stefan and others},
  booktitle =	 {Icml},
  volume =	 1,
  pages =	 {577--584},
  year =	 2001
}

@inproceedings{bottou1995convergence,
  title =	 {{Convergence Properties of the K-means Algorithms}},
  author =	 {Bottou, Leon and Bengio, Yoshua},
  booktitle =	 {Advances in neural information processing systems},
  pages =	 {585--592},
  year =	 1995
}

@inproceedings{ZhangRajbhandari2018-deepCPU,
  author =	 {Minjia Zhang and Samyam Rajbhandari and Wenhan Wang
                  and Yuxiong He},
  title =	 {DeepCPU: Serving RNN-based Deep Learning Models 10x
                  Faster},
  booktitle =	 {{2018 USENIX Annual Technical Conference}},
  series =	 {{USENIX ATC 18}},
  year =	 2018,
  isbn =	 {978-1-931971-44-7},
  address =	 {Boston, MA},
  pages =	 {951--965},
  url =
                  {https://www.usenix.org/conference/atc18/presentation/zhang-minjia},
  publisher =	 {{USENIX} Association},
}

@INPROCEEDINGS{AdolfRama2016-fathom,
  author =	 {Adolf, Robert and Rama, Saketh and Reagen, Brandon
                  and Wei, Gu-yeon and Brooks, David},
  booktitle =	 {{IEEE International Symposium on Workload
                  Characterization}},
  series =	 {{IISWC}},
  title =	 {{Fathom: Reference Workloads for Modern Deep
                  Learning Methods}},
  year =	 2016,
  pages =	 {1-10},
  keywords =	 {inference mechanisms;learning (artificial
                  intelligence);neural nets;parallel
                  programming;Facebook AI research group;Fathom
                  workload behavior analysis;TensorFlow deep learning
                  framework;application domain;application-level
                  modeling tool;archetypal deep learning
                  workload;artificial intelligence;computational
                  power;deep convolutional neural network;exotic
                  memory network;inference;modern deep learning
                  methods;parallel scalability;training;Analytical
                  models;Computational modeling;Computer
                  architecture;Hardware;Libraries;Machine
                  learning;Training},
  doi =		 {10.1109/IISWC.2016.7581275},
  month =	 {Sept},
}

@inproceedings{DongKaeli2017-dnnmark,
  author =	 {Dong, Shi and Kaeli, David},
  title =	 {{DNNMark: A Deep Neural Network Benchmark Suite for
                  GPUs}},
  booktitle =	 {{Proceedings of the General Purpose GPUs}},
  series =	 {{GPGPU}},
  year =	 2017,
  isbn =	 {978-1-4503-4915-4},
  location =	 {Austin, TX, USA},
  pages =	 {63--72},
  numpages =	 10,
  url =		 {http://doi.acm.org/10.1145/3038228.3038239},
  doi =		 {10.1145/3038228.3038239},
  acmid =	 3038239,
  publisher =	 {ACM},
  address =	 {New York, NY, USA},
  keywords =	 {Benchmark Suite, Deep Neural Network, GPU, cuDNN},
}

@inproceedings{Armitai2016,
  title =	 {{ML-Bench 1.0 Constructing and Analyzing a
                  Machine-Learning Benchmark}},
  author =	 {Armon, Amitai},
  year =	 2016,
  booktitle =	 {{Intel Machine Learning Summit}},
  howpublished =
                  {\url{https://software.intel.com/en-us/videos/l-bench-10-constructing-and-analyzing-a-machine-learning-benchmark}},
}

@inproceedings{jeon2019atc,
  title =	 {Analysis of large-scale multi-tenant GPU clusters
                  for DNN training workloads},
  author =	 {Jeon, Myeongjae and Venkataraman, Shivaram and
                  Phanishayee, Amar and Qian, Junjie and Xiao, Wencong
                  and Yang, Fan},
  booktitle =	 {2019 USENIX Annual Technical Conference},
  pages =	 {947--960},
  year =	 2019
}

%SP: for EW ops being negligible

@inproceedings{he2020newton,
  title =	 {Newton: A DRAM-maker’s accelerator-in-memory (AiM)
                  architecture for machine learning},
  author =	 {He, Mingxuan and Song, Choungki and Kim, Ilkon and
                  Jeong, Chunseok and Kim, Seho and Park, Il and
                  Thottethodi, Mithuna and Vijaykumar, TN},
  booktitle =	 {2020 53rd Annual IEEE/ACM International Symposium on
                  Microarchitecture (MICRO)},
  pages =	 {372--385},
  year =	 2020,
  organization = {IEEE}
}

@inproceedings{qin2020sigma,
  title =	 {Sigma: A sparse and irregular gemm accelerator with
                  flexible interconnects for dnn training},
  author =	 {Qin, Eric and Samajdar, Ananda and Kwon, Hyoukjun
                  and Nadella, Vineet and Srinivasan, Sudarshan and
                  Das, Dipankar and Kaul, Bharat and Krishna, Tushar},
  booktitle =	 {{26th IEEE International Symposium on High
                  Performance Computer Architecture}},
  series =	 {HPCA},
  pages =	 {58--70},
  year =	 2020,
  organization = {IEEE}
}

%SP: echeckpointing

@misc{chen2016training,
  title =	 {Training Deep Nets with Sublinear Memory Cost},
  author =	 {Tianqi Chen and Bing Xu and Chiyuan Zhang and Carlos
                  Guestrin},
  year =	 2016,
  eprint =	 {1604.06174},
  archivePrefix ={arXiv},
  primaryClass = {cs.LG}
}

%SP: Mixed Precision

@misc{micikevicius2018mixed,
  title =	 {{Mixed Precision Training}},
  author =	 {Paulius Micikevicius and Sharan Narang and Jonah
                  Alben and Gregory Diamos and Erich Elsen and David
                  Garcia and Boris Ginsburg and Michael Houston and
                  Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
  year =	 2018,
  eprint =	 {1710.03740},
  archivePrefix ={arXiv},
  primaryClass = {cs.AI}
}

@misc{ott2018scaling,
  title =	 {{Scaling Neural Machine Translation}},
  author =	 {Myle Ott and Sergey Edunov and David Grangier and
                  Michael Auli},
  year =	 2018,
  eprint =	 {1806.00187},
  archivePrefix ={arXiv},
  primaryClass = {cs.CL}
}

@inproceedings{wang2018training,
  title =	 {{Training Deep Neural Networks with 8-bit Floating
                  Point Numbers}},
  author =	 {Wang, Naigang and Choi, Jungwook and Brand, Daniel
                  and Chen, Chia-Yu and Gopalakrishnan, Kailash},
  booktitle =	 {{Proceedings of the 32nd International Conference on
                  Neural Information Processing Systems}},
  series =	 {NeurIPS},
  pages =	 {7686--7695},
  year =	 2018
}

@misc{mig,
  title =	 {{NVIDIA Multi-Instance GPU (MIG)}},
  author =	 {{NVIDIA Corp.}},
  howpublished = { \url{https://docs.nvidia.com/cuda/mig/index.html} },
  year =	 2021,
}

@misc{mxgpu,
  title =	 {{AMD MxGPU and VMware}},
  howpublished =
                  {\url{https://drivers.amd.com/relnotes/amd_mxgpu_deploymentguide_vmware.pdf}},
  year =	 2020,
  author =	 {{Advanced Micro Devices, Inc.}},
}

@inproceedings{kwon2021heterogeneous,
  title =	 {{Heterogeneous Dataflow Accelerators for Multi-DNN
                  Workloads}},
  author =	 {Kwon, Hyoukjun and Lai, Liangzhen and Pellauer,
                  Michael and Krishna, Tushar and Chen, Yu-Hsin and
                  Chandra, Vikas},
  booktitle =	 {{27th IEEE International Symposium on High
                  Performance Computer Architecture}},
  series =	 {HPCA},
  pages =	 {71--83},
  year =	 2021,
  organization = {IEEE}
}

@INPROCEEDINGS{NaffzigerBeck2021-amdChiplets,
  author =	 {Naffziger, Samuel and Beck, Noah and Burd, Thomas
                  and Lepak, Kevin and Loh, Gabriel H. and Subramony,
                  Mahesh and White, Sean},
  booktitle =	 {{ACM/IEEE 48th Annual International Symposium on
                  Computer Architecture}},
  series =	 {ISCA},
  title =	 {{Pioneering Chiplet Technology and Design for the
                  AMD EPYC™ and Ryzen™ Processor Families : Industrial
                  Product}},
  year =	 2021,
  pages =	 {57-70},
  doi =		 {10.1109/ISCA52012.2021.00014}
}

@article{jhurani2015gemm,
  title =	 {{A GEMM interface and implementation on NVIDIA GPUs
                  for multiple small matrices}},
  author =	 {Jhurani, Chetan and Mullowney, Paul},
  journal =	 {Journal of Parallel and Distributed Computing},
  volume =	 75,
  pages =	 {133--140},
  year =	 2015,
  publisher =	 {Elsevier}
}

@inproceedings{abdelfattah2016performance,
  title =	 {{Performance, design, and autotuning of batched GEMM
                  for GPUs}},
  author =	 {Abdelfattah, Ahmad and Haidar, Azzam and Tomov,
                  Stanimire and Dongarra, Jack},
  booktitle =	 {{International Conference on High Performance
                  Computing}},
  pages =	 {21--38},
  year =	 2016,
  organization = {Springer}
}

@INPROCEEDINGS{ElHajjGomezLuna2016-klap,
  author =	 {El Hajj, Izzat and Gomez-Luna, Juan and Li, Cheng
                  and Chang, Li-Wen and Milojicic, Dejan and Hwu,
                  Wen-mei},
  booktitle =	 {{49th Annual IEEE/ACM International Symposium on
                  Microarchitecture}},
  series =	 {MICRO},
  title =	 {{KLAP: Kernel launch aggregation and promotion for
                  optimizing dynamic parallelism}},
  year =	 2016,
  pages =	 {1-12},
  doi =		 {10.1109/MICRO.2016.7783716}
}

@inproceedings{chasapis2016runtime,
  author =	 {Chasapis, Dimitrios and Casas, Marc and Moret\'{o},
                  Miquel and Schulz, Martin and Ayguad\'{e}, Eduard
                  and Labarta, Jesus and Valero, Mateo},
  title =	 {{Runtime-Guided Mitigation of Manufacturing
                  Variability in Power-Constrained Multi-Socket NUMA
                  Nodes}},
  year =	 2016,
  articleno =	 5,
  numpages =	 12,
  location =	 {Istanbul, Turkey},
  booktitle =	 {{Proceedings of the 2016 International Conference on
                  Supercomputing}},
  series =	 {ICS '16}
}

@inproceedings{InadomiPatki2015-scVar,
  author =	 {Inadomi, Yuichi and Patki, Tapasya and Inoue, Koji
                  and Aoyagi, Mutsumi and Rountree, Barry and Schulz,
                  Martin and Lowenthal, David and Wada, Yasutaka and
                  Fukazawa, Keiichiro and Ueda, Masatsugu and Kondo,
                  Masaaki and Miyoshi, Ikuo},
  title =	 {{Analyzing and Mitigating the Impact of
                  Manufacturing Variability in Power-Constrained
                  Supercomputing}},
  year =	 2015,
  isbn =	 9781450337236,
  url =		 {https://doi.org/10.1145/2807591.2807638},
  doi =		 {10.1145/2807591.2807638},
  abstract =	 {A key challenge in next-generation supercomputing is
                  to effectively schedule limited power
                  resources. Modern processors suffer from
                  increasingly large power variations due to the chip
                  manufacturing process. These variations lead to
                  power inhomogeneity in current systems and manifest
                  into performance inhomogeneity in power constrained
                  environments, drastically limiting supercomputing
                  performance. We present a first-of-its-kind study on
                  manufacturing variability on four production HPC
                  systems spanning four microarchitectures, analyze
                  its impact on HPC applications, and propose a novel
                  variation-aware power budgeting scheme to maximize
                  effective application performance. Our low-cost and
                  scalable budgeting algorithm strives to achieve
                  performance homogeneity under a power constraint by
                  deriving application-specific, module-level power
                  allocations. Experimental results using a 1,920
                  socket system show up to 5.4X speedup, with an
                  average speedup of 1.8X across all benchmarks when
                  compared to a variation-unaware power allocation
                  scheme.},
  articleno =	 78,
  numpages =	 12,
  keywords =	 {power-constrained HPC, performance modeling},
  booktitle =	 {{Proceedings of the International Conference for
                  High Performance Computing, Networking, Storage and
                  Analysis}},
  series =	 {SC}
}
%  publisher = {Association for Computing Machinery},
%  address = {New York, NY, USA},
%  location = {Austin, Texas},

@inproceedings{DasOzdemir2007-processVar,
  author =	 {Das, Abhishek and Ozdemir, Serkan and Memik, Gokhan
                  and Zambreno, Joseph and Choudhary, Alok},
  title =	 {{Mitigating the Effects of Process Variations:
                  Architectural Approaches for Improving Batch
                  Performance}},
  booktitle =	 {{Workshop on Architectural Support for Gigascale
                  Integration}},
  series =	 {ASGI},
  year =	 2007,
}

@inproceedings{DuplyakinRicci2019-cloudlab,
  author =	 {Duplyakin, Dmitry and Ricci, Robert and Maricq,
                  Aleksander and Wong, Gary and Duerig, Jonathon and
                  Eide, Eric and Stoller, Leigh and Hibler, Mike and
                  Johnson, David and Webb, Kirk and Akella, Aditya and
                  Wang, Kuangching and Ricart, Glenn and Landweber,
                  Larry and Elliott, Chip and Zink, Michael and
                  Cecchet, Emmanuel and Kar, Snigdhaswin and Mishra,
                  Prabodh},
  title =	 {{The Design and Operation of Cloudlab}},
  year =	 2019,
  isbn =	 9781939133038,
  abstract =	 {Given the highly empirical nature of research in
                  cloud computing, networked systems, and related
                  fields, testbeds play an important role in the
                  research ecosystem. In this paper, we cover one such
                  facility, CloudLab, which supports systems research
                  by providing raw access to programmable hardware,
                  enabling research at large scales, and creating a
                  shared platform for repeatable research.We present
                  our experiences designing CloudLab and operating it
                  for four years, serving nearly 4,000 users who have
                  run over 79,000 experiments on 2,250 servers,
                  switches, and other pieces of datacenter
                  equipment. From this experience, we draw lessons
                  organized around two themes. The first set comes
                  from analysis of data regarding the use of CloudLab:
                  how users interact with it, what they use it for,
                  and the implications for facility design and
                  operation. Our second set of lessons comes from
                  looking at the ways that algorithms used "under the
                  hood," such as resource allocation, have important--
                  and sometimes unexpected--effects on user experience
                  and behavior. These lessons can be of value to the
                  designers and operators of IaaS facilities in
                  general, systems testbeds in particular, and users
                  who have a stake in understanding how these systems
                  are built.},
  booktitle =	 {{Proceedings of the 2019 USENIX Conference on Usenix
                  Annual Technical Conference}},
  series =	 {USENIX ATC},
  pages =	 {1–14},
  numpages =	 14,
}
%  location = {Renton, WA, USA},
%  publisher = {USENIX Association},

@misc{tacc,
  author =	 {TACC},
  title =	 {{Texas Advanced Computing Center}},
  howpublished = {\url{https://www.tacc.utexas.edu/}},
  year =	 2021,
}

@inproceedings{ChasapisMoreto2019-powerEfficJobSched,
  author =	 {Chasapis, Dimitrios and Moret\'{o}, Miquel and
                  Schulz, Martin and Rountree, Barry and Valero, Mateo
                  and Casas, Marc},
  title =	 {{Power Efficient Job Scheduling by Predicting the
                  Impact of Processor Manufacturing Variability}},
  year =	 2019,
  isbn =	 9781450360791,
  url =		 {https://doi.org/10.1145/3330345.3330372},
  doi =		 {10.1145/3330345.3330372},
  abstract =	 {Modern CPUs suffer from performance and power
                  consumption variability due to the manufacturing
                  process. As a result, systems that do not consider
                  such variability caused by manufacturing issues lead
                  to performance degradations and wasted power. In
                  order to avoid such negative impact, users and
                  system administrators must actively counteract any
                  manufacturing variability.In this work we show that
                  parallel systems benefit from taking into account
                  the consequences of manufacturing variability when
                  making scheduling decisions at the job scheduler
                  level. We also show that it is possible to predict
                  the impact of this variability on specific
                  applications by using variability-aware power
                  prediction models. Based on these power models, we
                  propose two job scheduling policies that consider
                  the effects of manufacturing variability for each
                  application and that ensure that power consumption
                  stays under a system-wide power budget. We evaluate
                  our policies under different power budgets and
                  traffic scenarios, consisting of both single- and
                  multi-node parallel applications, utilizing up to
                  4096 cores in total. We demonstrate that they
                  decrease job turnaround time, compared to
                  contemporary scheduling policies used on production
                  clusters, up to 31\% while saving up to 5.5\%
                  energy.},
  booktitle =	 {{Proceedings of the ACM International Conference on
                  Supercomputing}},
  series =	 {ICS '19},
  pages =	 {296–307},
  numpages =	 12,
  keywords =	 {power prediction, job scheduling, manufacturing
                  variability, HPC, energy efficient},
}
%  publisher = {Association for Computing Machinery},
%  address = {New York, NY, USA},
%  location = {Phoenix, Arizona},

@INPROCEEDINGS{GeVogt2013-dvfsKepler,
  author =	 {R. {Ge} and R. {Vogt} and J. {Majumder} and
                  A. {Alam} and M. {Burtscher} and Z. {Zong}},
  title =	 {{Effects of Dynamic Voltage and Frequency Scaling on
                  a K20 GPU}},
  year =	 2013,
  pages =	 {826-833},
  doi =		 {10.1109/ICPP.2013.98},
  booktitle =	 {{42nd International Conference on Parallel
                  Processing}},
  series =	 {ICPP},
}

@INPROCEEDINGS{CoplinBurtscher2016-gpgpuPower,
  author =	 {J. Coplin and M. Burtscher},
  booktitle =	 {{IEEE International Parallel and Distributed
                  Processing Symposium Workshops}},
  series =	 {IPDPSW},
  title =	 {{Energy, Power, and Performance Characterization of
                  GPGPU Benchmark Programs}},
  year =	 2016,
  pages =	 {1190-1199},
  keywords =	 {graphics processing units;benchmark
                  testing;instruction sets;runtime;energy
                  efficiency;hardware;power measurement},
  doi =		 {10.1109/IPDPSW.2016.164},
  url =
                  {https://doi.ieeecomputersociety.org/10.1109/IPDPSW.2016.164},
  month =	 {May}
} 
%  publisher = {IEEE Computer Society},
%  address = {Los Alamitos, CA, USA},

@misc{cublas,
  title =	 {{cuBLAS}},
  author =	 {NVIDIA},
  howpublished = {\url{https://developer.nvidia.com/cublas}},
  year =	 2021,
}

@misc{gpu-burn,
  title =	 {{Multi-GPU CUDA stress test}},
  author =	 {{Timonen, Ville}},
  howpublished = {\url{http://wili.cc/blog/gpu-burn.html}},
  year =	 2020,
}

@misc{nvprof,
  title =	 {{Profiler User's Guide}},
  author =	 {{NVIDIA Corp}},
  year =	 2018,
  howpublished =
                  {\url{https://docs.nvidia.com/cuda/profiler-users-guide/index.html}},
}

@misc{v100-tc,
  author =	 {{NVIDIA}},
  title =	 {{NVIDIA V100 Tensor Core GPU}},
  year =	 2020,
  howpublished =
                  {\url{https://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdf}},
}

@misc{coral2,
  author =	 {{Lawrence Livermore National Labs}},
  title =	 {{CORAL-2 Benchmarks}},
  howpublished = {\url{https://asc.llnl.gov/coral-2-benchmarks}},
  year =	 2020,
}

@misc{olcf6-bmks,
  author =	 {{Oak Ridge National Labs}},
  year =	 2024,
  title =	 {{OLCF-6 Benchmarks}},
  howpublished =
                  {\url{https://www.olcf.ornl.gov/draft-olcf-6-technical-requirements/benchmarks/}},
}

@misc{summit-layout,
  title =	 {{Job Step Viewer - Summit ORNL}},
  howpublished =
                  {\url{https://jobstepviewer.olcf.ornl.gov/summit/871957-1}},
  key =		 {summit}
}

@INPROCEEDINGS{BaruahShivdikar2021-gnnMark,
  author =	 {Baruah, Trinayan and Shivdikar, Kaustubh and Dong,
                  Shi and Sun, Yifan and Mojumder, Saiful A and Jung,
                  Kihoon and Abellán, José L. and Ukidave, Yash and
                  Joshi, Ajay and Kim, John and Kaeli, David},
  booktitle =	 {{IEEE International Symposium on Performance
                  Analysis of Systems and Software}},
  series =	 {ISPASS},
  title =	 {{GNNMark: A Benchmark Suite to Characterize Graph
                  Neural Network Training on GPUs}},
  year =	 2021,
  pages =	 {13-23},
  keywords =	 {Training;Machine learning
                  algorithms;Scalability;Software
                  performance;Benchmark testing;Hardware;Graph neural
                  networks;graphs;benchmarks;GNN training;GPUs},
  doi =		 {10.1109/ISPASS51385.2021.00013}
}

@inproceedings{ostrouchov2020gpulifetimes,
  author =	 {Ostrouchov, George and Maxwell, Don and Ashraf,
                  Rizwan A. and Engelmann, Christian and Shankar,
                  Mallikarjun and Rogers, James H.},
  title =	 {{GPU Lifetimes on Titan Supercomputer: Survival
                  Analysis and Reliability}},
  year =	 2020,
  publisher =	 {IEEE Press},
  booktitle =	 {{Proceedings of the International Conference for
                  High Performance Computing, Networking, Storage and
                  Analysis}},
  articleno =	 41,
  numpages =	 14,
  location =	 {Atlanta, Georgia},
  series =	 {SC '20}
}

@misc{gpu-throttling,
  title =	 {{What Is GPU Throttling And How To Fix It}},
  howpublished =
                  {\url{https://www.thesharedweb.com/gpu-throttling-and-how-to-fix-it}},
  key =		 {gpu-throttling},
}

@article{ZhengTiwari2018-ecoRNN,
  title =	 {{EcoRNN: Efficient Computing of LSTM RNN Training on
                  GPUs}},
  author =	 {Zheng, Bojian and Tiwari, Abhishek and Vijaykumar,
                  Nandita and Pekhimenko, Gennady},
  journal =	 {arXiv preprint arXiv:1805.08899},
  year =	 2018
}

@inproceedings{ZhuAkrout2018-tbd,
  title =	 {{TBD: Benchmarking and Analyzing Deep Neural Network
                  Training}},
  author =	 {Zhu, Hongyu and Akrout, Mohamed and Zheng, Bojian
                  and Pelegris, Andrew and Phanishayee, Amar and
                  Schroeder, Bianca and Pekhimenko, Gennady},
  booktitle =	 {{IEEE International Symposium on Workload
                  Characterization}},
  year =	 2018,
  month =	 {October},
  series =	 {{IISWC}},
}

@inproceedings{Sherwood02,
  author =	 {Sherwood, Timothy and Perelman, Erez and Hamerly,
                  Greg and Calder, Brad},
  title =	 {{Automatically Characterizing Large Scale Program
                  Behavior}},
  booktitle =	 {{Proceedings of the 10th International Conference on
                  Architectural Support for Programming Languages and
                  Operating Systems}},
  series =	 {ASPLOS X},
  year =	 2002,
}

@inproceedings{sherwood01,
  title =	 {Basic block distribution analysis to find periodic
                  behavior and simulation points in applications},
  author =	 {Sherwood, Timothy and Perelman, Erez and Calder,
                  Brad},
  booktitle =	 {Proceedings 2001 International Conference on
                  Parallel Architectures and Compilation Techniques},
  pages =	 {3--14},
  year =	 2001,
  organization = {IEEE}
}

@inproceedings{LymLee2019-delta,
  title =	 {DeLTA: GPU Performance Model for Deep Learning
                  Applications with In-depth Memory System Traffic
                  Analysis},
  author =	 {Lym, Sangkug and Lee, Donghyuk and O'Connor, Mike
                  and Chatterjee, Niladrish and Erez, Mattan},
  booktitle =	 {2019 IEEE International Symposium on Performance
                  Analysis of Systems and Software (ISPASS)},
  pages =	 {293--303},
  year =	 2019,
  organization = {IEEE}
}

@inproceedings{ParasharRaina2019-timeloop,
  title =	 {Timeloop: A Systematic Approach to DNN Accelerator
                  Evaluation},
  author =	 {Parashar, Angshuman and Raina, Priyanka and Shao,
                  Yakun Sophia and Chen, Yu-Hsin and Ying, Victor A
                  and Mukkara, Anurag and Venkatesan, Rangharajan and
                  Khailany, Brucek and Keckler, Stephen W and Emer,
                  Joel},
  booktitle =	 {{IEEE International Symposium on Performance
                  Analysis of Systems and Software}},
  series =	 {ISPASS},
  pages =	 {304--315},
  year =	 2019,
  organization = {IEEE}
}

@inproceedings{QiaoChoe2021-pollux,
  author =	 {Aurick Qiao and Sang Keun Choe and Suhas Jayaram
                  Subramanya and Willie Neiswanger and Qirong Ho and
                  Hao Zhang and Gregory R. Ganger and Eric P. Xing},
  title =	 {{Pollux: Co-adaptive Cluster Scheduling for
                  Goodput-Optimized Deep Learning}},
  booktitle =	 {{15th {USENIX} Symposium on Operating Systems Design
                  and Implementation}},
  series =	 {OSDI},
  year =	 2021,
  isbn =	 {978-1-939133-22-9},
  pages =	 {1--18},
  url =
                  {https://www.usenix.org/conference/osdi21/presentation/qiao},
  publisher =	 {{USENIX} Association},
  month =	 jul,
}

@article{AlsopNa2021-fcs,
  title =	 {{A Case for Fine-grain Coherence Specialization in
                  Heterogeneous Systems}},
  author =	 {Johnathan Alsop and Weon Taek Na and Matthew
                  D. Sinclair and Samuel Grayson and Sarita V. Adve},
  journal =	 {CoRR},
  volume =	 {abs/2104.11678},
  year =	 2021,
  eprint =	 {2104.11678},
  archivePrefix ={arXiv},
  primaryClass = {cs.AR}
}

@inproceedings{AlsopSinclair2018,
  author =	 {Alsop, Johnathan and Sinclair, Matthew D. and Adve,
                  Sarita V.},
  title =	 {{Spandex: A Flexible Interface for Efficient
                  Heterogeneous Coherence}},
  booktitle =	 {{Proceedings of the 45th Annual International
                  Symposium on Computer Architecture}},
  series =	 {ISCA},
  year =	 2018,
  isbn =	 {978-1-5386-5984-7},
  location =	 {Los Angeles, California},
  pages =	 {261--274},
  numpages =	 14,
  url =		 {https://doi.org/10.1109/ISCA.2018.00031},
  doi =		 {10.1109/ISCA.2018.00031},
  acmid =	 3276565,
  publisher =	 {IEEE Press},
  address =	 {Piscataway, NJ, USA},
}

@inproceedings{Kotsifakou18-hpvm,
  author =	 {Kotsifakou, Maria and Srivastava, Prakalp and
                  Sinclair, Matthew D. and Komuravelli, Rakesh and
                  Adve, Vikram and Adve, Sarita},
  title =	 {{HPVM: Heterogeneous Parallel Virtual Machine}},
  booktitle =	 {{Proceedings of the 23rd ACM SIGPLAN Symposium on
                  Principles and Practice of Parallel Programming}},
  series =	 {PPoPP},
  year =	 2018,
  isbn =	 {978-1-4503-4982-6},
  location =	 {Vienna, Austria},
  pages =	 {68--80},
  numpages =	 13,
  url =		 {http://doi.acm.org/10.1145/3178487.3178493},
  doi =		 {10.1145/3178487.3178493},
  acmid =	 3178493,
  publisher =	 {ACM},
  address =	 {New York, NY, USA},
  keywords =	 {GPU, compiler, heterogeneous systems, parallel IR,
                  vector SIMD, virtual ISA},
}

@INPROCEEDINGS{Srivastava16-hvisc,
  author =	 {P. Srivastava and M. Kotsifakou and M. D. Sinclair
                  and R. Komuravelli and V. Adve and S. Adve},
  booktitle =	 {{2016 International Conference on Parallel
                  Architecture and Compilation Techniques}},
  series =	 {PACT},
  title =	 {{POSTER - hVISC: A portable abstraction for
                  heterogeneous parallel systems}},
  year =	 2016,
  pages =	 {443-445},
  keywords =	 {instruction sets;parallel programming;storage
                  management;programming heterogeneous parallel
                  systems;POSTER-hVISC;portable abstraction;memory
                  hierarchies;heterogeneous hardware parallel
                  abstraction designed;hierarchical dataflow
                  graph;shared memory;virtual instruction set
                  architecture;ISA;functional portability;performance
                  portability;Hardware;Parallel processing;Instruction
                  sets;Image edge detection;Graphics processing
                  units;Streaming media;Data transfer},
  doi =		 {10.1145/2967938.2976039},
  month =	 {Sept},
}

@INPROCEEDINGS{AlsopSinclair2016,
  author =	 {Alsop, Johnathan and Sinclair, Matthew D. and
                  Komuravelli, Rakesh and Adve, Sarita V.},
  booktitle =	 {{IEEE International Symposium on Performance
                  Analysis of Systems and Software}},
  series =	 {ISPASS},
  title =	 {{GSI: A GPU Stall Inspector to Characterize the
                  Sources of Memory Stalls for Tightly Coupled GPUs}},
  year =	 2016,
  pages =	 {172-182},
  keywords =	 {electronic engineering computing;graphics processing
                  units;memory architecture;GSI;GPU stall
                  inspector;memory stalls;power wall;single core
                  performance;energy-efficient high-throughput GPU
                  cores;data-parallel applications;heterogeneous
                  cores;heterogeneous memory systems;performance
                  characterization;parallelism;GPU
                  codes;coarse-grained metrics;CPU-GPU memory
                  subsystem;heterogeneous CPU-GPU systems;graphics
                  processing unit;central processing unit;Graphics
                  processing units;Instruction
                  sets;Synchronization;Parallel
                  processing;Technological
                  innovation;Kernel;Classification algorithms},
  doi =		 {10.1109/ISPASS.2016.7482092},
  month =	 {April},
}

@misc{Boehm18-oota,
  title =	 {{P1217R0: Out-of-thin-air, revisited, again}},
  author =	 {Boehm, Hans},
  howpublished =
                  {\url{http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1217r0.html}},
  year =	 2018,
}

@MastersThesis{sinclair:msThesis,
  title =	 {Enabling New Uses for GPUs},
  author =	 {Matthew D. Sinclair},
  school =	 {University of Wisconsin-Madison},
  howpublished =
                  {\url{http://pages.cs.wisc.edu/~sinclair/papers/sinclair_mastersThesis.pdf}},
  year =	 2011,
  month =	 {May}
}

@article{SalvadorDarvin2020-specializingArxiv,
  title =	 {{Specializing Coherence, Consistency, and Push/Pull
                  for GPU Graph Analytics}},
  author =	 {Giordano Salvador and Wesley H. Darvin and Muhammad
                  Huzaifa and Johnathan Alsop and Matthew D. Sinclair
                  and Sarita V. Adve},
  year =	 2020,
  eprint =	 {2002.10245},
  journal =	 {arXiv preprint}
}

@inproceedings{SalvadorDarvin2020-specializing,
  title =	 {{Specializing Coherence, Consistency, and Push/Pull
                  for GPU Graph Analytics}},
  author =	 {Giordano Salvador and Wesley H. Darvin and Muhammad
                  Huzaifa and Johnathan Alsop and Matthew D. Sinclair
                  and Sarita V. Adve},
  year =	 2020,
  booktitle =	 {{IEEE International Symposium on Performance
                  Analysis of Systems and Software}},
  series =	 {ISPASS},
}

@article{HuzaifaAlsop2020-gpuReuse,
  author =	 {Huzaifa, Muhammad and Alsop, Johnathan and Mahmoud,
                  Abdulrahman and Salvador, Giordano and Sinclair,
                  Matthew D. and Adve, Sarita V.},
  title =	 {{Inter-Kernel Reuse-Aware Thread Block Scheduling}},
  year =	 2020,
  issue_date =	 {August 2020},
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  volume =	 17,
  number =	 3,
  issn =	 {1544-3566},
  url =		 {https://doi.org/10.1145/3406538},
  doi =		 {10.1145/3406538},
  abstract =	 {As GPUs have become more programmable, their
                  performance and energy benefits have made them
                  increasingly popular. However, while GPU compute
                  units continue to improve in performance, on-chip
                  memories lag behind and data accesses are becoming
                  increasingly expensive in performance and
                  energy. Emerging GPU coherence protocols can
                  mitigate this bottleneck by exploiting data reuse in
                  GPU caches across kernel boundaries. Unfortunately,
                  current GPU thread block schedulers are typically
                  not designed to expose such reuse. This article
                  proposes new hardware thread block schedulers that
                  optimize inter-kernel reuse while using work
                  stealing to preserve load balance. Our schedulers
                  are simple, decentralized, and have extremely low
                  overhead. Compared to a baseline round-robin
                  scheduler, the best performing scheduler reduces
                  average execution time and energy by 19\% and 11\%,
                  respectively, in regular applications, and 10\% and
                  8\%, respectively, in irregular applications.},
  journal =	 {{ACM Trans. Archit. Code Optim.}},
  series =	 {TACO},
  month =	 aug,
  articleno =	 24,
  numpages =	 27,
  keywords =	 {memory systems, GPUs, scheduling, caches}
}

@inproceedings{SinclairVenkataraman2021-gpuPower,
  title =	 {{Co-designing Power Management with Job Scheduling
                  for Efficient Exascale Computing}},
  author =	 {Sinclair, Matthew D. and Venkataraman, Shivaram},
  year =	 2021,
  booktitle =	 {{DOE ASCR Workshop on Reimagining Codesign}},
  month =	 {March},
}

@inproceedings{BruceLowePower2021-gem5HPC,
  title =	 {{Leveraging open source simulators for HPC
                  codesign}},
  author =	 {Bruce, Bobby and Lowe-Power, Jason and Sinclair,
                  Matthew D.},
  year =	 2021,
  booktitle =	 {{DOE ASCR Workshop on Reimagining Codesign}},
  month =	 {March},
}

@inproceedings{AlsopSinclair2019-iiswc,
  title =	 {{Optimizing GPU Cache Policies for MI Workloads}},
  author =	 {Alsop, Johnathan and Sinclair, Matthew D. and
                  Gutierrez, Anthony and Bharadwaj, Srikant and Zhang,
                  Xianwei and Beckmann, Bradford and Dutu, Alexandru
                  and Kayiran, Onur and LeBeane, Michael and Potter,
                  Brandon and Puthoor, Sooraj and Yeh, Tsung Tai},
  booktitle =	 {{IEEE International Symposium on Workload
                  Characterization}},
  series =	 {IISWC},
  year =	 2019,
}

@article{AlsopSinclair2019-arxiv,
  title =	 {{Optimizing GPU Cache Policies for MI Workloads}},
  author =	 {Alsop, Johnathan and Sinclair, Matthew D. and
                  Gutierrez, Anthony and Bharadwaj, Srikant and Zhang,
                  Xianwei and Beckmann, Bradford and Dutu, Alexandru
                  and Kayiran, Onur and LeBeane, Michael and Potter,
                  Brandon and Puthoor, Sooraj and Yeh, Tsung Tai},
  booktitle =	 {{IEEE International Symposium on Workload
                  Characterization}},
  journal =	 {arXiv preprint arXiv:1910.00134},
  year =	 2019,
}

@inproceedings{KomuravelliSinclair2015-stash,
  author =	 {Komuravelli, Rakesh and Sinclair, Matthew D. and
                  Alsop, Johnathan and Huzaifa, Muhammad and
                  Srivastava, Prakalp and Kotsifakou, Maria and Adve,
                  Sarita V. and Adve, Vikram S.},
  title =	 {{Stash: Have Your Scratchpad and Cache it Too}},
  booktitle =	 {{Proceedings of the 42nd Annual International
                  Symposium on Computer Architecture}},
  year =	 2015,
  numpages =	 13,
  pages =	 {707--719},
  series =	 {{ISCA}},
  location =	 {Portland, Oregon},
}

@inproceedings{SinclairAlsop2015,
  author =	 {Sinclair, Matthew D. and Alsop, Johnathan and Adve,
                  Sarita V.},
  title =	 {{Efficient GPU Synchronization without Scopes:
                  Saying No to Complex Consistency Models}},
  booktitle =	 {{Proceedings of the 48th Annual IEEE/ACM
                  International Symposium on Microarchitecture}},
  year =	 2015,
  MONTH =	 {December},
  pages =	 {647--659},
  series =	 {{MICRO}},
}

@techreport{SinclairDuwe2011,
  author =	 {Matthew Sinclair and Henry Duwe and Karthikeyan
                  Sankaralingam},
  title =	 {{Porting CMP Benchmarks to GPUs}},
  institution =	 "{Department of Computer Sciences, The University of
                  Wisconsin-Madison}",
  school =	 {The University of Wisconsin-Madison},
  year =	 2011,
  bib2html_dl_html ={http://www.cs.wisc.edu/techreports/},
  bib2html_dl_pdf ={http://www.cs.wisc.edu/techreports/},
  bib2html_pubtype ={Tech Report},
  bib2html_rescat ={Architecture},
}

@inproceedings{SinclairAlsop2017,
  author =	 {Sinclair, Matthew D. and Alsop, Johnathan and Adve,
                  Sarita V.},
  title =	 {{Chasing Away RAts: Semantics and Evaluation for
                  Relaxed Atomics on Heterogeneous Systems}},
  booktitle =	 {{Proceedings of the 44th Annual International
                  Symposium on Computer Architecture}},
  year =	 2017,
  series =	 {{ISCA}},
  isbn =	 {978-1-4503-4892-8},
  location =	 {Toronto, ON, Canada},
  pages =	 {161--174},
  numpages =	 14,
  url =		 {http://doi.acm.org/10.1145/3079856.3080206},
  doi =		 {10.1145/3079856.3080206},
  acmid =	 3080206,
  publisher =	 {ACM},
  address =	 {New York, NY, USA},
  keywords =	 {GPGPU, data-race-free models, memory consistency,
                  relaxed atomics},
}

@inproceedings{SinclairAlsop2017_2,
  author =	 {Sinclair, Matthew D. and Alsop, Johnathan and Adve,
                  Sarita V.},
  title =	 {{HeteroSync: A Benchmark Suite for Fine-Grained
                  Synchronization on Tightly Coupled GPUs}},
  booktitle =	 {{IEEE International Symposium on Workload
                  Characterization}},
  year =	 2017,
  month =	 {October},
  series =	 {{IISWC}},
}

@inproceedings{BlemSinclair2011,
  author =	 {Emily Blem and Matthew Sinclair and Karthikeyan
                  Sankaralingam},
  title =	 {Challenge Benchmarks that Must be Conquered to
                  Sustain the GPU Revolution},
  booktitle =	 "{Proceedings of the 4th Workshop on Emerging
                  Applications for Manycore Architecture}",
  bib2html_dl_pdf ={http://bit.ly/laizPz},
  bib2html_pubtype ={Refereed Conference},
  bib2html_rescat ={Architecture},
  month =	 {June},
  year =	 2011
}

@book{TownsendSankaralingam2010,
  author =	 {Richard Townsend and Karthikeyan Sankaralingam and
                  Matthew D. Sinclair},
  title =	 {{Leveraging the Untapped Computation Power of GPUs:
                  Fast Spectral Synthesis Using Texture
                  Interpolation}},
  publisher =	 {Addison-Wesley},
  year =	 2010
}
% editor={Wen Mei-Hwu},

@inproceedings{SankaralingamTownsend2010,
  author =	 {Karthikeyan Sankaralingam and Richard Townsend and
                  Matthew D. Sinclair},
  title =	 {GRASSY: Leveraging GPU Texture Units for
                  Asteroseismic Data Analysis},
  booktitle =	 "{Proceedings of GPU Technology Conference}",
  series =	 {{GTC}},
  year =	 2010
}

@inproceedings{ChouNg2020-dab,
  title =	 {{Deterministic Atomic Buffering}},
  author =	 {Chou, Yuan Hsi and Ng, Christopher and Cattell,
                  Shaylin and Intan, Jeremy and Sinclair, Matthew
                  D. and Devietti, Joseph and Rogers, Timothy G. and
                  Aamodt, Tor M.},
  booktitle =	 {{Proceedings of the 53rd IEEE/ACM International
                  Symposium on Microarchitecture}},
  series =	 {MICRO},
  month =	 {October},
  year =	 2020,
}

@inproceedings{DutuSinclair2020-ifp,
  title =	 {{Independent Forward Progress of Work-groups}},
  author =	 {Dutu, Alexandru and Sinclair, Matthew D. and
                  Beckmann, Bradford M. and Wood, David A. and Chow,
                  Marcus},
  booktitle =	 {{Proceedings of the 47th International Symposium on
                  Computer Architecture}},
  series =	 {ISCA},
  year =	 2020,
  month =	 {May}
}

@inproceedings{YogatamaSinclair2020-multiGPU,
  title =	 {{Enabling Multi-GPU Support in gem5}},
  author =	 {Yogatama, Bobbi W. and Sinclair, Matthew D. and
                  Swift, Michael M.},
  year =	 2020,
  month =	 {June},
  booktitle =	 {{3rd gem5 Users' Workshop}},
}

@misc{regem5,
  author =	 {Lowe-Power, Jason and Sinclair, Matthew D.},
  title =	 {{RE-gem5: Building Sustainable Research
                  Infrastructure}},
  howpublished =
                  {\url{https://www.sigarch.org/re-gem5-building-sustainable-research-infrastructure/}},
  year =	 2019,
  month =	 {September},
}

@article{LewShah2018-gpgpusimML,
  author =	 {Jonathan Lew and Deval Shah and Suchita Pati and
                  Shaylin Cattell and Mengchi Zhang and Amruth
                  Sandhupatla and Christopher Ng and Negar Goli and
                  Matthew D. Sinclair and Timothy G. Rogers and Tor
                  M. Aamodt},
  title =	 {{Analyzing Machine Learning Workloads Using a
                  Detailed GPU Simulator}},
  journal =	 {CoRR},
  volume =	 {abs/1811.08933},
  year =	 2018,
  url =		 {http://arxiv.org/abs/1811.08933},
  archivePrefix ={arXiv},
  eprint =	 {1811.08933},
  timestamp =	 {Fri, 30 Nov 2018 12:44:28 +0100},
  biburl =
                  {https://dblp.org/rec/bib/journals/corr/abs-1811-08933},
  bibsource =	 {dblp computer science bibliography,
                  https://dblp.org}
}

@inproceedings{LewShah2019-gpgpusimML,
  author =	 {Jonathan Lew and Deval Shah and Suchita Pati and
                  Shaylin Cattell and Mengchi Zhang and Amruth
                  Sandhupatla and Christopher Ng and Negar Goli and
                  Matthew D. Sinclair and Timothy G. Rogers and Tor
                  M. Aamodt},
  title =	 {{Analyzing Machine Learning Workloads Using a
                  Detailed GPU Simulator}},
  year =	 2019,
  booktitle =	 {{IEEE International Symposium on Performance
                  Analysis of Systems and Software}},
  series =	 {ISPASS},
}

@inproceedings{DalmiaMahapatra2022-lab,
  author =	 {Dalmia, Preyesh and Mahapatra, Rohan and Sinclair,
                  Matthew D.},
  title =	 {{Only Buffer When You Need To: Reducing On-chip GPU
                  Traffic with Reconfigurable Local Atomic Buffers}},
  year =	 2022,
  booktitle =	 {{28th IEEE International Symposium on High
                  Performance Computer Architecture}},
  series =	 {HPCA},
}

@article{PhillipsBraun2005-namd,
  title =	 {{Scalable Molecular Dynamics with NAMD}},
  author =	 {Phillips, James C and Braun, Rosemary and Wang, Wei
                  and Gumbart, James and Tajkhorshid, Emad and Villa,
                  Elizabeth and Chipot, Christophe and Skeel, Robert D
                  and Kale, Laxmikant and Schulten, Klaus},
  journal =	 {{Journal of Computational Chemistry}},
  volume =	 26,
  number =	 16,
  pages =	 {1781--1802},
  year =	 2005,
  publisher =	 {Wiley Online Library}
}

@article{HospitalGoni2015-md,
  title =	 {{Molecular Dynamics Simulations: Advances and
                  Applications}},
  author =	 {Hospital, Adam and Go{\~n}i, Josep Ramon and Orozco,
                  Modesto and Gelp{\'\i}, Josep L},
  journal =	 {{Advances and Applications in Bioinformatics and
                  Chemistry: AABC}},
  volume =	 8,
  pages =	 37,
  year =	 2015,
  publisher =	 {Dove Press}
}

@article{TorrieValleau1977-umbrella,
  title =	 {Nonphysical sampling distributions in Monte Carlo
                  free-energy estimation: Umbrella sampling},
  author =	 {Torrie, Glenn M and Valleau, John P},
  journal =	 {Journal of Computational Physics},
  volume =	 23,
  number =	 2,
  pages =	 {187--199},
  year =	 1977,
  publisher =	 {Elsevier}
}

@article{narayanan2021efficient,
  title =	 {Efficient large-scale language model training on gpu
                  clusters},
  author =	 {Narayanan, Deepak and Shoeybi, Mohammad and Casper,
                  Jared and LeGresley, Patrick and Patwary, Mostofa
                  and Korthikanti, Vijay Anand and Vainbrand, Dmitri
                  and Kashinkunti, Prethvi and Bernauer, Julie and
                  Catanzaro, Bryan and others},
  journal =	 {arXiv preprint arXiv:2104.04473},
  year =	 2021
}

@INPROCEEDINGS{JiaoLin2010-gpuPowerPerf,
  author =	 {Jiao, Y. and Lin, H. and Balaji, P. and Feng, W.},
  booktitle =	 {IEEE/ACM International Conference on Green Computing
                  and Communications \& International Conference on
                  Cyber, Physical and Social Computing},
  title =	 {{Power and Performance Characterization of
                  Computational Kernels on the GPU}},
  year =	 2010,
  pages =	 {221-228},
  doi =		 {10.1109/GreenCom-CPSCom.2010.143}
}

@InProceedings{pbs-qsub,
  author =	 "Henderson, Robert L.",
  editor =	 "Feitelson, Dror G. and Rudolph, Larry",
  title =	 "Job scheduling under the Portable Batch System",
  booktitle =	 "Job Scheduling Strategies for Parallel Processing",
  year =	 1995,
  publisher =	 "Springer Berlin Heidelberg",
  address =	 "Berlin, Heidelberg",
  pages =	 "279--294",
  isbn =	 "978-3-540-49459-1"
}

@article{condor,
  author =	 {Thain, Douglas and Tannenbaum, Todd and Livny,
                  Miron},
  title =	 {{Distributed Computing in Practice: the Condor
                  Experience}},
  journal =	 {Concurrency and Computation: Practice and
                  Experience},
  volume =	 17,
  number =	 {2‐4},
  pages =	 {323-356},
  year =	 2005
}

@book{barroso2009datacenter,
  title =	 {{The Datacenter as a Computer: An Introduction to
                  the Design of Warehouse-Scale Machines}},
  author =	 {Luiz André Barroso and Urs Hölzle},
  year =	 2009,
  booktitle =	 {{The Datacenter as a Computer: An Introduction to
                  the Design of Warehouse-Scale Machines}},
  publisher =	 {Morgan \& Claypool}
}

@inproceedings{feitelson1996packing,
  title =	 {{Packing Schemes for Gang Scheduling}},
  author =	 {Feitelson, Dror G},
  booktitle =	 {{Workshop on Job Scheduling Strategies for Parallel
                  Processing}},
  pages =	 {89--110},
  year =	 1996,
  organization = {Springer}
}

@inproceedings{verma2015large,
  title =	 {{Large-scale cluster management at Google with
                  Borg}},
  author =	 {Verma, Abhishek and Pedrosa, Luis and Korupolu,
                  Madhukar and Oppenheimer, David and Tune, Eric and
                  Wilkes, John},
  booktitle =	 {Eurosys},
  year =	 2015
}

@inproceedings{hindman2011mesos,
  title =	 {Mesos: A platform for fine-grained resource sharing
                  in the data center},
  author =	 {Hindman, Benjamin and Konwinski, Andy and Zaharia,
                  Matei and Ghodsi, Ali and Joseph, Anthony D and
                  Katz, Randy and Shenker, Scott and Stoica, Ion},
  booktitle =	 {NSDI},
  year =	 2011,
}

@inproceedings{isard2009quincy,
  title =	 {{Quincy: Fair scheduling for distributed computing
                  clusters}},
  author =	 {Isard, Michael and Prabhakaran, Vijayan and Currey,
                  Jon and Wieder, Udi and Talwar, Kunal and Goldberg,
                  Andrew},
  booktitle =	 {SOSP},
  year =	 2009
}

@inproceedings{malte2013omega,
  title =	 {Omega: flexible, scalable schedulers for large scale
                  compute clusters},
  author =	 {Schwarzkopf, Malte and Konwinski, Andy and
                  Abd-el-Malek, Michael and Wilkes, John},
  booktitle =	 {Eurosys},
  year =	 2013
}

@inproceedings{wencong1,
  title =	 {{Gandiva: Introspective Cluster Scheduling for Deep
                  Learning}},
  author =	 {{Wencong Xiao, Romil Bhardwaj, Ramachandran Ramjee,
                  Muthian Sivathanu, Nipun Kwatra, Zhenhua Han,
                  Pratyush Patel, Xuan Peng, Hanyu Zhao, Quanlu Zhang,
                  Fan Yang, Lidong Zhou}},
  booktitle =	 {OSDI},
  year =	 2018
}

@inproceedings{gu2019tiresias,
  title =	 {Tiresias: A GPU cluster manager for distributed deep
                  learning},
  author =	 {Gu, Juncheng and Chowdhury, Mosharaf and Shin, Kang
                  G and Zhu, Yibo and Jeon, Myeongjae and Qian, Junjie
                  and Liu, Hongqiang and Guo, Chuanxiong},
  booktitle =	 {16th USENIX Symposium on Networked Systems Design
                  and Implementation (NSDI)},
  pages =	 {485--500},
  year =	 2019
}

@inproceedings{ghodsi2011dominant,
  title =	 {{Dominant resource fairness: Fair allocation of
                  multiple resource types}},
  author =	 {Ghodsi, Ali and Zaharia, Matei and Hindman, Benjamin
                  and Konwinski, Andy and Shenker, Scott and Stoica,
                  Ion},
  booktitle =	 {USENIX NSDI},
  year =	 2011
}

@inproceedings{le2020allox,
  title =	 {{AlloX: Compute Allocation in Hybrid Clusters}},
  author =	 {Le, Tan N and Sun, Xiao and Chowdhury, Mosharaf and
                  Liu, Zhenhua},
  booktitle =	 {Proceedings of the Fifteenth European Conference on
                  Computer Systems},
  year =	 2020
}

@inproceedings{optimus,
  title =	 {{Optimus: an Efficient Dynamic Resource Scheduler
                  for Deep Learning Clusters}},
  author =	 {Peng, Yanghua and Bao, Yixin and Chen, Yangrui and
                  Wu, Chuan and Guo, Chuanxiong},
  booktitle =	 {{Proceedings of the Thirteenth EuroSys Conference}},
  pages =	 3,
  year =	 2018,
  organization = {ACM}
}

@inproceedings{slaq,
  title =	 {{SLAQ: quality-driven scheduling for distributed
                  machine learning}},
  author =	 {Zhang, Haoyu and Stafman, Logan and Or, Andrew and
                  Freedman, Michael J},
  booktitle =	 {Proceedings of the 2017 Symposium on Cloud
                  Computing},
  pages =	 {390--404},
  year =	 2017,
  organization = {ACM}
}

@article{gns_scaling,
  author =	 {Sam McCandlish and Jared Kaplan and Dario Amodei and
                  OpenAI Dota Team},
  title =	 {{An Empirical Model of Large-Batch Training}},
  journal =	 {CoRR},
  volume =	 {abs/1812.06162},
  year =	 2018,
  url =		 {http://arxiv.org/abs/1812.06162},
  eprinttype =	 {arXiv},
  eprint =	 {1812.06162},
  timestamp =	 {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl =
                  {https://dblp.org/rec/journals/corr/abs-1812-06162.bib},
  bibsource =	 {dblp computer science bibliography,
                  https://dblp.org}
}

@inproceedings {kungfu20,
  author =	 {Luo Mai and Guo Li and Marcel Wagenl{\"a}nder and
                  Konstantinos Fertakis and Andrei-Octavian Brabete
                  and Peter Pietzuch},
  title =	 {KungFu: Making Training in Distributed Machine
                  Learning Adaptive},
  booktitle =	 {14th {USENIX} Symposium on Operating Systems Design
                  and Implementation ({OSDI} 20)},
  year =	 2020,
  isbn =	 {978-1-939133-19-9},
  pages =	 {937--954},
  url =
                  {https://www.usenix.org/conference/osdi20/presentation/mai},
  publisher =	 {{USENIX} Association},
  month =	 nov,
}

@article{agarwal2021adaptive,
  title =	 {Adaptive Gradient Communication via Critical
                  Learning Regime Identification},
  author =	 {Agarwal, Saurabh and Wang, Hongyi and Lee, Kangwook
                  and Venkataraman, Shivaram and Papailiopoulos,
                  Dimitris},
  journal =	 {Proceedings of Machine Learning and Systems},
  volume =	 3,
  year =	 2021
}

@InProceedings{slurmSimulator1,
  author =	 "Simakov, Nikolay A.  and Innus, Martins D.  and
                  Jones, Matthew D.  and DeLeon, Robert L.  and White,
                  Joseph P.  and Gallo, Steven M.  and Patra, Abani K.
                  and Furlani, Thomas R.",
  editor =	 "Jarvis, Stephen and Wright, Steven and Hammond,
                  Simon",
  title =	 "A Slurm Simulator: Implementation and Parametric
                  Analysis",
  booktitle =	 "High Performance Computing Systems. Performance
                  Modeling, Benchmarking, and Simulation",
  year =	 2018,
  publisher =	 "Springer International Publishing",
  pages =	 "197--217",
}

@inproceedings{slurmSimulator2,
  author =	 {Simakov, Nikolay A. and DeLeon, Robert L. and Innus,
                  Martins D. and Jones, Matthew D. and White, Joseph
                  P. and Gallo, Steven M. and Patra, Abani K. and
                  Furlani, Thomas R.},
  title =	 {{Slurm Simulator: Improving Slurm Scheduler
                  Performance on Large HPC Systems by Utilization of
                  Multiple Controllers and Node Sharing}},
  year =	 2018,
  isbn =	 9781450364461,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  booktitle =	 {{Proceedings of the Practice and Experience on
                  Advanced Research Computing}},
  articleno =	 25,
  numpages =	 8,
  keywords =	 {Workload, Simulation, HPC, Scheduling},
  location =	 {Pittsburgh, PA, USA},
  series =	 {PEARC '18}
}

@misc{balasubramanian2021accelerating,
  title =	 {{Accelerating Deep Learning Inference via Learned
                  Caches}},
  author =	 {Arjun Balasubramanian and Adarsh Kumar and Yuhan Liu
                  and Han Cao and Shivaram Venkataraman and Aditya
                  Akella},
  year =	 2021,
  eprint =	 {2101.07344},
  archivePrefix ={arXiv},
  primaryClass = {cs.LG}
}

@inproceedings{venkataraman2013presto,
  title =	 {{Presto: Distributed Machine Learning and Graph
                  Processing with Sparse Matrices}},
  author =	 {Venkataraman, Shivaram and Bodzsar, Erik and Roy,
                  Indrajit and AuYoung, Alvin and Schreiber, Robert S},
  booktitle =	 {{Proceedings of the 8th ACM European Conference on
                  Computer Systems}},
  pages =	 {197--210},
  year =	 2013
}

@inproceedings{wang2020blink,
  author =	 {Wang, Guanhua and Venkataraman, Shivaram and
                  Phanishayee, Amar and Devanur, Nikhil and Thelin,
                  Jorgen and Stoica, Ion},
  booktitle =	 {Proceedings of Machine Learning and Systems 2020},
  pages =	 {172--186},
  title =	 {{Blink: Fast and Generic Collectives for Distributed
                  ML}},
  year =	 2020
}

@inproceedings{kosaian2019parity,
  title =	 {Parity models: erasure-coded resilience for
                  prediction serving systems},
  author =	 {Kosaian, Jack and Rashmi, KV and Venkataraman,
                  Shivaram},
  booktitle =	 {Proceedings of the 27th ACM Symposium on Operating
                  Systems Principles},
  pages =	 {30--46},
  year =	 2019
}

@inproceedings{venkataraman2017drizzle,
  title =	 {Drizzle: Fast and adaptable stream processing at
                  scale},
  author =	 {Venkataraman, Shivaram and Panda, Aurojit and
                  Ousterhout, Kay and Armbrust, Michael and Ghodsi,
                  Ali and Franklin, Michael J and Recht, Benjamin and
                  Stoica, Ion},
  booktitle =	 {Proceedings of the 26th Symposium on Operating
                  Systems Principles},
  pages =	 {374--389},
  year =	 2017,
  organization = {ACM}
}

@inproceedings{venkataraman2014power,
  title =	 {The power of choice in data-aware cluster
                  scheduling},
  author =	 {Venkataraman, Shivaram and Panda, Aurojit and
                  Ananthanarayanan, Ganesh and Franklin, Michael J and
                  Stoica, Ion},
  booktitle =	 {OSDI},
  pages =	 {301--316},
  year =	 2014
}

@inproceedings{venkataraman2016sparkr,
  title =	 {Sparkr: Scaling r programs with spark},
  author =	 {Venkataraman, Shivaram and Yang, Zongheng and Liu,
                  Davies and Liang, Eric and Falaki, Hossein and Meng,
                  Xiangrui and Xin, Reynold and Ghodsi, Ali and
                  Franklin, Michael and Stoica, Ion and Zaharia,
                  Matei},
  booktitle =	 {Proceedings of the 2016 International Conference on
                  Management of Data},
  pages =	 {1099--1104},
  year =	 2016
}

@inproceedings{mohoney2021marius,
  title =	 {Marius: Learning Massive Graph Embeddings on a
                  Single Machine},
  author =	 {Mohoney, Jason and Waleffe, Roger and Xu, Henry and
                  Rekatsinas, Theodoros and Venkataraman, Shivaram},
  booktitle =	 {15th USENIX Symposium on Operating Systems Design
                  and Implementation (OSDI 21)},
  pages =	 {533--549},
  year =	 2021
}

@inproceedings {aarati-hotcloud,
  author =	 {Aarati Kakaraparthy and Abhay Venkatesh and Amar
                  Phanishayee and Shivaram Venkataraman},
  title =	 {The Case for Unifying Data Loading in Machine
                  Learning Clusters},
  booktitle =	 {11th {USENIX} Workshop on Hot Topics in Cloud
                  Computing (HotCloud 19)},
  year =	 2019,
  month =	 jul,
}

@misc{accordion-opensource,
  title =	 {{Source code for Accordion: Adaptive Gradient
                  Communication via Critical Learning Regime
                  Identification}},
  howpublished = {\url{https://github.com/uw-mad-dash/accordion}},
  year =	 2021,
  key =		 {accordion}
},

@misc{gati-opensource,
  title =	 {{Source code for Gati: Accelerating Deep Learning
                  Inference via Learned Caches}},
  howpublished = {\url{https://github.com/uw-mad-dash/gati-code}},
  year =	 2021,
  key =		 {gati}
}

@inproceedings{branzei2014fisher,
  title =	 {The fisher market game: Equilibrium and welfare},
  author =	 {Br{\^a}nzei, Simina and Chen, Yiling and Deng,
                  Xiaotie and Filos-Ratsikas, Aris and Frederiksen,
                  S{\o}ren and Zhang, Jie},
  booktitle =	 {Proceedings of the AAAI Conference on Artificial
                  Intelligence},
  volume =	 28,
  year =	 2014
}

@inproceedings {gavel20,
  author =	 {Deepak Narayanan and Keshav Santhanam and Fiodar
                  Kazhamiaka and Amar Phanishayee and Matei Zaharia},
  title =	 {Heterogeneity-Aware Cluster Scheduling Policies for
                  Deep Learning Workloads},
  booktitle =	 {14th {USENIX} Symposium on Operating Systems Design
                  and Implementation ({OSDI} 20)},
  year =	 2020,
  pages =	 {481--498},
  month =	 nov,
}

@inproceedings{chaudhary2020balancing,
  title =	 {Balancing efficiency and fairness in heterogeneous
                  GPU clusters for deep learning},
  author =	 {Chaudhary, Shubham and Ramjee, Ramachandran and
                  Sivathanu, Muthian and Kwatra, Nipun and Viswanatha,
                  Srinidhi},
  booktitle =	 {Proceedings of the Fifteenth European Conference on
                  Computer Systems},
  pages =	 {1--16},
  year =	 2020
}

@InProceedings{OtternessAnderson2020-cuMask,
  author =	 {Nathan Otterness and James H. Anderson},
  title =	 {{AMD GPUs as an Alternative to NVIDIA for Supporting
                  Real-Time Workloads}},
  booktitle =	 {32nd Euromicro Conference on Real-Time Systems},
  pages =	 {10:1--10:23},
  series =	 {ECRTS},
  ISBN =	 {978-3-95977-152-8},
  ISSN =	 {1868-8969},
  year =	 2020,
  volume =	 165,
  editor =	 {Marcus V{\"o}lp},
  publisher =	 {Schloss Dagstuhl--Leibniz-Zentrum f{\"u}r
                  Informatik},
  address =	 {Dagstuhl, Germany},
  URL =		 {https://drops.dagstuhl.de/opus/volltexte/2020/12373},
  URN =		 {urn:nbn:de:0030-drops-123732},
  doi =		 {10.4230/LIPIcs.ECRTS.2020.10},
  annote =	 {Keywords: real-time systems, graphics processing
                  units, parallel computing}
}

@inproceedings {XiaoRen2020-antMan,
  author =	 {Wencong Xiao and Shiru Ren and Yong Li and Yang
                  Zhang and Pengyang Hou and Zhi Li and Yihui Feng and
                  Wei Lin and Yangqing Jia},
  title =	 {{AntMan: Dynamic Scaling on GPU Clusters for Deep
                  Learning}},
  booktitle =	 {{14th USENIX Symposium on Operating Systems Design
                  and Implementation}},
  series =	 {OSDI},
  year =	 2020,
  isbn =	 {978-1-939133-19-9},
  pages =	 {533--548},
  url =
                  {https://www.usenix.org/conference/osdi20/presentation/xiao},
  publisher =	 {USENIX Association},
  month =	 nov,
}

@article{basar2010lecture,
  title =	 {Lecture notes on non-cooperative game theory},
  author =	 {Basar, Tamer and others},
  journal =	 {Game Theory Module of the Graduate Program in
                  Network Mathematics},
  pages =	 {3--6},
  year =	 2010
}

@article{thompson2022lammps,
  title =	 {LAMMPS-a flexible simulation tool for particle-based
                  materials modeling at the atomic, meso, and
                  continuum scales},
  author =	 {Thompson, Aidan P and Aktulga, H Metin and Berger,
                  Richard and Bolintineanu, Dan S and Brown, W Michael
                  and Crozier, Paul S and in't Veld, Pieter J and
                  Kohlmeyer, Axel and Moore, Stan G and Nguyen, Trung
                  Dac and others},
  journal =	 {Computer Physics Communications},
  volume =	 271,
  pages =	 108171,
  year =	 2022,
  publisher =	 {Elsevier}
}

@article{van2005gromacs,
  title =	 {GROMACS: fast, flexible, and free},
  author =	 {Van Der Spoel, David and Lindahl, Erik and Hess,
                  Berk and Groenhof, Gerrit and Mark, Alan E and
                  Berendsen, Herman JC},
  journal =	 {Journal of computational chemistry},
  volume =	 26,
  number =	 16,
  pages =	 {1701--1718},
  year =	 2005,
  publisher =	 {Wiley Online Library}
}

@article{case2005amber,
  title =	 {The Amber biomolecular simulation programs},
  author =	 {Case, David A and Cheatham III, Thomas E and Darden,
                  Tom and Gohlke, Holger and Luo, Ray and Merz Jr,
                  Kenneth M and Onufriev, Alexey and Simmerling,
                  Carlos and Wang, Bing and Woods, Robert J},
  journal =	 {Journal of computational chemistry},
  volume =	 26,
  number =	 16,
  pages =	 {1668--1688},
  year =	 2005,
  publisher =	 {Wiley Online Library}
}

@misc{brace2021achieving,
  title =	 {{Achieving 100X faster simulations of complex
                  biological phenomena by coupling ML to HPC
                  ensembles}},
  author =	 {Alexander Brace and Hyungro Lee and Heng Ma and Anda
                  Trifan and Matteo Turilli and Igor Yakushin and Todd
                  Munson and Ian Foster and Shantenu Jha and Arvind
                  Ramanathan},
  year =	 2021,
  eprint =	 {2104.04797},
  archivePrefix ={arXiv},
  primaryClass = {cs.DC}
}

@article{singhvi2019archipelago,
  title =	 {Archipelago: A Scalable Low-Latency Serverless
                  Platform},
  author =	 {Singhvi, Arjun and Houck, Kevin and Balasubramanian,
                  Arjun and Shaikh, Mohammed Danish and Venkataraman,
                  Shivaram and Akella, Aditya},
  journal =	 {arXiv preprint arXiv:1911.09849},
  year =	 2019
}

@article{mohanty2016using,
  title =	 {Using deep learning for image-based plant disease
                  detection},
  author =	 {Mohanty, Sharada P and Hughes, David P and
                  Salath{\'e}, Marcel},
  journal =	 {Frontiers in plant science},
  volume =	 7,
  pages =	 1419,
  year =	 2016,
  publisher =	 {frontiers}
}

@InProceedings{DeakinPrice2016-gpuStream,
  author =	 "Deakin, Tom and Price, James and Martineau, Matt and
                  McIntosh-Smith, Simon",
  editor =	 "Taufer, Michela and Mohr, Bernd and Kunkel, Julian
                  M.",
  title =	 "GPU-STREAM v2.0: Benchmarking the Achievable Memory
                  Bandwidth of Many-Core Processors Across Diverse
                  Parallel Programming Models",
  booktitle =	 "High Performance Computing",
  year =	 2016,
  publisher =	 "Springer International Publishing",
  address =	 "Cham",
  pages =	 "489--507",
  abstract =	 "Many scientific codes consist of memory bandwidth
                  bound kernels --- the dominating factor of the
                  runtime is the speed at which data can be loaded
                  from memory into the Arithmetic Logic Units, before
                  results are written back to memory. One major
                  advantage of many-core devices such as General
                  Purpose Graphics Processing Units (GPGPUs) and the
                  Intel Xeon Phi is their focus on providing increased
                  memory bandwidth over traditional CPU
                  architectures. However, as with CPUs, this peak
                  memory bandwidth is usually unachievable in practice
                  and so benchmarks are required to measure a
                  practical upper bound on expected performance.",
  isbn =	 "978-3-319-46079-6"
}

@article{zhang2022opt,
  title =	 {{Opt: Open Pre-trained Transformer Language Models}},
  author =	 {Zhang, Susan and Roller, Stephen and Goyal, Naman
                  and Artetxe, Mikel and Chen, Moya and Chen, Shuohui
                  and Dewan, Christopher and Diab, Mona and Li, Xian
                  and Lin, Xi Victoria and others},
  journal =	 {arXiv preprint arXiv:2205.01068},
  year =	 2022
}

@article{dosovitskiy2020image,
  title =	 {An image is worth 16x16 words: Transformers for
                  image recognition at scale},
  author =	 {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov,
                  Alexander and Weissenborn, Dirk and Zhai, Xiaohua
                  and Unterthiner, Thomas and Dehghani, Mostafa and
                  Minderer, Matthias and Heigold, Georg and Gelly,
                  Sylvain and others},
  journal =	 {arXiv preprint arXiv:2010.11929},
  year =	 2020
}

@article{ahdritz2022openfold,
  title =	 {{OpenFold: Retraining AlphaFold2 yields new insights
                  into its learning mechanisms and capacity for
                  generalization}},
  author =	 {Ahdritz, Gustaf and Bouatta, Nazim and Kadyan,
                  Sachin and Xia, Qinghui and Gerecke, William and
                  O'Donnell, Timothy J and Berenberg, Daniel and Fisk,
                  Ian and Zanichelli, Niccola and Zhang, Bo and
                  others},
  journal =	 {bioRxiv},
  year =	 2022,
  publisher =	 {Cold Spring Harbor Laboratory}
}

@misc{nsight,
  title =	 {{NVIDIA Nsight Systems}},
  author =	 {NVIDIA},
  year =	 2022,
  howpublished = {\url{https://developer.nvidia.com/nsight-systems}},
}

@misc{omnitrace,
  title =	 {{Omnitrace: Application Profiling, Tracing, and
                  Analysis}},
  author =	 {Madsen, Jonathan R.},
  year =	 2022,
  howpublished = {\url{https://github.com/AMDResearch/omnitrace}},
}

@misc{nvidia-smi,
  author =	 {{NVIDIA}},
  title =	 {{NVIDIA System Management Interface}},
  howpublished =
                  {\url{https://developer.nvidia.com/nvidia-system-management-interface}},
  year =	 2020,
}

@misc{rocm-smi,
  author =	 {{Advanced Micro Devices, Inc}},
  title =	 {{ROCm Command Line Interface}},
  howpublished =
                  {\url{https://rocmdocs.amd.com/en/latest/ROCm_System_Managment/ROCm-SMI-CLI.html}},
  year =	 2022,
}

@inproceedings{Scogland2014-MeasurementLevels,
  author =	 {Scogland, Thomas R.W. and Steffen, Craig P. and
                  Wilde, Torsten and Parent, Florent and Coghlan,
                  Susan and Bates, Natalie and Feng, Wu-chun and
                  Strohmaier, Erich},
  title =	 {{A Power-Measurement Methodology for Large-Scale,
                  High-Performance Computing}},
  year =	 2014,
  isbn =	 9781450327336,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/2568088.2576795},
  doi =		 {10.1145/2568088.2576795},
  abstract =	 {Improvement in the energy efficiency of
                  supercomputers can be accelerated by improving the
                  quality and comparability of efficiency
                  measurements. The ability to generate accurate
                  measurements at extreme scale are just now
                  emerging. The realization of system-level
                  measurement capabilities can be accelerated with a
                  commonly adopted and high quality measurement
                  methodology for use while running a workload,
                  typically a benchmark. This paper describes a
                  methodology that has been developed collaboratively
                  through the Energy Efficient HPC Working Group to
                  support architectural analysis and comparative
                  measurements for rankings, such as the Top500 and
                  Green500. To support measurements with varying
                  amounts of effort and equipment required we present
                  three distinct levels of measurement, which provide
                  increasing levels of accuracy. Level 1 is similar to
                  the Green500 run rules today, a single average power
                  measurement extrapolated from a subset of a
                  machine. Level 2 is more comprehensive, but still
                  widely achievable. Level 3 is the most rigorous of
                  the three methodologies but is only possible at a
                  few sites. However, the Level 3 methodology
                  generates a high quality result that exposes details
                  that the other methodologies may miss. In addition,
                  we present case studies from the Leibniz
                  Supercomputing Centre (LRZ), Argonne National
                  Laboratory (ANL) and Calcul Qu\'{e}bec
                  Universit\'{e} Laval that explore the benefits and
                  difficulties of gathering high quality, system-level
                  measurements on large-scale machines.},
  booktitle =	 {{Proceedings of the 5th ACM/SPEC International
                  Conference on Performance Engineering}},
  pages =	 {149–159},
  numpages =	 11,
  keywords =	 {green500, top500, power-measurement methodology,
                  datacenter, high-performance computing},
  location =	 {Dublin, Ireland},
  series =	 {ICPE '14}
}

@inproceedings{EsmaeilzadehBlem2011-darkSilicon,
  author =	 {Esmaeilzadeh, Hadi and Blem, Emily and St. Amant,
                  Renee and Sankaralingam, Karthikeyan and Burger,
                  Doug},
  title =	 {{Dark silicon and the End of Multicore Scaling}},
  year =	 2011,
  isbn =	 9781450304726,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/2000064.2000108},
  doi =		 {10.1145/2000064.2000108},
  abstract =	 {Since 2005, processor designers have increased core
                  counts to exploit Moore's Law scaling, rather than
                  focusing on single-core performance. The failure of
                  Dennard scaling, to which the shift to multicore
                  parts is partially a response, may soon limit
                  multicore scaling just as single-core scaling has
                  been curtailed. This paper models multicore scaling
                  limits by combining device scaling, single-core
                  scaling, and multicore scaling to measure the
                  speedup potential for a set of parallel workloads
                  for the next five technology generations. For device
                  scaling, we use both the ITRS projections and a set
                  of more conservative device scaling parameters. To
                  model single-core scaling, we combine measurements
                  from over 150 processors to derive Pareto-optimal
                  frontiers for area/performance and
                  power/performance. Finally, to model multicore
                  scaling, we build a detailed performance model of
                  upper-bound performance and lower-bound core
                  power. The multicore designs we study include
                  single-threaded CPU-like and massively threaded
                  GPU-like multicore chip organizations with
                  symmetric, asymmetric, dynamic, and composed
                  topologies. The study shows that regardless of chip
                  organization and topology, multicore scaling is
                  power limited to a degree not widely appreciated by
                  the computing community. Even at 22 nm (just one
                  year from now), 21\% of a fixed-size chip must be
                  powered off, and at 8 nm, this number grows to more
                  than 50\%. Through 2024, only 7.9x average speedup
                  is possible across commonly used parallel workloads,
                  leaving a nearly 24-fold gap from a target of
                  doubled performance per generation.},
  booktitle =	 {{Proceedings of the 38th Annual International
                  Symposium on Computer Architecture}},
  pages =	 {365–376},
  numpages =	 12,
  keywords =	 {dark silicon, modeling, multicore, power, technology
                  scaling},
  location =	 {San Jose, California, USA},
  series =	 {ISCA '11}
}

@inproceedings{Scogland2015-pwrPerspectives,
  author =	 {Scogland, Thomas and Azose, Jonathan and Rohr, David
                  and Rivoire, Suzanne and Bates, Natalie and
                  Hackenberg, Daniel},
  title =	 {{Node Variability in Large-Scale Power Measurements:
                  Perspectives from the Green500, Top500 and EEHPCWG}},
  year =	 2015,
  isbn =	 9781450337236,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/2807591.2807653},
  doi =		 {10.1145/2807591.2807653},
  abstract =	 {The last decade has seen power consumption move from
                  an afterthought to the foremost design constraint of
                  new supercomputers. Measuring the power of a
                  supercomputer can be a daunting proposition, and as
                  a result, many published measurements are
                  extrapolated. This paper explores the validity of
                  these extrapolations in the context of inter-node
                  power variability and power variations over time
                  within a run. We characterize power variability
                  across nodes in systems at eight supercomputer
                  centers across the globe. This characterization
                  shows that the current requirement for measurements
                  submitted to the Green500 and others is
                  insufficient, allowing variations of up to 20\% due
                  to measurement timing and a further 10--15\% due to
                  insufficient sample sizes. This paper proposes new
                  power and energy measurement requirements for
                  supercomputers, some of which have been accepted for
                  use by the Green500 and Top500, to ensure consistent
                  accuracy.},
  booktitle =	 {{Proceedings of the International Conference for
                  High Performance Computing, Networking, Storage and
                  Analysis}},
  articleno =	 74,
  numpages =	 11,
  location =	 {Austin, Texas},
  series =	 {SC '15}
}

@inproceedings{PatelWagenhauser2020-hpcPowerConsump,
  author =	 {Patel, Tirthak and Wagenhäuser, Adam and Eibel,
                  Christopher and Hönig, Timo and Zeiser, Thomas and
                  Tiwari, Devesh},
  booktitle =	 {{IEEE International Parallel and Distributed
                  Processing Symposium}},
  series =	 {IPDPS},
  title =	 {{What does Power Consumption Behavior of HPC Jobs
                  Reveal? : Demystifying, Quantifying, and Predicting
                  Power Consumption Characteristics}},
  year =	 2020,
  pages =	 {799-809},
  doi =		 {10.1109/IPDPS47924.2020.00087}
}

@misc{vtune,
  author =	 {{Intel}},
  title =	 {{Intel VTune Profiler: Find and Fix Performance
                  Bottlenecks Quickly and Realize All the Value of
                  Your Hardware}},
  howpublished =
                  {\url{https://www.intel.com/content/www/us/en/developer/tools/oneapi/vtune-profiler.html#gs.ldsbw8}},
  year =	 2022,
}

@misc{graphcore-prof,
  author =	 {{Graphcore}},
  title =	 {{Poplar: Profiling}},
  howpublished =
                  {\url{https://docs.graphcore.ai/projects/poplar-user-guide/en/latest/profiler.html}},
  year =	 2022,
}

@misc{PattersonGonzalez2021-mlCarbon,
  doi =		 {10.48550/ARXIV.2104.10350},
  url =		 {https://arxiv.org/abs/2104.10350},
  author =	 {Patterson, David and Gonzalez, Joseph and Le, Quoc
                  and Liang, Chen and Munguia, Lluis-Miquel and
                  Rothchild, Daniel and So, David and Texier, Maud and
                  Dean, Jeff},
  keywords =	 {Machine Learning (cs.LG), Computers and Society
                  (cs.CY), FOS: Computer and information sciences,
                  FOS: Computer and information sciences},
  title =	 {{Carbon Emissions and Large Neural Network
                  Training}},
  publisher =	 {arXiv},
  year =	 2021,
  copyright =	 {arXiv.org perpetual, non-exclusive license}
}

@misc{kecklerpicojoule,
  author =	 {Keckler, Stephen W.},
  title =	 {{Life After Dennard and How I Learned to Love the
                  Picojoule}},
  howpublished = {Keynote at MICRO},
  year =	 2011,
}

@misc{uarch,
  title =	 {{Second Undergrad Architecture Mentoring Workshop
                  (uArch)}},
  year =	 2020,
  howpublished = {\url{https://sites.google.com/view/uarch2020/home}},
}

@misc{sarita-will,
  title =	 {{Illinois Public Media: Women And Computer Science}},
  author =	 {WILL},
  year =	 2016,
  howpublished =
                  {\url{https://will.illinois.edu/player/audio/homelessness-prevention-women-and-computer-science-urban-gardening}},
}

@misc{taulbeeSurvey2019,
  title =	 {{2019 Taulbee Survey}},
  author =	 {CRA},
  year =	 2019,
  howpublished =
                  {\url{https://cra.org/wp-content/uploads/2020/05/2019-Taulbee-Survey.pdf}},
}

@misc{sigarch-pipelineProb,
  title =	 {{Tackling the Pipeline Problem in the Architecture
                  Research Community}},
  author =	 {Olson, Lena and Ardlani, Newsha},
  howpublished =
                  {\url{https://www.sigarch.org/tackling-the-pipeline-problem-in-the-architecture-research-community/}},
  year =	 2019,
  month =	 {April},
}

@misc{parityGame,
  title =	 {{Parity Magic}},
  author =	 {CS Unplugged},
  year =	 2022,
  howpublished =
                  {\url{https://csunplugged.org/en/topics/error-detection-and-correction/unit-plan/parity-magic/}},
}

@misc{BharadwajDas2022-gpuDVFS,
  doi =		 {10.48550/ARXIV.2205.00121},
  url =		 {https://arxiv.org/abs/2205.00121},
  author =	 {Bharadwaj, Srikant and Das, Shomit and Mazumdar,
                  Kaushik and Beckmann, Bradford and Kosonocky,
                  Stephen},
  keywords =	 {Hardware Architecture (cs.AR), FOS: Computer and
                  information sciences, FOS: Computer and information
                  sciences},
  title =	 {{Predict; Do not React for Enabling Efficient Fine
                  Grain DVFS in GPUs}},
  publisher =	 {arXiv},
  year =	 2022,
}

@INPROCEEDINGS{MeinerzhagenTokunaga2018-gpuDVFS,
  author =	 {Meinerzhagen, Pascal and Tokunaga, Carlos and
                  Malavasi, Andres and Vaidya, Vaibhav and Mendon,
                  Ashwin and Mathaikutty, Deepak and Kulkarni, Jaydeep
                  and Augustine, Charles and Cho, Minki and Kim,
                  Stephen and Matthew, George and Jain, Rinkle and
                  Ryan, Joseph and Peng, Chung-Ching and Paul, Somnath
                  and Vangal, Sriram and Esparza, Brando Perez and
                  Cuellar, Luis and Woodman, Michael and Iyer, Bala
                  and Maiyuran, Subramaniam and Chinya, Gautham and
                  Zou, Chris and Liao, Yuyun and Ravichandran,
                  Krishnan and Wang, Hong and Khellah, Muhammad and
                  Tschanz, James and De, Vivek},
  booktitle =	 {{IEEE International Solid - State Circuits
                  Conference}},
  series =	 {ISSCC},
  title =	 {{An energy-efficient graphics processor featuring
                  fine-grain DVFS with integrated voltage regulators,
                  execution-unit turbo, and retentive sleep in 14nm
                  tri-gate CMOS}},
  year =	 2018,
  pages =	 {38-40},
  doi =		 {10.1109/ISSCC.2018.8310172}
}

@inproceedings{NathTullsen2015-crisp,
  author =	 {Nath, Rajib and Tullsen, Dean},
  title =	 {{The CRISP Performance Model for Dynamic Voltage and
                  Frequency Scaling in a GPGPU}},
  year =	 2015,
  isbn =	 9781450340342,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/2830772.2830826},
  doi =		 {10.1145/2830772.2830826},
  abstract =	 {This paper presents CRISP, the first runtime
                  analytical model of performance in the face of
                  changing frequency in a GPGPU. It shows that prior
                  models not targeted at a GPGPU fail to account for
                  important characteristics of GPGPU execution,
                  including the high degree of overlap between memory
                  access and computation and the frequency of
                  store-related stalls.CRISP provides significantly
                  greater accuracy than prior runtime performance
                  models, being within 4\% on average when scaling
                  frequency by up to 7X. Using CRISP to drive a
                  runtime energy efficiency controller yields a 10.7\%
                  improvement in energy-delay product, vs 6.2\%
                  attainable via the best prior performance model.},
  booktitle =	 {{Proceedings of the 48th International Symposium on
                  Microarchitecture}},
  pages =	 {281–293},
  numpages =	 13,
  keywords =	 {DVFS, GPGPU, critical path},
  location =	 {Waikiki, Hawaii},
  series =	 {MICRO}
}

@inproceedings{MeiYung2013-gpuDVFSMeasure,
  author =	 {Mei, Xinxin and Yung, Ling Sing and Zhao, Kaiyong
                  and Chu, Xiaowen},
  title =	 {{A Measurement Study of GPU DVFS on Energy
                  Conservation}},
  year =	 2013,
  isbn =	 9781450324588,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/2525526.2525852},
  doi =		 {10.1145/2525526.2525852},
  abstract =	 {Nowadays, GPUs are widely used to accelerate many
                  high performance computing applications. Energy
                  conservation of such computing systems has become an
                  important research topic. Dynamic voltage/frequency
                  scaling (DVFS) is proved to be an appealing method
                  for saving energy for traditional computing
                  centers. However, there is still a lack of firsthand
                  study on the effectiveness of GPU DVFS. This paper
                  presents a thorough measurement study that aims to
                  explore how GPU DVFS affects the system energy
                  consumption. We conduct experiments on a real GPU
                  platform with 37 benchmark applications. Our results
                  show that GPU voltage/frequency scaling is an
                  effective approach to conserving energy. For
                  example, by scaling down the GPU core voltage and
                  frequency, we have achieved an average of 19.28\%
                  energy reduction compared with the default setting,
                  while giving up no more than 4\% of performance. For
                  all tested GPU applications, core voltage scaling is
                  significantly effective to reduce system energy
                  consumption. Meanwhile the effects of scaling core
                  frequency and memory frequency depend on the
                  characteristics of GPU applications.},
  booktitle =	 {{Proceedings of the Workshop on Power-Aware
                  Computing and Systems}},
  articleno =	 10,
  numpages =	 5,
  keywords =	 {voltage/frequency scaling, energy conservation, GPU},
  location =	 {Farmington, Pennsylvania},
  series =	 {HotPower}
}

@ARTICLE{LengBuyuktosunoglu2021-gpuGuardband,
  author =	 {Leng, Jingwen and Buyuktosunoglu, Alper and Bertran,
                  Ramon and Bose, Pradip and Zu, Yazhou and Reddi,
                  Vijay Janapa},
  journal =	 {{IEEE Transactions on Computer-Aided Design of
                  Integrated Circuits and Systems}},
  title =	 {{Predictive Guardbanding: Program-Driven Timing
                  Margin Reduction for GPUs}},
  year =	 2021,
  volume =	 40,
  number =	 1,
  pages =	 {171-184},
  doi =		 {10.1109/TCAD.2020.2992684}
}

@ARTICLE{KimShih2016-voltageReg,
  author =	 {Kim, Stephen T. and Shih, Yi-Chun and Mazumdar,
                  Kaushik and Jain, Rinkle and Ryan, Joseph F. and
                  Tokunaga, Carlos and Augustine, Charles and
                  Kulkarni, Jaydeep P. and Ravichandran, Krishnan and
                  Tschanz, James W. and Khellah, Muhammad M. and De,
                  Vivek},
  journal =	 {{IEEE Journal of Solid-State Circuits}},
  title =	 {{Enabling Wide Autonomous DVFS in a 22 nm Graphics
                  Execution Core Using a Digitally Controlled Fully
                  Integrated Voltage Regulator}},
  year =	 2016,
  volume =	 51,
  number =	 1,
  pages =	 {18-30},
  doi =		 {10.1109/JSSC.2015.2457920}
}

@INPROCEEDINGS{StraunbeLowePower2018-gpuCAPP,
  author =	 {Straube, Kramer and Lowe-Power, Jason and Nitta,
                  Christopher and Farrens, Matthew and Akella,
                  Venkatesh},
  booktitle =	 {{IEEE 25th International Conference on High
                  Performance Computing}},
  series =	 {HiPC},
  title =	 {{Improving Provisioned Power Efficiency in HPC
                  Systems with GPU-CAPP}},
  year =	 2018,
  pages =	 {112-122},
  doi =		 {10.1109/HiPC.2018.00021}
}

@inproceedings{NugterenVanDenBraak2014-rooflineGPUDVFS,
  author =	 {Nugteren, Cedric and van den Braak, Gert-Jan and
                  Corporaal, Henk},
  title =	 {{Roofline-Aware DVFS for GPUs}},
  year =	 2014,
  isbn =	 9781450325141,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/2553062.2553067},
  doi =		 {10.1145/2553062.2553067},
  abstract =	 {Graphics processing units (GPUs) are becoming
                  increasingly popular for compute workloads, mainly
                  because of their large number of processing elements
                  and high-bandwidth to off-chip memory. The roofline
                  model captures the ratio between the two (the
                  compute-memory ratio), an important architectural
                  parameter. This work proposes to change the
                  compute-memory ratio dynamically, scaling the
                  voltage and frequency (DVFS) of 1) memory for
                  compute-intensive workloads and 2) processing
                  elements for memory-intensive workloads. The result
                  is an adaptive roofline-aware GPU that increases
                  energy efficiency (up to 58\%) while maintaining
                  performance.},
  booktitle =	 {{Proceedings of International Workshop on Adaptive
                  Self-Tuning Computing Systems}},
  pages =	 {8–10},
  numpages =	 3,
  keywords =	 {Parallel Computing, GPU, The Roofline Model, DVFS},
  location =	 {Vienna, Austria},
  series =	 {ADAPT}
}

@misc{MishraKhare2016-gpuDVFSEval,
  title =	 {{Analysis of DVFS Techniques for Improving the GPU
                  Energy Efficiency}},
  author =	 {Mishra, Ashish and Khare, Nilay},
  year =	 2015,
  howpublished =
                  {\url{http://file.scirp.org/pdf/ojee_2015121415504865.pdf}},
}

@inproceedings{TangWang2019-gpuDVFSDL,
  author =	 {Tang, Zhenheng and Wang, Yuxin and Wang, Qiang and
                  Chu, Xiaowen},
  title =	 {{The Impact of GPU DVFS on the Energy and
                  Performance of Deep Learning: An Empirical Study}},
  year =	 2019,
  isbn =	 9781450366717,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/3307772.3328315},
  doi =		 {10.1145/3307772.3328315},
  abstract =	 {Over the past years, great progress has been made in
                  improving the computing power of general-purpose
                  graphics processing units (GPGPUs), which
                  facilitates the prosperity of deep neural networks
                  (DNNs) in multiple fields like computer vision and
                  natural language processing. A typical DNN training
                  process repeatedly updates tens of millions of
                  parameters, which not only requires huge computing
                  resources but also consumes significant energy. In
                  order to train DNNs in a more energy-efficient way,
                  we empirically investigate the impact of GPU Dynamic
                  Voltage and Frequency Scaling (DVFS) on the energy
                  consumption and performance of deep learning. Our
                  experiments cover a wide range of GPU architectures,
                  DVFS settings, and DNN configurations. We observe
                  that, compared to the default core frequency
                  settings of three tested GPUs, the optimal core
                  frequency can help conserve 8.7\%~23.1\% energy
                  consumption for different DNN training
                  cases. Regarding the inference, the benefits vary
                  from 19.6\%~26.4\%. Our findings suggest that GPU
                  DVFS has great potentials to help develop energy
                  efficient DNN training/inference schemes.},
  booktitle =	 {{Proceedings of the Tenth ACM International
                  Conference on Future Energy Systems}},
  pages =	 {315–325},
  numpages =	 11,
  keywords =	 {Graphics Processing Units, Dynamic Voltage and
                  Frequency Scaling, Deep Convolutional Neural
                  Network},
  location =	 {Phoenix, AZ, USA},
  series =	 {e-Energy}
}

@inproceedings{DuttaAdhinarayanan2018-gpuPowerPredML,
  author =	 {Dutta, Bishwajit and Adhinarayanan, Vignesh and
                  Feng, Wu-chun},
  title =	 {{GPU Power Prediction via Ensemble Machine Learning
                  for DVFS Space Exploration}},
  year =	 2018,
  isbn =	 9781450357616,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/3203217.3203273},
  doi =		 {10.1145/3203217.3203273},
  abstract =	 {A software-based approach to achieve high
                  performance within a power budget often involves
                  dynamic voltage and frequency scaling (DVFS). Thus,
                  accurately predicting the power consumption of an
                  application at different DVFS levels (or more
                  generally, different processor configurations) is
                  paramount for the energy-efficient functioning of a
                  high-performance computing (HPC) system. The
                  increasing prevalence of graphics processing units
                  (GPUs) in HPC systems presents new challenges in
                  power management, and machine learning presents an
                  unique way to improve the software-based power
                  management of these systems. As such, we explore the
                  problem of GPU power prediction at different DVFS
                  states via machine learning. Specifically, we
                  propose a new ensemble technique that incorporates
                  three machine-learning techniques --- sequential
                  minimal optimization regression, simple linear
                  regression, and decision tree --- to reduce the mean
                  absolute error (MAE) to 3.5\%.},
  booktitle =	 {{Proceedings of the 15th ACM International
                  Conference on Computing Frontiers}},
  pages =	 {240–243},
  numpages =	 4,
  location =	 {Ischia, Italy},
  series =	 {CF}
}

@inproceedings{FanCosenza2019-predGPUFreq,
  author =	 {Fan, Kaijie and Cosenza, Biagio and Juurlink, Ben},
  title =	 {{Predictable GPUs Frequency Scaling for Energy and
                  Performance}},
  year =	 2019,
  isbn =	 9781450362955,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/3337821.3337833},
  doi =		 {10.1145/3337821.3337833},
  abstract =	 {Dynamic voltage and frequency scaling (DVFS) is an
                  important solution to balance performance and energy
                  consumption, and hardware vendors provide management
                  libraries that allow the programmer to change both
                  memory and core frequencies. The possibility to
                  manually set these frequencies is a great
                  opportunity for application tuning, which can focus
                  on the best application-dependent setting. However,
                  this task is not straightforward because of the
                  large set of possible configurations and because of
                  the multi-objective nature of the problem, which
                  minimizes energy consumption and maximizes
                  performance.This paper proposes a method to predict
                  the best core and memory frequency configurations on
                  GPUs for an input OpenCL kernel. Our modeling
                  approach, based on machine learning, first predicts
                  speedup and normalized energy over the default
                  frequency configuration. Then, it combines the two
                  models into a multi-objective one that predicts a
                  Pareto-set of frequency configurations. The approach
                  uses static code features, is built on a set of
                  carefully designed micro-benchmarks, and can predict
                  the best frequency settings of a new kernel without
                  executing it. Test results show that our modeling
                  approach is very accurate on predicting extrema
                  points and Pareto set for ten out of twelve test
                  benchmarks, and discover frequency configurations
                  that dominate the default configuration in either
                  energy or performance.},
  booktitle =	 {{Proceedings of the 48th International Conference on
                  Parallel Processing}},
  articleno =	 52,
  numpages =	 10,
  keywords =	 {GPUs, Frequency scaling, Modeling, Energy
                  efficiency},
  location =	 {Kyoto, Japan},
  series =	 {ICPP 2019}
}

@misc{amd-power,
  author =	 {Chandrasekhar, Mithun},
  year =	 2019,
  title =	 {{AMD Radeon Community Update: More Control Over GPU
                  Power and Performance, Enhanced Thermal Monitoring,
                  Maximized Performance}},
  howpublished =
                  {\url{https://community.amd.com/t5/gaming/amd-radeon-community-update-more-control-over-gpu-power-and/ba-p/418629}},
}

@misc{vega11-maxTemp,
  author =	 {{SafeTemp}},
  title =	 {{AMD Radeon RX Vega 11 Max Temp}},
  year =	 2021,
  month =	 {November},
  howpublished =
                  {\url{https://safetemp.blogspot.com/2021/11/amd-radeon-rx-vega-11-max-temp.html}},
}

@article{Guerreiro-appClasses,
  title =	 {{DVFS-aware application classification to improve
                  GPGPUs energy efficiency}},
  journal =	 {Parallel Computing},
  volume =	 83,
  pages =	 {93-117},
  year =	 2019,
  issn =	 {0167-8191},
  doi =		 {https://doi.org/10.1016/j.parco.2018.02.001},
  url =
                  {https://www.sciencedirect.com/science/article/pii/S0167819118300243},
  author =	 {João Guerreiro and Aleksandar Ilic and Nuno Roma and
                  Pedro Tomás},
  keywords =	 {GPGPU, Application classification, DVFS, Optimal
                  frequency, Energy savings},
}

@misc{DRAMthermalissues,
  author =	 {Karen Heyman},
  title =	 {{DRAM Thermal Issues Reach Crisis Point}},
  howpublished =
                  {\url{https://semiengineering.com/dram-thermal-issues-reach-crisis-point/}},
  year =	 2022
}

@misc{SinhaGuliani2022-gpuPowerVar,
  doi =		 {10.48550/ARXIV.2208.11035},
  url =		 {https://arxiv.org/abs/2208.11035},
  author =	 {Sinha, Prasoon and Guliani, Akhil and Jain, Rutwik
                  and Tran, Brandon and Sinclair, Matthew D. and
                  Venkataraman, Shivaram},
  keywords =	 {Distributed, Parallel, and Cluster Computing
                  (cs.DC), FOS: Computer and information sciences,
                  FOS: Computer and information sciences},
  title =	 {{Not All GPUs Are Created Equal: Characterizing
                  Variability in Large-Scale, Accelerator-Rich
                  Systems}},
  publisher =	 {arXiv},
  year =	 2022,
  copyright =	 {arXiv.org perpetual, non-exclusive license}
}

@article{AcunLanger2016-power,
  author =	 {Acun, Bilge and Langer, Akhil and Meneses, Esteban
                  and Menon, Harshitha and Sarood, Osman and Totoni,
                  Ehsan and Kalé, Laxmikant V.},
  journal =	 {Computer},
  title =	 {{Power, Reliability, and Performance: One System to
                  Rule them All}},
  year =	 2016,
  volume =	 49,
  number =	 10,
  pages =	 {30-37},
  doi =		 {10.1109/MC.2016.310}
}

@inproceedings{AcunMiller2016-variationTurbo,
  author =	 {Acun, Bilge and Miller, Phil and Kale, Laxmikant V.},
  title =	 {{Variation Among Processors Under Turbo Boost in HPC
                  Systems}},
  year =	 2016,
  isbn =	 9781450343619,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/2925426.2926289},
  doi =		 {10.1145/2925426.2926289},
  abstract =	 {The design and manufacture of present-day CPUs
                  causes inherent variation in supercomputer
                  architectures such as variation in power and
                  temperature of the chips. The variation also
                  manifests itself as frequency differences among
                  processors under Turbo Boos\ t dynamic
                  overclocking. This variation can lead to
                  unpredictable and suboptimal performance in tightly
                  coupled HPC applications. In this study, we use
                  compute-intensive kernels and applications to
                  analyze the variation among processors in four top
                  supercomputers: Edison\ , Cab, Stampede, and Blue
                  Waters. We observe that there is an execution time
                  difference of up to 16\% among processors on the
                  Turbo Boost-enabled supercomputers: Edison, Cab,
                  Stampede. There is less than 1\% variation on Blue
                  Waters, which does not have a dynamic overcl\ ocking
                  feature. We analyze measurements from temperature
                  and power instrumentation and find that intrinsic
                  differences in the chips' power efficiency is the
                  culprit behind the frequency variation. Moreover, we
                  analyze potential solutions such as disabling Turbo
                  Boost, l\ eaving idle cores and replacing slow chips
                  to mitigate the variation. We also propose a
                  speed-aware dynamic task redistribution (load
                  balancing) algorithm to reduce the negative effects
                  of performance variation. Our speed-aware load
                  balancing algorithm improves the perf\ ormance up to
                  18\% compared to no load balancing performance and
                  6\% better than the non-speed aware counterpart.},
  booktitle =	 {{Proceedings of the 2016 International Conference on
                  Supercomputing}},
  articleno =	 6,
  numpages =	 12,
  location =	 {Istanbul, Turkey},
  series =	 {ICS '16}
}

@inproceedings{AcunKale2016-varLoadBal,
  author =	 {Acun, Bilge and Kale, Laxmikant V.},
  booktitle =	 {{IEEE International Parallel and Distributed
                  Processing Symposium Workshops}},
  series =	 {IPDPSW},
  title =	 {{Mitigating Processor Variation through Dynamic Load
                  Balancing}},
  year =	 2016,
  pages =	 {1073-1076},
  doi =		 {10.1109/IPDPSW.2016.74}
}

@inproceedings{AcunLee2017-proactiveCooling,
  author =	 {Acun, Bilge and Lee, Eun Kyung and Park, Yoonho and
                  Kale, Laxmikant V.},
  booktitle =	 {{IEEE 24th International Conference on High
                  Performance Computing}},
  series =	 {HiPC},
  title =	 {{Support for Power Efficient Proactive Cooling
                  Mechanisms}},
  year =	 2017,
  pages =	 {94-103},
  doi =		 {10.1109/HiPC.2017.00020}
}

@inproceedings{MenonAcun2013-thermAwareLoadBal,
  author =	 {Menon, Harshitha and Acun, Bilge and De Gonzalo,
                  Simon Garcia and Sarood, Osman and Kalé, Laxmikant},
  booktitle =	 {{IEEE International Conference on Cluster
                  Computing}},
  series =	 {CLUSTER},
  title =	 {{Thermal aware automated load balancing for HPC
                  applications}},
  year =	 2013,
  pages =	 {1-8},
  doi =		 {10.1109/CLUSTER.2013.6702627}
}

@inproceedings{SaroodLanger2014-throughputPowerBudget,
  author =	 {Sarood, Osman and Langer, Akhil and Gupta, Abhishek
                  and Kale, Laxmikant},
  booktitle =	 {{Proceedings of the International Conference for
                  High Performance Computing, Networking, Storage and
                  Analysis}},
  series =	 {SC},
  title =	 {{Maximizing Throughput of Overprovisioned HPC Data
                  Centers Under a Strict Power Budget}},
  year =	 2014,
  pages =	 {807-818},
  doi =		 {10.1109/SC.2014.71}
}

@inproceedings{AcunLee2016-nnFanControl,
  author =	 {Acun, Bilge and Lee, Eun Kyung and Park, Yoonho and
                  Kale, Laxmikant V.},
  booktitle =	 {{4th International Workshop on Energy Efficient
                  Supercomputing}},
  series =	 {E2SC},
  title =	 {{Neural Network-Based Task Scheduling with
                  Preemptive Fan Control}},
  year =	 2016,
  pages =	 {77-84},
  doi =		 {10.1109/E2SC.2016.016}
}

@inproceedings{ZhangOgrenci2015-minThermalVar,
  author =	 {Zhang, Kaicheng and Ogrenci-Memik, Seda and Memik,
                  Gokhan and Yoshii, Kazutomo and Sankaran, Rajesh and
                  Beckman, Pete},
  booktitle =	 {{IEEE International Parallel and Distributed
                  Processing Symposium}},
  series =	 {IPDPS},
  title =	 {{Minimizing Thermal Variation Across System
                  Components}},
  year =	 2015,
  pages =	 {1139-1148},
  doi =		 {10.1109/IPDPS.2015.37}
}

@inproceedings{SkinnerKramer2005-perfVarCauses,
  author =	 {Skinner, D. and Kramer, W.},
  booktitle =	 {{Proceedings of the IEEE Workload Characterization
                  Symposium}},
  series =	 {IISWC},
  title =	 {{Understanding the causes of performance variability
                  in HPC workloads}},
  year =	 2005,
  pages =	 {137-149},
  doi =		 {10.1109/IISWC.2005.1526010}
}

@inproceedings{AvalosKhairy2021-pka,
  author =	 {Avalos Baddouh, Cesar and Khairy, Mahmoud and Green,
                  Roland N. and Payer, Mathias and Rogers, Timothy G.},
  title =	 {{Principal Kernel Analysis: A Tractable Methodology
                  to Simulate Scaled GPU Workloads}},
  year =	 2021,
  isbn =	 9781450385572,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/3466752.3480100},
  doi =		 {10.1145/3466752.3480100},
  abstract =	 {Simulating all threads in a scaled GPU workload
                  results in prohibitive simulation cost. Cycle-level
                  simulation is orders of magnitude slower than native
                  silicon, the only solution is to reduce the amount
                  of work simulated while accurately representing the
                  program. Existing solutions to simulate GPU programs
                  either scale the input size, simulate the first
                  several billion instructions, or simulate a portion
                  of both the GPU and the workload. These solutions
                  lack validation against scaled systems, produce
                  unrealistic contention conditions and frequently
                  miss critical code sections. Existing CPU sampling
                  mechanisms, like SimPoint, reduce per-thread
                  workload, and are ill-suited to GPU programs where
                  reducing the number of threads is critical. Sampling
                  solutions on GPUs space lack silicon validation,
                  require per-workload parameter tuning, and do not
                  scale. A tractable solution, validated on
                  contemporary scaled workloads, is needed to provide
                  credible simulation results. By studying scaled
                  workloads with centuries-long simulation times, we
                  uncover practical and algorithmic limitations of
                  existing solutions and propose Principal Kernel
                  Analysis: a hierarchical program sampling
                  methodology that concisely represents GPU programs
                  by selecting representative kernel portions using a
                  scalable profiling methodology, tractable clustering
                  algorithm and detection of intra-kernel IPC
                  stability. We validate Principal Kernel Analysis
                  across 147 workloads and three GPU generations using
                  the Accel-Sim simulator, demonstrating a better
                  performance/error tradeoff than prior work and that
                  century-long MLPerf simulations are reduced to hours
                  with an average cycle error of 27\% versus silicon.},
  booktitle =	 {{54th Annual IEEE/ACM International Symposium on
                  Microarchitecture}},
  pages =	 {724–737},
  numpages =	 14,
  keywords =	 {Simulation methodology, GPU, Workload sampling},
  location =	 {Virtual Event, Greece},
  series =	 {MICRO '21}
}

@INPROCEEDINGS{WunderlichWenisch2003-smarts,
  author =	 {Wunderlich, R.E. and Wenisch, T.F. and Falsafi,
                  B. and Hoe, J.C.},
  booktitle =	 {{Proceedings of 30th Annual International Symposium
                  on Computer Architecture}},
  title =	 {{SMARTS: Accelerating Microarchitecture Simulation
                  via Rigorous Statistical Sampling}},
  year =	 2003,
  pages =	 {84-95},
  doi =		 {10.1109/ISCA.2003.1206991}
}

@misc{mlcommons-power,
  title =	 {{Best Practices Working Group: Power Working Group}},
  author =	 {{MLCommons}},
  howpublished =
                  {\url{https://mlcommons.org/en/groups/best-practices-power/}},
  year =	 2022,
}

@inproceedings{philly_atc19,
  author =	 {Jeon, Myeongjae and Venkataraman, Shivaram and
                  Phanishayee, Amar and Qian, unjie and Xiao, Wencong
                  and Yang, Fan},
  title =	 {Analysis of Large-Scale Multi-Tenant GPU Clusters
                  for DNN Training Workloads},
  year =	 2019,
  isbn =	 9781939133038,
  publisher =	 {USENIX Association},
  address =	 {USA},
  abstract =	 {With widespread advances in machine learning, a
                  number of large enterprises are beginning to
                  incorporate machine learning models across a number
                  of products. These models are typically trained on
                  shared, multi-tenant GPU clusters. Similar to
                  existing cluster computing workloads, scheduling
                  frameworks aim to provide features like high
                  efficiency, resource isolation, fair sharing across
                  users, etc. However Deep Neural Network (DNN) based
                  workloads, predominantly trained on GPUs, differ in
                  two significant ways from traditional big data
                  analytics workloads. First, from a cluster
                  utilization perspective, GPUs represent a monolithic
                  resource that cannot be shared at a fine granularity
                  across users. Second, from a workload perspective,
                  deep learning frameworks require gang scheduling
                  reducing the flexibility of scheduling and making
                  the jobs themselves inelastic to failures at
                  runtime. In this paper we present a detailed
                  workload characterization of a two-month long trace
                  from a multi-tenant GPU cluster in Microsoft. By
                  correlating scheduler logs with logs from individual
                  jobs, we study three distinct issues that affect
                  cluster utilization for DNN training workloads on
                  multi-tenant clusters: (1) the effect of gang
                  scheduling and locality constraints on queuing, (2)
                  the effect of locality on GPU utilization, and (3)
                  failures during training. Based on our experience
                  running a large-scale operation, we provide design
                  guidelines pertaining to next-generation cluster
                  schedulers for DNN training workloads.},
  booktitle =	 {Proceedings of the 2019 USENIX Conference on Usenix
                  Annual Technical Conference},
  pages =	 {947–960},
  numpages =	 14,
  location =	 {Renton, WA, USA},
  series =	 {USENIX ATC '19}
}

@INPROCEEDINGS{CheBeckmann2013,
  author =	 {Che, Shuai and Beckmann, Bradford M. and Reinhardt,
                  Stephen K. and Skadron, Kevin},
  booktitle =	 {{IEEE International Symposium on Workload
                  Characterization}},
  series =	 {{IISWC}},
  title =	 {{Pannotia: Understanding Irregular GPGPU Graph
                  Applications}},
  year =	 2013,
  pages =	 {185-195},
  keywords =	 {data mining;data structures;graph theory;graphics
                  processing units;parallel processing;pattern
                  clustering;scheduling;Pannotia;irregular GPGPU graph
                  applications;general-purpose data-parallel
                  applications;GPU-friendly applications;data
                  structures;access patterns;commercial
                  domains;scientific domains;graph mining;Web
                  analysis;social network analysis;graph
                  algorithms;SIMD architectures;data-dependent
                  behavior;branch and memory
                  divergence;OpenCL;clustering
                  analysis;Kernel;Labeling;Radiation detectors},
  doi =		 {10.1109/IISWC.2013.6704684},
  month =	 {Sept},
}

@article{mccalpin1995memory,
  title =	 {Memory bandwidth and machine balance in current high
                  performance computers},
  author =	 {McCalpin, John D and others},
  journal =	 {IEEE computer society technical committee on
                  computer architecture (TCCA) newsletter},
  volume =	 2,
  number =	 {19-25},
  year =	 1995
}

@article{WangPan2017-gunrock,
  author =	 {Wang, Yangzihao and Pan, Yuechao and Davidson,
                  Andrew and Wu, Yuduo and Yang, Carl and Wang, Leyuan
                  and Osama, Muhammad and Yuan, Chenshan and Liu,
                  Weitang and Riffel, Andy T. and Owens, John D.},
  title =	 {{Gunrock: GPU Graph Analytics}},
  year =	 2017,
  issue_date =	 {March 2017},
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  volume =	 4,
  number =	 1,
  issn =	 {2329-4949},
  url =		 {https://doi.org/10.1145/3108140},
  doi =		 {10.1145/3108140},
  abstract =	 {For large-scale graph analytics on the GPU, the
                  irregularity of data access and control flow, and
                  the complexity of programming GPUs, have presented
                  two significant challenges to developing a
                  programmable high-performance graph
                  library. “Gunrock,” our graph-processing system
                  designed specifically for the GPU, uses a
                  high-level, bulk-synchronous, data-centric
                  abstraction focused on operations on a vertex or
                  edge frontier. Gunrock achieves a balance between
                  performance and expressiveness by coupling
                  high-performance GPU computing primitives and
                  optimization strategies with a high-level
                  programming model that allows programmers to quickly
                  develop new graph primitives with small code size
                  and minimal GPU programming knowledge. We
                  characterize the performance of various optimization
                  strategies and evaluate Gunrock’s overall
                  performance on different GPU architectures on a wide
                  range of graph primitives that span from
                  traversal-based algorithms and ranking algorithms,
                  to triangle counting and bipartite-graph-based
                  algorithms. The results show that on a single GPU,
                  Gunrock has on average at least an order of
                  magnitude speedup over Boost and PowerGraph,
                  comparable performance to the fastest GPU hardwired
                  primitives and CPU shared-memory graph libraries,
                  such as Ligra and Galois, and better performance
                  than any other GPU high-level graph library.},
  journal =	 {ACM Trans. Parallel Comput.},
  month =	 {Aug},
  articleno =	 3,
  numpages =	 49,
  keywords =	 {GPU, Graph processing, runtime framework}
}

@techreport{LULESH:spec,
  title =	 {{H}ydrodynamics {C}hallenge {P}roblem, {L}awrence
                  {L}ivermore {N}ational {L}aboratory},
  number =	 {LLNL-TR-490254},
  location =	 {Livermore, CA},
  institution =	 {Lawrence Livermore National Laboratory},
  pages =	 {1-28},
  author =	 {Hornung, R. D. and Keasler, Jeff A. and Gokhale,
                  M. B.},
  year =	 2011,
}

@techreport{LULESH2:changes,
  author =	 {Karlin, Ian and Keasler, Jeff and Neely, Rob},
  title =	 {{LULESH 2.0 Updates and Changes}},
  institution =	 {Lawrence Livermore National Laboratory},
  number =	 {LLNL-TR-641973},
  location =	 {Livermore, CA},
  pages =	 {1-9},
  year =	 2013,
  month =	 {August}
}

@techreport{LULESH:prog,
  author =	 {Ian Karlin},
  title =	 {{LULESH Programming Model and Performance Ports
                  Overview}},
  institution =	 {Lawrence Livermore National Laboratory},
  number =	 {LLNL-TR-608824},
  location =	 {Livermore, CA},
  pages =	 {1-17},
  year =	 2012,
  month =	 {December}
}

@INPROCEEDINGS{IPDPS13:LULESH,
  AUTHOR =	 "Ian Karlin and Abhinav Bhatele and Jeff Keasler and
                  Bradford L. Chamberlain and Jonathan Cohen and
                  Zachary DeVito and Riyaz Haque and Dan Laney and
                  Edward Luke and Felix Wang and David Richards and
                  Martin Schulz and Charles Still",
  TITLE =	 "Exploring Traditional and Emerging Parallel
                  Programming Models using a Proxy Application",
  BOOKTITLE =	 "27th IEEE International Parallel \& Distributed
                  Processing Symposium",
  series =	 {IPDPS},
  ADDRESS =	 "Boston, USA",
  MONTH =	 may,
  YEAR =	 2013
}

@inproceedings{JainTran2024-pal,
  author =	 {Jain, Rutwik and Tran, Brandon and Chen, Keting and
                  Sinclair, Matthew D. and Venkataraman, Shivaram},
  title =	 {{PAL: A Variability-Aware Policy for Scheduling ML
                  Workloads in GPU Clusters}},
  booktitle =	 {{Proceedings of the International Conference for
                  High Performance Computing, Networking, Storage, and
                  Analysis}},
  series =	 {SC},
  year =	 2024,
  month =	 {November},
}

@inproceedings{YouXuan2024-gvarp,
  author =	 {You, Xin and Xuan, Zhibo and Yang, Hailong and Luan,
                  Zhongzhi and Liu, Yi and Qian, Depei},
  title =	 {{GVARP: Detecting Performance Variance on
                  Large-Scale Heterogeneous System}},
  booktitle =	 {{Proceedings of the International Conference for
                  High Performance Computing, Networking, Storage, and
                  Analysis}},
  series =	 {SC},
  year =	 2024,
}

@inproceedings{KandiahPeverelle2021-accelWattch,
  author =	 {Kandiah, Vijay and Peverelle, Scott and Khairy,
                  Mahmoud and Manjunath, Amogh and Pan, Junrui and
                  Rogers, Timothy G. and Aamodt, Tor M. and
                  Hardavellas, Nikos},
  title =	 {{AccelWattch: A Power Modeling Framework for Modern
                  GPUs}},
  booktitle =	 {{Proceedings of the 54th IEEE/ACM International
                  Symposium on Microarchitecture}},
  series =	 {MICRO},
  year =	 2021,
  month =	 {October}
}

@inproceedings{JamiesonChandrashekar2022-gap,
  title =	 {{GAP: gem5 GPU Accuracy Profiler}},
  author =	 {Jamieson, Charles and Chandrashekar, Anushka and
                  McDougall, Ian and M. D. Sinclair},
  year =	 2022,
  month =	 {June},
  booktitle =	 {{4th gem5 Users' Workshop}},
}

@inproceedings{RamadasKouchekinia2023-gap,
  title =	 {{Closing the Gap: Improving the Accuracy of gem5’s
                  GPU Models}},
  author =	 {Ramadas, Vishnu and Kouchekinia, Daniel and Osuji,
                  Ndubuisi and Sinclair, Matthew D.},
  year =	 2023,
  month =	 {June},
  booktitle =	 {{5th gem5 Users' Workshop}},
}

@inproceedings{RamadasKouchekinia2024-gap,
  title =	 {{Further Closing the GAP: Improving the Accuracy of
                  gem5’s GPU Models}},
  author =	 {Ramadas, Vishnu and Kouchekinia, Daniel and
                  Sinclair, Matthew D.},
  year =	 2024,
  month =	 {April},
  booktitle =	 {{6th Young Architects' Workshop}},
  series =	 {YArch},
}

@inproceedings{SunBaruah2019-mgpusim,
  author =	 {Sun, Yifan and Baruah, Trinayan and Mojumder, Saiful
                  A. and Dong, Shi and Gong, Xiang and Treadway, Shane
                  and Bao, Yuhui and Hance, Spencer and McCardwell,
                  Carter and Zhao, Vincent and Barclay, Harrison and
                  Ziabari, Amir Kavyan and Chen, Zhongliang and Ubal,
                  Rafael and Abell\'{a}n, Jos\'{e} L. and Kim, John
                  and Joshi, Ajay and Kaeli, David},
  title =	 {{MGPUSim: Enabling Multi-GPU Performance Modeling
                  and Optimization}},
  year =	 2019,
  isbn =	 9781450366694,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/3307650.3322230},
  doi =		 {10.1145/3307650.3322230},
  abstract =	 {The rapidly growing popularity and scale of
                  data-parallel workloads demand a corresponding
                  increase in raw computational power of Graphics
                  Processing Units (GPUs). As single-GPU platforms
                  struggle to satisfy these performance demands,
                  multi-GPU platforms have started to dominate the
                  high-performance computing world. The advent of such
                  systems raises a number of design challenges,
                  including the GPU microarchitecture, multi-GPU
                  interconnect fabric, runtime libraries, and
                  associated programming models. The research
                  community currently lacks a publicly available and
                  comprehensive multi-GPU simulation framework to
                  evaluate next-generation multi-GPU system designs.In
                  this work, we present MGPUSim, a cycle-accurate,
                  extensively validated, multi-GPU simulator, based on
                  AMD's Graphics Core Next 3 (GCN3) instruction set
                  architecture. MGPUSim comes with in-built support
                  for multi-threaded execution to enable fast,
                  parallelized, and accurate simulation.  In terms of
                  performance accuracy, MGPUSim differs by only 5.5%
                  on average from the actual GPU hardware. We also
                  achieve a 3.5\texttimes{} and a 2.5\texttimes{}
                  average speedup running functional emulation and
                  detailed timing simulation, respectively, on a
                  4-core CPU, while delivering the same accuracy as
                  serial simulation.We illustrate the flexibility and
                  capability of the simulator through two concrete
                  design studies. In the first, we propose the
                  Locality API, an API extension that allows the GPU
                  programmer to both avoid the complexity of multi-GPU
                  programming, while precisely controlling data
                  placement in the multi-GPU memory. In the second
                  design study, we propose Progressive Page Splitting
                  Migration (PASI), a customized multi-GPU memory
                  management system enabling the hardware to
                  progressively improve data placement. For a discrete
                  4-GPU system, we observe that the Locality API can
                  speed up the system by 1.6\texttimes{} (geometric
                  mean), and PASI can improve the system performance
                  by 2.6\texttimes{} (geometric mean) across all
                  benchmarks, compared to a unified 4-GPU platform.},
  booktitle =	 {{Proceedings of the 46th International Symposium on
                  Computer Architecture}},
  pages =	 {197–209},
  numpages =	 13,
  keywords =	 {memory management, multi-GPU systems, simulation},
  location =	 {Phoenix, Arizona},
  series =	 {ISCA '19}
}

@inproceedings{GPUWattch,
  author =	 {Leng, Jingwen and Hetherington, Tayler and
                  ElTantawy, Ahmed and Gilani, Syed and Kim, Nam Sung
                  and Aamodt, Tor M. and Reddi, Vijay Janapa},
  title =	 {{GPUWattch: Enabling Energy Optimizations in
                  GPGPUs}},
  booktitle =	 {{Proceedings of the 40th International Symposium on
                  Computer Architecture}},
  year =	 2013,
}

@inproceedings{hopper,
  author =	 {NVIDIA},
  title =	 {{NVIDIA H100 Tensor Core GPU Architecture}},
  booktitle =	 "{Proceedings of GPU Technology Conference}",
  series =	 {{GTC}},
  year =	 2022,
  howpublished =
                  {\url{https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper}},
}

@misc{blackwell,
  title =	 {{NVIDIA Blackwell Architecture Technical Brief}},
  author =	 {{NVIDIA}},
  howpublished =
                  {\url{https://resources.nvidia.com/en-us-blackwell-architecture/blackwell-architecture-technical-brief}},
  year =	 2024,
}

@INPROCEEDINGS{RaghavanLuo2012-compSprint,
  author =	 {Raghavan, Arun and Luo, Yixin and Chandawalla, Anuj
                  and Papaefthymiou, Marios and Pipe, Kevin P. and
                  Wenisch, Thomas F. and Martin, Milo M. K.},
  booktitle =	 {IEEE International Symposium on High-Performance
                  Comp Architecture},
  series =	 {HPCA},
  title =	 {{Computational Sprinting}},
  year =	 2012,
  pages =	 {1-12},
  keywords =	 {Heating;Phase change
                  materials;Capacitance;Silicon;Thermal
                  conductivity;Mobile communication},
  doi =		 {10.1109/HPCA.2012.6169031}
}

@inproceedings{FanZahedi2016-compSprintGame,
  author =	 {Fan, Songchun and Zahedi, Seyed Majid and Lee,
                  Benjamin C.},
  title =	 {{The Computational Sprinting Game}},
  year =	 2016,
  isbn =	 9781450340915,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/2872362.2872383},
  doi =		 {10.1145/2872362.2872383},
  abstract =	 {Computational sprinting is a class of mechanisms
                  that boost performance but dissipate additional
                  power. We describe a sprinting architecture in which
                  many, independent chip multiprocessors share a power
                  supply and sprints are constrained by the chips'
                  thermal limits and the rack's power
                  limits. Moreover, we present the computational
                  sprinting game, a multi-agent perspective on
                  managing sprints. Strategic agents decide whether to
                  sprint based on application phases and system
                  conditions. The game produces an equilibrium that
                  improves task throughput for data analytics
                  workloads by 4-6\texttimes{} over prior greedy
                  heuristics and performs within 90\% of an upper
                  bound on throughput from a globally optimized
                  policy.},
  booktitle =	 {{Proceedings of the Twenty-First International
                  Conference on Architectural Support for Programming
                  Languages and Operating Systems}},
  pages =	 {561–575},
  numpages =	 15,
  keywords =	 {computational sprinting, energy efficiency, game
                  theory, performance},
  location =	 {Atlanta, Georgia, USA},
  series =	 {ASPLOS '16}
}

@inproceedings{MorrisSaravanan2018-compSprintSLO,
  author =	 {Morris, Nathaniel and Saravanan, Indrajeet and Cao,
                  Pollyanna and Ding, Jerry and Stewart, Christopher},
  title =	 {{SLO Computational Sprinting}},
  year =	 2018,
  isbn =	 9781450360111,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/3267809.3275452},
  doi =		 {10.1145/3267809.3275452},
  booktitle =	 {{Proceedings of the ACM Symposium on Cloud
                  Computing}},
  pages =	 510,
  numpages =	 1,
  keywords =	 {Resource Management, Queuing Models, Computational
                  Sprinting},
  location =	 {Carlsbad, CA, USA},
  series =	 {SoCC '18}
}

@INPROCEEDINGS{MorrisRenganathan2016-compSprintSW,
  author =	 {Morris, Nathaniel and Renganathan, Siva Meenakshi
                  and Stewart, Christopher and Birke, Robert and Chen,
                  Lydia},
  booktitle =	 {{IEEE International Conference on Autonomic
                  Computing}},
  series =	 {ICAC},
  title =	 {{Sprint Ability: How Well Does Your Software Exploit
                  Bursts in Processing Capacity?}},
  year =	 2016,
  pages =	 {173-178},
  keywords =	 {Time factors;Software;Computational
                  modeling;Delays;Resource management;Adaptation
                  models;sprinting;sprintability;queuing;dvfs;burst},
  doi =		 {10.1109/ICAC.2016.61}
}

@INPROCEEDINGS{YangAdamek2024-gpuPowerMeas,
  author =	 {Yang, Zeyu and Adamek, Karel and Armour, Wesley },
  booktitle =	 {{International Conference for High Performance
                  Computing, Networking, Storage and Analysis SC}},
  title =	 {{Accurate and Convenient Energy Measurements for
                  GPUs: A Detailed Study of NVIDIA GPU’s Built-In
                  Power Sensor}},
  year =	 2024,
  pages =	 {307-323},
  abstract =	 {GPU has emerged as the go-to accelerator for HPC
                  workloads, however its power consumption has become
                  a major limiting factor for further scaling HPC
                  systems. An accurate understanding of GPU power
                  consumption is essential for further improving its
                  energy efficiency, and consequently reducing the
                  associated carbon footprint. Despite the limited
                  documentation and lack of understanding, NVIDIA
                  GPUs’ built-in power sensor is widely used in
                  energy-efficient computing research. Our study seeks
                  to elucidate the internal mechanisms of the power
                  readings provided by nvidia-smi and assess the
                  accuracy of the measurements. We evaluated over 70
                  different GPUs across 12 architectural generations,
                  and identified several unforeseen problems that can
                  lead to drastic under/overestimation of energy
                  consumed, for example on the A100 and H100 GPUs only
                  25% of the runtime is sampled. We proposed several
                  mitigations that could reduce the energy measurement
                  error by an average of 35% in the test cases we
                  present.},
  keywords =	 {High performance computing;Green computing;Energy
                  consumption;Energy measurement;Power measurement},
  doi =		 {10.1109/SC41406.2024.00028},
  url =
                  {https://doi.ieeecomputersociety.org/10.1109/SC41406.2024.00028},
  publisher =	 {IEEE Computer Society},
  address =	 {Los Alamitos, CA, USA},
  month =	 {Nov},
}

@inproceedings{Stelmach2023-sysPower,
  author =	 {Stelmach, Shane},
  title =	 {{System Power Integrity Analysis: from the PMIC to
                  the Transistor}},
  booktitle =	 {{Proceedings of the 61st Design Automation
                  Conference}},
  series =	 {DAC},
  year =	 2023
}

@INPROCEEDINGS{FelixMorton2023-waferCapacitors,
  author =	 {Felix, Stephen and Morton, Shannon and Stacey, Simon
                  and Walsh, John},
  booktitle =	 {{IEEE International Solid-State Circuits
                  Conference}},
  series =	 {ISSCC},
  title =	 {{29.4 Wafer-Level Stacking of High-Density
                  Capacitors to Enhance the Performance of a Large
                  Multicore Processor for Machine Learning
                  Applications}},
  year =	 2023,
  pages =	 {424-426},
  keywords =	 {Multicore processing;Capacitors;Stacking;Random
                  access memory;Voltage;Machine learning;Packaging},
  doi =		 {10.1109/ISSCC42615.2023.10067282}
}

@INPROCEEDINGS{NithinShanmugam2010-voltageDrop,
  author =	 {Nithin, S K and Shanmugam, Gowrysankar and
                  Chandrasekar, Sreeram},
  booktitle =	 {{11th International Symposium on Quality Electronic
                  Design}},
  series =	 {ISQED},
  title =	 {{Dynamic Voltage (IR) Drop Analysis and Design
                  Closure: Issues and Challenges}},
  year =	 2010,
  pages =	 {611-617},
  keywords =	 {Voltage;Robustness;Switches;Power distribution;Power
                  grids;Power system dynamics;Switching
                  circuits;Design methodology;Energy
                  management;Timing;Dynamic voltage Drop;DvD;Dynamic
                  IR;Peak power;Power switch;VCD;Power gate;SDF},
  doi =		 {10.1109/ISQED.2010.5450515}
}

@inproceedings{Austin2006-razor,
  author =	 {Austin, Todd},
  title =	 {{Razor: A Low-power Pipeline Based on Circuit-level
                  Timing Speculation}},
  year =	 2006,
  isbn =	 1595934790,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/1150343.1150348},
  doi =		 {10.1145/1150343.1150348},
  abstract =	 {With increasing clock frequencies and silicon
                  integration, power aware computing has become a
                  critical concern in the design of embedded
                  processors and systems-on-chip. One of the more
                  effective and widely used methods for power-aware
                  computing is dynamic voltage scaling (DVS). In order
                  to obtain the maximum power savings from DVS, it is
                  essential to scale the supply voltage as low as
                  possible while ensuring correct operation of the
                  processor. The critical voltage is chosen such that
                  under a worst-case scenario of process and
                  environmental variations, the processor always
                  operates correctly. However, this approach leads to
                  a very conservative supply voltage since such a
                  worst-case combination of different variabilities
                  will be very rare.In this talk, I detail a novel
                  approach to DVS, called Razor, based on dynamic
                  detection and correction of circuit timing
                  errors. The key idea of Razor is to tune the supply
                  voltage by monitoring the error rate during circuit
                  operation, thereby eliminating the need for voltage
                  margins and exploiting the data dependence of
                  circuit delay. A Razor flip-flop is introduced that
                  double-samples pipeline stage values, once with a
                  fast clock and again with a time-borrowing delayed
                  clock. A metastability tolerant comparator then
                  validates latch values sampled with the fast
                  clock. In the event of a timing error, a modified
                  pipeline mispeculation recovery mechanism restores
                  correct program state. A prototype Razor processor
                  will be described, along with physical
                  measurements.},
  booktitle =	 {{Proceedings of the 19th Annual Symposium on
                  Integrated Circuits and Systems Design}},
  pages =	 13,
  numpages =	 1,
  location =	 {Ouro Preto, MG, Brazil},
  series =	 {SBCCI '06}
}

@inproceedings{godoy2023software,
  title =	 {{Software Engineering to Sustain a High-performance
                  Computing Scientific Application: QMCPACK}},
  author =	 {Godoy, William F. and Hahn, Steven E. and Walsh,
                  Michael M. and Fackler, Philip W. and Krogel, Jaron
                  T. and Doak, Peter W. and Paul R. C. Kent and
                  Alfredo A. Correa and Ye Luo and Mark Dewing},
  year =	 2023,
  doi =		 {10.5281/zenodo.10420939},
  booktitle =	 {Proceedings of the 1st Annual Conference of the US
                  Research Software Engineering Association,
                  USRSE2023}
}

@article{kim2018qmcpack,
  title =	 {{QMCPACK: An Open Source ab initio Quantum Monte
                  Carlo Package for the Electronic Structure of Atoms,
                  Molecules and Solids}},
  author =	 {Kim, Jeongnim and Baczewski, Andrew D and Beaudet,
                  Todd D and Benali, Anouar and Bennett, M Chandler
                  and Berrill, Mark A and Blunt, Nick S and Borda,
                  Edgar Josu{\'e} Landinez and Casula, Michele and
                  Ceperley, David M and others},
  journal =	 {Journal of Physics: Condensed Matter},
  volume =	 30,
  number =	 19,
  pages =	 195901,
  year =	 2018,
  publisher =	 {IOP Publishing}
}

@article{WangZhang2018-deepmd,
  title =	 {{DeePMD-kit: A deep learning package for many-body
                  potential energy representation and molecular
                  dynamics}},
  journal =	 {{Computer Physics Communications}},
  volume =	 228,
  pages =	 {178-184},
  year =	 2018,
  issn =	 {0010-4655},
  doi =		 {https://doi.org/10.1016/j.cpc.2018.03.016},
  url =
                  {https://www.sciencedirect.com/science/article/pii/S0010465518300882},
  author =	 {Wang, Han and Zhang, Linfeng and Han, Jiequn and E,
                  Weinan},
  keywords =	 {Many-body potential energy, Molecular dynamics, Deep
                  neural networks},
  abstract =	 {Recent developments in many-body potential energy
                  representation via deep learning have brought new
                  hopes to addressing the accuracy-versus-efficiency
                  dilemma in molecular simulations. Here we describe
                  DeePMD-kit, a package written in Python/C++ that has
                  been designed to minimize the effort required to
                  build deep learning based representation of
                  potential energy and force field and to perform
                  molecular dynamics. Potential applications of
                  DeePMD-kit span from finite molecules to extended
                  systems and from metallic systems to chemically
                  bonded systems. DeePMD-kit is interfaced with
                  TensorFlow, one of the most popular deep learning
                  frameworks, making the training process highly
                  automatic and efficient. On the other end,
                  DeePMD-kit is interfaced with high-performance
                  classical molecular dynamics and quantum
                  (path-integral) molecular dynamics packages, i.e.,
                  LAMMPS and the i-PI, respectively. Thus, upon
                  training, the potential energy and force field
                  models can be used to perform efficient molecular
                  simulations for different purposes. As an example of
                  the many potential applications of the package, we
                  use DeePMD-kit to learn the interatomic potential
                  energy and forces of a water model using data
                  obtained from density functional theory. We
                  demonstrate that the resulted molecular dynamics
                  model reproduces accurately the structural
                  information contained in the original model.
                  Program summary Program Title: DeePMD-kit Program
                  Files doi: http://dx.doi.org/10.17632/hvfh9yvncf.1
                  Licensing provisions: LGPL Programming language:
                  Python/C++ Nature of problem: Modeling the many-body
                  atomic interactions by deep neural network
                  models. Running molecular dynamics simulations with
                  the models. Solution method: The Deep Potential for
                  Molecular Dynamics (DeePMD) method is implemented
                  based on the deep learning framework
                  TensorFlow. Supports for using a DeePMD model in
                  LAMMPS and i-PI, for classical and quantum (path
                  integral) molecular dynamics are
                  provided. Additional comments including Restrictions
                  and Unusual features: The code defines a data
                  protocol such that the energy, force, and virial
                  calculated by different third-party molecular
                  simulation packages can be easily processed and used
                  as model training data.}
}

@article{ZengZhang2023-deepmd2,
  author =	 {Zeng, Jinzhe and Zhang, Duo and Lu, Denghui and Mo,
                  Pinghui and Li, Zeyu and Chen, Yixiao and Rynik,
                  Marián and Huang, Li’ang and Li, Ziyao and Shi,
                  Shaochen and Wang, Yingze and Ye, Haotian and Tuo,
                  Ping and Yang, Jiabin and Ding, Ye and Li, Yifan and
                  Tisi, Davide and Zeng, Qiyu and Bao, Han and Xia, Yu
                  and Huang, Jiameng and Muraoka, Koki and Wang, Yibo
                  and Chang, Junhan and Yuan, Fengbo and Bore,
                  Sigbjørn Løland and Cai, Chun and Lin, Yinnian and
                  Wang, Bo and Xu, Jiayan and Zhu, Jia-Xin and Luo,
                  Chenxing and Zhang, Yuzhi and Goodall, Rhys
                  E. A. and Liang, Wenshuo and Singh, Anurag Kumar and
                  Yao, Sikai and Zhang, Jingchao and Wentzcovitch,
                  Renata and Han, Jiequn and Liu, Jie and Jia, Weile
                  and York, Darrin M. and E, Weinan and Car, Roberto
                  and Zhang, Linfeng and Wang, Han},
  title =	 "{DeePMD-kit v2: A software package for deep
                  potential models}",
  journal =	 {The Journal of Chemical Physics},
  volume =	 159,
  number =	 5,
  pages =	 054801,
  year =	 2023,
  month =	 08,
  abstract =	 "{DeePMD-kit is a powerful open-source software
                  package that facilitates molecular dynamics
                  simulations using machine learning potentials known
                  as Deep Potential (DP) models. This package, which
                  was released in 2017, has been widely used in the
                  fields of physics, chemistry, biology, and material
                  science for studying atomistic systems. The current
                  version of DeePMD-kit offers numerous advanced
                  features, such as DeepPot-SE, attention-based and
                  hybrid descriptors, the ability to fit tensile
                  properties, type embedding, model deviation,
                  DP-range correction, DP long range, graphics
                  processing unit support for customized operators,
                  model compression, non-von Neumann molecular
                  dynamics, and improved usability, including
                  documentation, compiled binary packages, graphical
                  user interfaces, and application programming
                  interfaces. This article presents an overview of the
                  current major version of the DeePMD-kit package,
                  highlighting its features and technical
                  details. Additionally, this article presents a
                  comprehensive procedure for conducting molecular
                  dynamics as a representative application, benchmarks
                  the accuracy and efficiency of different models, and
                  discusses ongoing developments.}",
  issn =	 {0021-9606},
  doi =		 {10.1063/5.0155600},
  url =		 {https://doi.org/10.1063/5.0155600},
  eprint =
                  {https://pubs.aip.org/aip/jcp/article-pdf/doi/10.1063/5.0155600/18281511/054801\_1\_5.0155600.pdf},
}

@misc{openfold2,
  title =	 {{OpenFold2: Replicating AlphaFold2 in the Dark}},
  author =	 {Derevyanko, Georgy and Lamoureux, Guillame and
                  Outeiral, Carlos and Oda, Toshiyuki and Fuchs,
                  Fabian and Mahajan, Sai Pooja and Moult, John and
                  Haas, Juergen and Maragakis, Paul and Ruzmetov,
                  Talant and AlQuraishi, Mohammed},
  year =	 2023,
  howpublished = {\url{https://lupoglaz.github.io/OpenFold2/}},
}

@article{Stevens2023-auroraGPT,
  title =	 {{Argonne's "AuroraGPT" Project}},
  author =	 {Stevens, Rick},
  year =	 2023,
  journal =	 {{Trillion Parameter Consortium Seminar}},
  series =	 {TPC},
}

@inproceedings{WuTaylor2019-candle,
  author =	 {Wu, Xingfu and Taylor, Valerie and Wozniak, Justin
                  M. and Stevens, Rick and Brettin, Thomas and Xia,
                  Fangfang},
  title =	 {{Performance, Energy, and Scalability Analysis and
                  Improvement of Parallel Cancer Deep Learning CANDLE
                  Benchmarks}},
  year =	 2019,
  isbn =	 9781450362955,
  publisher =	 {Association for Computing Machinery},
  address =	 {New York, NY, USA},
  url =		 {https://doi.org/10.1145/3337821.3337905},
  doi =		 {10.1145/3337821.3337905},
  abstract =	 {Training scientific deep learning models requires
                  the significant compute power of high-performance
                  computing systems. In this paper, we analyze the
                  performance characteristics of the benchmarks from
                  the exploratory research project CANDLE (Cancer
                  Distributed Learning Environment) with a focus on
                  the hyperparameters epochs, batch sizes, and
                  learning rates. We present the parallel methodology
                  that uses the distributed deep learning framework
                  Horovod to parallelize the CANDLE benchmarks. We
                  then use scaling strategies for both epochs and
                  batch size with linear learning rate scaling to
                  investigate how they impact the execution time and
                  accuracy as well as the power, energy, and
                  scalability of the parallel CANDLE benchmarks under
                  conditions of strong scaling and weak scaling on the
                  IBM Power9 heterogeneous system Summit at Oak Ridge
                  National Laboratory and the Cray XC40 Theta at
                  Argonne National Laboratory. This study provides
                  insights into how to set the proper numbers of
                  epochs, batch sizes, and compute resources for these
                  benchmarks to preserve the high accuracy and to
                  reduce the execution time of the benchmarks. We
                  identify the data-loading performance bottleneck and
                  then improve the performance and energy for better
                  scalability. Results with the modified benchmarks on
                  Summit indicate up to 78.25\% in performance
                  improvement and up to 78\% in energy saving under
                  strong scaling on up to 384 GPUs, and up to 79.5\%
                  in performance improvement and up to 77.11\% in
                  energy saving under weak scaling on up to 3,072
                  GPUs. On Theta, we achieve up to 45.22\% performance
                  improvement and up to 41.78\% in energy saving under
                  strong scaling on up to 384 nodes. Moreover, the
                  modification dramatically reduces the broadcast
                  overhead.},
  booktitle =	 {{Proceedings of the 48th International Conference on
                  Parallel Processing}},
  articleno =	 78,
  numpages =	 11,
  keywords =	 {CANDLE benchmarks, Deep learning, Horovod,
                  TensorFlow, energy, performance improvement},
  location =	 {Kyoto, Japan},
  series =	 {ICPP},
}

@InProceedings{ThiyagalingamVonLaszewski2022-aiForSciMLCommons,
  author =	 "Thiyagalingam, Jeyan and von Laszewski, Gregor and
                  Yin, Junqi and Emani, Murali and Papay, Juri and
                  Barrett, Gregg and Luszczek, Piotr and Tsaris,
                  Aristeidis and Kirkpatrick, Christine and Wang,
                  Feiyi and Gibbs, Tom and Vishwanath, Venkatram and
                  Shankar, Mallikarjun and Fox, Geoffrey and Hey,
                  Tony",
  editor =	 "Anzt, Hartwig and Bienz, Amanda and Luszczek, Piotr
                  and Baboulin, Marc",
  title =	 "AI Benchmarking for Science: Efforts from the
                  MLCommons Science Working Group",
  booktitle =	 "High Performance Computing. ISC High Performance
                  2022 International Workshops",
  year =	 2022,
  publisher =	 "Springer International Publishing",
  address =	 "Cham",
  pages =	 "47--64",
  abstract =	 "With machine learning (ML) becoming a transformative
                  tool for science, the scientific community needs a
                  clear catalogue of ML techniques, and their relative
                  benefits on various scientific problems, if they
                  were to make significant advances in science using
                  AI. Although this comes under the purview of
                  benchmarking, conventional benchmarking initiatives
                  are focused on performance, and as such, science,
                  often becomes a secondary criteria.",
  isbn =	 "978-3-031-23220-6"
}

@article{ThiyagalingamShankar2022-mlSci,
  title =	 {{Scientific Machine Learning Benchmarks}},
  author =	 {Thiyagalingam, Jeyan and Shankar, Mallikarjun and
                  Fox, Geoffrey and Hey, Tony},
  journal =	 {Nature Reviews Physics},
  volume =	 4,
  number =	 6,
  pages =	 {413--420},
  year =	 2022,
  publisher =	 {Nature Publishing Group UK London}
}

@article{ColemanNarayanan2017-dawnbench,
  title =	 {{DAWNBench: An end-to-end Deep Learning Benchmark
                  and Competition}},
  author =	 {Coleman, Cody and Narayanan, Deepak and Kang, Daniel
                  and Zhao, Tian and Zhang, Jian and Nardi, Luigi and
                  Bailis, Peter and Olukotun, Kunle and R{\'e}, Chris
                  and Zaharia, Matei},
  journal =	 {Training},
  volume =	 100,
  number =	 101,
  pages =	 102,
  year =	 2017
}

@inproceedings{DeBardeleben-LBNL-EuroPar13,
  author =	 {Nathan DeBardeleben and Sean Blanchard and Laura
                  Monroe and Philip Romero and Daryl Grunau and Craig
                  Idler and Cornell Wright},
  editor =	 {Dieter an Mey and Michael Alexander and Paolo
                  Bientinesi and Mario Cannataro and Carsten Clauss
                  and Alexandru Costan and Gabor Kecskemeti and
                  Christine Morin and Laura Ricci and Julio Sahuquillo
                  and Martin Schulz and Vittorio Scarano and Stephen
                  L. Scott and Josef Weidendorfer},
  title =	 {{{GPU} Behavior on a Large {HPC} Cluster}},
  booktitle =	 {Euro-Par 2013: Parallel Processing Workshops -
                  BigDataCloud, DIHC, FedICI, HeteroPar, HiBB, LSDVE,
                  MHPC, OMHI, PADABS, PROPER, Resilience, ROME, and
                  {UCHPC} 2013, Aachen, Germany, August 26-27,
                  2013. Revised Selected Papers},
  series =	 {Lecture Notes in Computer Science},
  volume =	 8374,
  pages =	 {680--689},
  publisher =	 {Springer},
  year =	 2013,
  url =		 {https://doi.org/10.1007/978-3-642-54420-0_66},
  doi =		 {10.1007/978-3-642-54420-0\_66},
  timestamp =	 {Wed, 19 Feb 2020 14:52:57 +0100},
  biburl =
                  {https://dblp.org/rec/conf/europar/DeBardelebenBMRGIW13.bib},
  bibsource =	 {dblp computer science bibliography,
                  https://dblp.org}
}

@ARTICLE{Fraternali-EEHPCVar-2018,
  author =	 {Fraternali, Francesco and Bartolini, Andrea and
                  Cavazzoni, Carlo and Benini, Luca},
  journal =	 {IEEE Transactions on Parallel and Distributed
                  Systems},
  title =	 {{Quantifying the Impact of Variability and
                  Heterogeneity on the Energy Efficiency for a
                  Next-Generation Ultra-Green Supercomputer}},
  year =	 2018,
  volume =	 29,
  number =	 7,
  pages =	 {1575-1588},
  keywords =	 {Supercomputers;Frequency measurement;Computer
                  architecture;Power measurement;Hardware;Energy
                  measurement;Program
                  processors;Green500;high-performance
                  computing;hardware variability;energy-efficient
                  software design;energy-aware computing;green
                  supercomputer;heterogeneous supercomputer;dynamic
                  resource management;hardware accelerator;DVFS},
  doi =		 {10.1109/TPDS.2017.2766151}
}

@article{DBLP:journals/corr/abs-2102-06604,
  author =	 {Frank Schneider and Felix Dangel and Philipp Hennig},
  title =	 {Cockpit: {A} Practical Debugging Tool for Training
                  Deep Neural Networks},
  journal =	 {CoRR},
  volume =	 {abs/2102.06604},
  year =	 2021,
  url =		 {https://arxiv.org/abs/2102.06604},
  eprinttype =	 {arXiv},
  eprint =	 {2102.06604},
  timestamp =	 {Mon, 06 Mar 2023 15:08:32 +0100},
  biburl =
                  {https://dblp.org/rec/journals/corr/abs-2102-06604.bib},
  bibsource =	 {dblp computer science bibliography,
                  https://dblp.org}
}

@misc{zhao2024deepcontextcontextawarecrossplatformcrossframework,
  title =	 {DeepContext: A Context-aware, Cross-platform, and
                  Cross-framework Tool for Performance Profiling and
                  Analysis of Deep Learning Workloads},
  author =	 {Qidong Zhao and Hao Wu and Yuming Hao and Zilingfeng
                  Ye and Jiajia Li and Xu Liu and Keren Zhou},
  year =	 2024,
  eprint =	 {2411.02797},
  archivePrefix ={arXiv},
  primaryClass = {cs.PF},
  url =		 {https://arxiv.org/abs/2411.02797},
}

@article{binkert2011gem5,
  title={The gem5 simulator},
  author = {Binkert, Nathan and Beckmann, Bradford and Black, Gabriel and
  Reinhardt, Steven K. and Saidi, Ali and Basu, Arkaprava and Hestness, Joel and
  Hower, Derek R. and Krishna, Tushar and Sardashti, Somayeh and Sen, Rathijit
  and Sewell, Korey and Shoaib, Muhammad and Vaish, Nilay and Hill, Mark D. and
  Wood, David A.},
  journal={ACM SIGARCH Computer Architecture News},
  volume={39},
  number={2},
  pages={1--7},
  year={2011},
  publisher={ACM}
}

@inproceedings{RamadasPoremba2023-gem5GPUFS,
  title = {{Improving gem5’s GPU FS Support}},
  author = {Ramadas, Vishnu and Poremba, Matthew and Beckmann, Bradford M. and Sinclair, Matthew D.},
  booktitle = {{The 5th gem5 Users’ Workshop}},
  year = {2023},
  month = {6},
}

@inproceedings{RamadasPoremba2024-gem5MLSim,
  author = {Ramadas, Vishnu and Poremba, Matthew and Beckmann, Bradford M. and Sinclair, Matthew D.},
  title = {{Simulation Support for Fast and Accurate Large-Scale GPGPU and Accelerator Workloads}},
  year = {2024},
  booktitle = {{3rd Open-Source Computer Architecture Research Workshop}},
  series = {OSCAR},
}

@inproceedings{SmithBruce2024-gem5Power,
  author = {Smith, Alex and Bruce, Bobby and Lowe-Power, Jason and Sinclair, Matthew D.},
  title = {{Designing Generalizable Power Models For Open-Source Architecture Simulators}},
  year = {2024},
  booktitle = {{3rd Open-Source Computer Architecture Research Workshop}},
  series = {OSCAR},
}

@inproceedings{RamadasSinclair2024-gem5MLSim,
  author = {Ramadas, Vishnu and Sinclair, Matthew D.},
  title = {{Simulating Machine Learning Models at Scale}},
  year = {2024},
  booktitle = {{SRC TECHCON}},
  month = {9},
}

@article{RodriguesHemmert2011-sst,
  author = {Rodrigues, A. F. and Hemmert, K. S. and Barrett, B. W. and Kersey, C. and Oldfield, R. and Weston, M. and Risen, R. and Cook, J. and Rosenfeld, P. and Cooper-Balis, E. and Jacob, B.},
  title = {{The Structural Simulation Toolkit}},
  year = {2011},
  issue_date = {March 2011},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {38},
  number = {4},
  issn = {0163-5999},
  url = {https://doi.org/10.1145/1964218.1964225},
  doi = {10.1145/1964218.1964225},
  abstract = {As supercomputers grow, understanding their behavior and performance has become increasingly challenging. New hurdles in scalability, programmability, power consumption, reliability, cost, and cooling are emerging, along with new technologies such as 3D integration, GP-GPUs, silicon-photonics, and other "game changers". Currently, they HPC community lacks a unified toolset to evaluate these technologies and design for these challenges.To address this problem, a number of institutions have joined together to create the Structural Simulation Toolkit (SST), an open, modular, parallel, multi-criteria, multi-scale simulation framework. The SST includes a number of processor, memory, and network models. The SST has been used in a variety of network, memory, and application studies and aims to become the standard simulation framework for designing and procuring HPC systems.},
  journal = {SIGMETRICS Perform. Eval. Rev.},
  month = mar,
  pages = {37–42},
  numpages = {6},
  keywords = {simulation, performance analysis, architecture, SST}
}

@article{SST,
  title = {{ERAS: Enabling the Integration of Real-World Intellectual Properties (IPs) in Architectural Simulators}},
  author = {Nema, Shubham and Razdan, Rohin and Rodrigues, Arun and Hemmert, Karl and Voskuilen, Gwendolyn and Adak, Debratim and Hammond, Simon and Awad, Amro and Hughes, Clayton},
  abstractNote = {Sandia National Laboratories is investigating scalable architectural simulation capabilities with a focus on simulating and evaluating highly scalable supercomputers for high performance comput- ing applications. There is a growing demand for RTL model integration to provide the capability to simulate customized node architectures and heterogeneous systems. This report describes the ?rst steps integrating the ESSENTial Signal Simulation Enabled by Netlist Transforms (ESSENT) tool with the Structural Simulation Toolkit (SST). ESSENT can emit C++ models from models written in FIRRTL to automatically generate components. The integration work?ow will automatically generate the SST component and necessary interfaces to ?plug? the ESSENT model into the SST framework.},
  doi = {10.2172/1854734},
  url = {https://www.osti.gov/biblio/1854734},
  journal = {Sandia National Labs Tech Report},
  number = {},
  volume = {},
  place = {United States},
  year = {2021},
  month = {9}
}

@inproceedings{nguyen2022gem5sst,
 title = {{gem5/SST Integration 2021: Scaling Full-system Simulations}},
 author = {Hoa Nguyen and Jason Lowe-Power},
 booktitle = {The 4th gem5 Users’ Workshop with ISCA},
 year = {2022}
}

@inproceedings{hsieh2012gem5sst,
  author = {Hsieh, Mingyu and Pedretti, Kevin and Meng, Jie and Coskun, Ayse and Levenhagen, Michael and Rodrigues, Arun},
  title = {{SST + Gem5 = a Scalable Simulation Infrastructure for High Performance Computing}},
  year = {2012},
  isbn = {9781450315104},
  publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
  address = {Brussels, BEL},
  booktitle = {{Proceedings of the 5th International ICST Conference on Simulation Tools and Techniques}},
  pages = {196–201},
  numpages = {6},
  keywords = {architecture, simulation},
  location = {Desenzano del Garda, Italy},
  series = {SIMUTOOLS '12}
}

@INPROCEEDINGS{RogersSlycord2020-gem5Salam,
  author={Rogers, Samuel and Slycord, Joshua and Baharani, Mohammadreza and Tabkhi, Hamed},
  booktitle={{53rd Annual IEEE/ACM International Symposium on Microarchitecture}},
  serie = {MICRO},
  title={{gem5-SALAM: A System Architecture for LLVM-based Accelerator Modeling}},
  year={2020},
  volume={},
  number={},
  pages={471-482},
  doi={10.1109/MICRO50266.2020.00047}
}

@article{SpencerRogers2024-gem5Salam2,
  title = {{Expanding Hardware Accelerator System Design Space Exploration with gem5-SALAMv2}},
  journal = {{Journal of Systems Architecture}},
  volume = {154},
  pages = {103211},
  year = {2024},
  issn = {1383-7621},
  doi = {{https://doi.org/10.1016/j.sysarc.2024.103211  },
  url = {https://www.sciencedirect.com/science/article/pii/S138376212400148}6},
  author = {Zephaniah Spencer and Samuel Rogers and Joshua Slycord and Hamed Tabkhi},
  keywords = {Pre-RTL simulation, Design space exploration, Hardware simulation, Gem5},
  abstract = {With the prevalence of hardware accelerators as an integral part of the modern systems on chip (SoCs), the ability to model accelerators quickly and accurately within the system in which it operates is critical. This paper presents gem5-SALAMv2 as a novel system architecture for LLVM-based modeling and simulation of custom hardware accelerators integrated into the gem5 framework. It overcomes the inherent limitations of state-of-the-art trace-based pre-register-transfer level (RTL) simulators by offering a truly “execute-in-execute” LLVM-based model. It enables scalable modeling of multiple dynamically interacting accelerators with full-system simulation support. To create long-term sustainable expansion compatible with the gem5 system framework, gem5-SALAM offers a general-purpose and modular communication interface and memory hierarchy integrated into the gem5 ecosystem, streamlining designing and modeling accelerators for new and emerging applications. gem5-SALAMv2 expands upon the framework established in gem5-SALAMv1 with improved LLVM-based elaboration and simulation, improved and more extensible system integration, and new automations to simplify rapid prototyping and design space exploration. 11Conference Paper Extension: This work extends the work presented in gem5-SALAM: A System Architecture for LLVM-based Accelerator Modeling from MICRO 2020 (Rogers et al., 2020). This work expands on the aforementioned work by revamping the gem5-SALAM internals to provide more robust and extensible simulations, introducing new automation tools for expanding and simplifying design space exploration, and demonstrates the new capabilities of gem5-SALAMv2 by exploring multiple configurations of simple neural network architectures. Validation on the MachSuite (Reagen et al., 2014) benchmarks presents a timing estimation error of less than 1% against the Vivado High-Level Synthesis (HLS) tool. Results also show less than a 4% area and power estimation error against Synopsys Design Compiler. Additionally, system validation against implementations on an Ultrascale+ ZCU102 shows an average end-to-end timing error of less than 2%. Lastly, we demonstrate the upgraded capabilities of gem5-SALAMv2 by exploring accelerator platforms for two deep neural networks, LeNet5 and MobileNetv2. In these explorations, we demonstrate how gem5-SALAMv2 can simulate such systems and guide architectural optimizations for these types of accelerator-rich architectures. 22The most up-to-date version of gem5-SALAMv2 is available at https://github.com/TeCSAR-UNCC/gem5-SALAM..}
}

@inproceedings{ChaudhariSinclair2025-gem5Accel,
  author = {Chaudhari, Akanksha and Sinclair, Matthew D.},
  title = {{Toward Full-System Heterogeneous Simulation: Merging gem5-SALAM with Mainline gem5}},
  year = {2025},
  booktitle = {6th gem5 Users' Workshop},
  month = {6},
  numpages = {2}
}

@article{sandia_2,
  title = {{SST-GPU: A Scalable SST GPU Component for Performance Modeling and Profiling}},
  author = {Hughes, Clayton and Hammond, Simon David and Hoekstra, Robert J. and Zhang, Mengchi and Liu, Yechen and Rogers, Tim},
  abstractNote = {Programmable accelerators have become commonplace in modern computing systems. Advances in programming models and the availability of unprecedented amounts of data have created a space for massively parallel accelerators capable of maintaining context for thousands of concurrent threads resident on-chip. These threads are grouped and interleaved on a cycle-by-cycle basis among several massively parallel computing cores. One path for the design of future supercomputers relies on an ability to model the performance of these massively parallel cores at scale. The SST framework has been proven to scale up to run simulations containing tens of thousands of nodes. A previous report described the initial integration of the open-source, execution-driven GPU simulator, GPGPU-Sim, into the SST framework. This report discusses the results of the integration and how to use the new GPU component in SST. It also provides examples of what it can be used to analyze and a correlation study showing how closely the execution matches that of a Nvidia V100 GPU when running kernels and mini-apps.},
  doi = {10.2172/1762830},
  url = {https://www.osti.gov/biblio/1762830},
  journal = {Sandia National Lab},
  place = {United States},
  year = {2021},
  month = {1}
}

@article{sandia_3,
  title = {{Balar: A SST GPU Component for Performance Modeling and Profiling}},
  author = {Hughes, Clayton and Hammond, Simon David and Khairy, Mahmoud and Zhang, Mengchi and Green, Roland and Rogers, Timothy and Hoekstra, Robert J.},
  abstractNote = {Programmable accelerators have become commonplace in modern computing systems. Advances in programming models and the availability of massive amounts of data have created a space for massively parallel accelerators capable of maintaining context for thousands of concurrent threads resident on-chip. These threads are grouped and interleaved on a cycle-by-cycle basis among several massively parallel computing cores. One path for the design of future supercomputers relies on an ability to model the performance of these massively parallel cores at scale. The SST framework has been proven to scale up to run simulations containing tens of thousands of nodes. A previous report described the initial integration of the open-source, execution-driven GPU simulator, GPGPU-Sim, into the SST framework. This report discusses the results of the integration and how to use the new GPU component in SST. It also provides examples of what it can be used to analyze and a correlation study showing how closely the execution matches that of a Nvidia V100 GPU when running kernels and mini-apps.},
  doi = {10.2172/1560919},
  url = {https://www.osti.gov/biblio/1560919},
  journal = {Sandia National Lab},
  place = {United States},
  year = {2019},
  month = {9}
}
