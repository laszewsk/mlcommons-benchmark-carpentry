
\section{Definitions}
\label{sec:definition}

In this section, we introduce some of the terminology used throughout this work in order to work towards a formal definition of AI benchmarks.

\subsection{What is benchmarking?}
\label{sec:definition-benchmarking}


Benchmarking is the process of comparing performance measurements for a particular product or system offered by different entities. This can take several forms, including competitive benchmarks that compare different offerings, functional benchmarks that evaluate outcomes across various approaches, and process-oriented benchmarks—an extension of functional benchmarks—that focus on the workflows applied during evaluation. 

The goals of benchmarking include identifying performance gaps, establishing baseline expectations, driving innovation, and supporting continuous improvement over both short- and long-term horizons. Benchmarking has been extensively used in computer engineering and science—across both industry and academia—to measure the performance of computing equipment and the applications running on such systems. These efforts aim to improve understanding of important characteristics relevant to specific communities, including hardware and software (e.g., operating systems, databases, and applications).


\subsection{Lessons learned from traditional HPC benchmarking}

Traditional high-performance computing (HPC) benchmarking includes:  

\begin{enumerate}
\item[a.] \textit{synthetic benchmarks} that simulate characteristic community workloads, as exemplified by the TOP500 and Green500 benchmarks;  
\item[b.] \textit{application benchmarks} that represent real-world applications to measure end-to-end performance, such as SPEC HPC; and  
\item[c.] \textit{scientific application benchmarks} that emphasize the accuracy of computational methods in solving domain-specific scientific problems.
\end{enumerate}

(For a more detailed discussion, see Section \ref{sec:hpc})

Important design and applicability criteria for benchmarks include relevance and representativeness for the field, fairness, repeatability, cost-effectiveness, scalability, and transparency~\cite{wikipedia:benchmarking}. One caveat is that vendors may optimize hardware specifically for these benchmarks, potentially neglecting new real-world problems and emerging challenges not captured by traditional benchmark suites.  

Therefore, it is essential to provide a diverse set of benchmarks so that different communities can evaluate and interpret results in terms of the performance metrics most relevant to their specific needs.  

HPC benchmarking has traditionally focused on supercomputing performance comparisons, targeting compute performance~\cite{Dongarra1989LinpackReport,Dongarra2016HPCG}, as well as memory, communication, and storage performance~\cite{PerfKitBenchmarker,IO500}. With the resurgence of AI and machine learning—including deep learning—it is now appropriate to explore additional lessons for benchmarking drawn from these domains.  

HPC benchmarks are often executed under controlled conditions, such as those maintained by system administrators, to ensure exclusive access to hardware and eliminate interference from other users or applications. This approach allows for measurement of the best achievable performance and is frequently used to guide system procurement decisions. However, such conditions do not reflect the shared nature of most computing environments, which often include factors such as queue wait times and concurrent multi-user workloads sharing hardware resources.


\subsection{What is democratization?}
\label{sec:definition-democratization}

We believe it is vital not only to allow experts and power users to participate in benchmarking efforts but also to lower barriers to entry — making powerful benchmarks, tools, knowledge, and infrastructure available to everyone, not just those with specialized resources or expertise.

For benchmarking, this implies in particular to improve the following:-

\begin{enumerate}
\item[a.] {\bf Accessibility:} Making benchmarks easier to use, enforcing  open-source licensing.

\item[b.] {\bf Open participation:} Encouraging community contributions through open-source development (e.g, on GitHub; shared repositories with transparent governance).

\item [c.] {\bf Knowledge sharing:} Providing tutorials, documentation, and educational resources so that non-experts can effectively use and modify the benchmarks.

\item[d.] {\bf Affordability:} Reducing cost barriers not only by introducing open source benchmarks, but also by allowing benchmarks to be offered at various scales and not only for leadership-class computing resources.

\end{enumerate}


\subsection{AI Software Democratization}

One of the major success stories in the field of artificial intelligence is the emergence of AI-specific software libraries such as TensorFlow, PyTorch, and Jupyter Notebooks. These tools have democratized machine learning and data science by making advanced computational capabilities accessible to students, researchers, and small organizations that previously lacked the resources to develop such tools from scratch.

\subsection{AI Hardware Democratization}

One must recognize that a significant amount of progress in AI research is conducted on campus computers that are much smaller than hyperscale AI machines or leadership-class government systems. Furthermore, many scientists have begun to use {\em desktop} computers equipped with high-powered graphics cards. Hence, it is important to have meaningful AI benchmarks available that allow for comparisons across different scales.


\subsection{What is Software Carpentry?}
\label{sec:towards-carp}

To set the stage for why we need AI benchmark carpentry, we need to first look at how the term has been introduced and is now commonly associated with software carpentry. After a more detailed analysis of software carpentry, we define the term AI benchmark carpentry.

Software Carpentry \cite{wilson2014software} was initially conceived to teach researchers in scientific fields fundamental computational and software development skills. Thus, non-computer scientists would improve the use and development of the software they need to conduct their own research while benefiting from targeted, short educational tutorials. 

Today, a global community effort has sprung up since 2018~\cite{softwarecarpentry2024} that provides a number of training materials and sessions to the scientific community to we can leverage in some extend. Recently, additional areas beyond software, such as data \cite{teal2015data,datacarpentry2025} and library carpentry \cite{baker2016library,librarycarpentry2025}, have been added, but does not cover benchmarking sufficiently.
Together, this includes:

\begin{itemize}
  \item \textbf{Software Carpentry Core Efforts:} 
  Teach researchers foundational computing skills to enhance their productivity and efficiency in research tasks. This includes lessons in 
  Programming with Python, Version Control with Git, The Unix Shell, Programming with R, Building Programs with Python, Automation, and Make.

 \item \textbf{Data Carpentry Efforts:}
  Teach researchers skills necessary to work effectively and reproducibly with data. This includes Data Analysis and Visualization in R for Social Scientists, Data Analysis and Visualization in Python for Social Scientists, Data Cleaning with OpenRefine, Spreadsheets for Data Organization, SQL for Data Management, Ecology Workshop (R, spreadsheets, OpenRefine, SQL), Genomics Workshop, and Geospatial Data Workshop.

  \item \textbf{Library Carpentry Efforts:}
  Teach how to develop software libraries.
  
  \item \textbf{Other Carpentry Efforts:}
   Additional lessons available include High-Performance Computing (HPC Carpentry) \cite{reid2025hpc,HPCcarpentry2025}, Cloud Computing, FAIR Data and Software, and Machine Learning for Domain Scientists.
  
  \item \textbf{Instructor Training:} Educate individuals to teach coding and data workshops effectively.
\end{itemize}

From this list, we see that benchmark carpentry is missing.


\subsection{Benchmark carpentry}
\label{sec:benchmark-carpentry}

Based on our observations in the educational and scientific communities, we find that similar efforts are needed to focus on benchmarking.
This is more important as AI applications consume enormous resources, and properly scaling and using them requires a much deeper understanding of their time and space requirements.
The hope is that, from similar benchmarks, not only can the scientist learn lessons about their own applications, but, if needed, their own benchmarks can be developed to estimate costs and effort more precisely.
In addition, reproducible, portable benchmarks enable the selection and comparison of suitable hardware for the effort.

In general, we distinguish between hardware, software, and application components that significantly impact benchmarks.

On the hardware side, we deal with compute-oriented components such as CPUs, GPUs, and/or AI/neural accelerators (NPUs). Benchmarking them in the traditional way includes processing speed, core utilization, and instruction efficiency of a computer's central processing unit, data movement between xPU and main memory, to name a few. However, for AI, we also need performance in parallel computation, as well as AI workloads derived from AI kernels and applications.

As many AI applications require a large amount of \text{data} to be moved between memory, disks, and the CPUs and GPUs' memory, evaluating bandwidth, latency, and throughput is critical to understanding their impact on system performance. Hence, estimating and measuring the impact of, for example, assessing read/write speeds, IOPS, and access latency to identify bottlenecks in data storage systems is important.

Related to this is the \text{Network performance} metric, which measures bandwidth, latency, and packet loss to ensure efficient data transfer across systems, especially when parallel processing is used to address the scale required for good performance.

An additional component that needs to be integrated into the benchmark carpentry is to teach about \textbf{System Profiling and Monitoring} principles and tools so as to measure real-time system metrics in order to detect performance bottlenecks and resource usage, as well as potential patterns that provide more insight into the analysis of the application.

Obviously, \textbf{Interpreting Results, Bottleneck Analysis, and Performance Optimization} must be taught to interpret the benchmarks as well as identify limitations and improve through iterative strategies the overall performance, where applicable.

To achieve comparable results, the \textbf{Benchmark Design and Reproducibility} needs to be taught.
This includes fair, repeatable benchmarks that reflect real-world workloads and enable comparative analysis. Apparently, we also need to address portability across different hardware and scales.