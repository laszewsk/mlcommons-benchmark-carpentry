
\TODO{Author Order TBD}

\href{mailto:laszewski@gmail.com,christine@sdsc.edu,luszczek@icl.utk.edu,gregg.barrett@cirrusai.net,a.foundjem@polymtl.ca,gavinmichael.farrell@phd.unipd.it,memani@anl.gov,svmoore@utep.edu,sujatagoswami@lbl.gov,sinclair@cs.wisc.edu,shivaram@cs.wisc.edu,rnjain@wisc.edu,kongtao@google.com,kartikmathur@microsoft.com,sasidhar.kunapuli@gmail.com,victorjunlu@gmail.com,knmorehouse@gmail.com,tianhao.li@duke.edu,sebastian.lobentanzer@helmholtz-munich.de,kzm6@cornell.edu,Singh.Tejinder@Dell.com,a.alsudais@psau.edu.sa,gcfexchange@gmail.com,t.jeyan@stfc.ac.uk,juri.papay@stfc.ac.uk,brewerwh@ornl.gov,vj@eecs.harvard.edu?subject=Carpentry%20Paper%20Collaboration%20Discussion&body=Dear%20all,%0A%0APlease%20find%20attached%20the%20latest%20draft%20at%0A%0Ahttps://www.overleaf.com/project/68f8f5b979760743fe572121%0A%0ABest,%0AGregor}{Email all authors}


\begin{abstract}
 
Benchmarks are one cornerstone of modern machine learning practice, providing standardized evaluations that enable reproducibility, comparison, and scientific progress. 
However, AI benchmarks are and become increasingly complex and special care has to be taken into account to deal with this complexity.
This includes the integration of dynamic workflows; their models evolve rapidly in scale AI models rapidly evolve in architecture, scale, and capability; datasets evolve; and deployment contexts continuously change, creating a moving target for evaluation.
Besides standard accepted fixed benchmarks as we know from the traditional computing community, we need to develop and evolve continuous adaptive benchmarking frameworks, both scientific assessment and real-world deployment risk becoming misaligned with actual system behavior.
This requires the skills and educated community to foster usefulness of such benchmarks in the scientific community. 

Drawing on our experience from MLCommons, educational initiatives, and government programs such as the DOE's Trillion Parameter Consortium, we identify key barriers that hinder the broader adoption, utility, and evolution of benchmarking in AI. These include substantial resource demands, limited access to specialized hardware, lack of expertise in benchmark design, and uncertainty among practitioners about how to relate benchmark results to their own application domains. Moreover, current benchmarks often emphasize peak performance on leadership-class hardware, offering limited guidance for more diverse, real-world deployment scenarios. This may include applications to smaller compute resources, but also to larger systems such as predeployed LLMs by commercial entities.

We argue that benchmarking itself must become dynamic in order to incorporate evolving models, updated data, and heterogeneous computational platforms while maintaining transparency, reproducibility, and interpretability. Democratizing this process requires not only technical innovation, but also systematic educational efforts as part of AI benchmark carpentry offerings spanning undergraduate to professional levels to develop sustained expertise in benchmark design and use. Finally, benchmarks should be framed and communicated to support application-relevant comparisons, enabling both developers and users to make informed, context-sensitive decisions. Advancing dynamic and inclusive benchmarking practices will be essential to ensure that evaluation keeps pace with the evolving AI landscape and supports responsible, reproducible, and accessible AI deployment. Furthermore, we believe that it is timely to provide a solid foundation for designing, using, and evolving benchmarks through community efforts that allows us to enable the concept of {\em AI benchmark carpentry.}

\begin{comment}
\TODO{OLD: In this document we describe the importance of benchmarks in relationship to AI and especially deep learning. Our experience is gained from working with MLCommons but also while working in educational settings as well as government settings such as the million parameter consortium headed by DOE. What we observed is that in many cases benchmarking in this domain is significantly more challenging due to a number of reasons. This includes the scale in time and space of the benchmarks, the expertise and access to hardware to run them, and the lack of experience in benchmarks in general. On the other hand it is often not yet clear to scientists how to use the benchmarks or apply them to their own applications. Therefore, efforts need to be projected to democratize such benchmark endeavors to make the more available and transparent to the users. Preparedness to use and develop such benchmarks must be accompanied by educational activities starting from the undergraduate level to the professional developers. The goal is to develop and contribute to repeatable benchmarks that can be adopted to a wide variety of applications with various scales in algorithmic complexity but also hardware requirements. The benchmarks must be communicated and collected in a fashion allowing meaningful comparisons for application users that go beyond the demonstration of them on a leadership class hardware platform.}
\end{comment}

\end{abstract}

%\twocolumn

\begin{IEEEkeywords}
benchmark, AI benchmark, AI benchmark carpentry, AI benchmark democratization, MLCommons
\end{IEEEkeywords}

\bigskip
