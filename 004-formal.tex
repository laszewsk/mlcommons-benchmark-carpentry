\section{Towards AI specific benchmarks}
\label{sec:towards}


As part of the MLCommons Science Working group meetings, we have identified that ingredients of ML benchmarks include:

\begin{enumerate}
\item Datasets (such as images, application specific scientific data, time series)
\item Tasks to be performed 
\item Methods to perform these tasks (such as machine language model, language models)
\item Metrics (accuracy; performance of the resource executing the task at runtime, such as  
  space, 
  memory usage, 
  energy efficiency, 
  power draw)
\item ML oriented performance impacts such as Latency impacted by the time per inference, Throughput for the  inferences per second, 
and training time to reach target accuracy.

\item Replication which includes the ability to replicate the experiment while at the same time being able in a structured fashion to compare the results.
\end{enumerate}

\subsection{Formalization}
\label{subsec:towards-formal}

To formalize the specification of a benchmark we introduce the following notation

\[  B = (I, D, T, M, C_B, R) \]

\[
\begin{array}{ll}
B & = \text{Benchmark} \\
I & = \text{Infrastructure} \\
D & = \text{Dataset} \\
T & = \text{Scientific Task} \\
M & = \text{Metrics} \\
C_B & = \text{Benchmark Constraint} \\
R & = \text{Results}
\end{array}
\]

Further we define the task to be executed as an application applied to a set of parameters.

\[ T = (A, P) \]

\[
\begin{array}{ll}
A & = \text{Application} \\
P & = \text{Parameters}
\end{array}
\]

Each of $B, I, D, T, M. R, A$ can have constraints. This is notated with 

\[C_c ~|~ c \in \{B, I, D, T, M. R, A\}\]

And can be simply written in general as 

\[ c = (c, C_c) ~|~ c \in \{B, I, D, T, M, R, A\} \]
if needed.

 Next we describe briefly each of the parts that build a benchmark in more detail.

\subsection{Infrastructure}
\label{subsec:towards-infra}

Infrastructure refers to the computational and software environment required to execute the scientific task.

This includes computational hardware, software libraries, operating systems, and cloud platforms, but also power related infrastructure to operate the resources.
In many cases some of these parameters are not defined and are open targets to conduct benchmark comparison within the various infrastructures provided.
However a single benchmark must be clearly described for each infrastructure on which it is executed with as many parameters as possible described.

Clearly defined infrastructure will help with (a) reproducibility, as it ensures results can be reproduced across different environments, (b) fairness, as it identifies clearly the differences between different hardware and  software used, (c) scalability as through comparison we can identify various scalability issues and properties, (d) efficiency, as we can assess  resource use in regards to common metrics such as time, space, energy, and cost.

\subsection{Dataset}
\label{subsec:towards-dataset}

A dataset or multiple datasets provide the input data for the scientific task to be performed.
It is often delivering a sufficiently detailed and sized training data set to  evaluating various approaches for identifying AI models and their parameters.
In many cases, it is important to provide different sizes of data sets to enable (a) a small set for fast development of the approach, and (b) a larger set that fosters scientific accuracy with longer run-times.
Intermitted sizes are also sometimes needed to adapt to available resource constraints to compare them on different scales.
Data should always be sufficiently described through for example metadata or documentation so their context within the scientific application can be determined.
Through these efforts we support the establishment of (a) 
a ground truth, that serves as the basis for evaluating the scientific accuracy (b) a relevant and representative example that is influential for the scientific application, and (c) the identification of bias for data-driven applications.

It is important that we distinguish two different data sets.
Although one would naively limit benchmarks to static data to limit the benchmarks on a defined set of data, recent efforts show in some cases we need to consider live data ingestion into benchmarks as they are already a standard way of conducting science in for example earth science.
We term such datasets {\em living dataset}.
Such living dataset are continuously updated with new data, edge cases, or corrections.
Such live datasets could be real-time data, but could also include a simulation of real-time data while using a static dataset that is simulating the real time data update. 
Obviously the simulated living data will be easier to use when trying to compare different approaches to the scientific task.

The important part about live datasets is that it allows the identification of how a benchmarking task keeps relevant over time while improving it throughout time.
It also can be used to identifying if it can adapt to issues such as over- and underfitting.
Naturally, it encourages the development of solutions that provide continuous progress through evolving challenges set by scientific tasks.

\subsection{Scientific Task}
\label{subsec:towards-task}

The scientific task identifies the core challenge being evaluated while precisely identifying its purpose within the scientific goals to be achieved. 
It also identifies simple classifications of AI tasks such as classification, translation, reasoning, time series prediction, or planning conducted within the scientific applications.

Through the precise put short definition of task it sets the scope of the benchmark and introduces the community to the task to be executed and/or measured.

It sets the stage for specific research or application goals, while also providing input to the reasoning that determines dataset and the scientific metric definition.
Most importantly it sets the stage for how different approaches can be compared by conduction the same scientific task.

\subsection{Metrics}
\label{subsec:towards-metrics}

Metrics are quantitative measures used to assess the performance conducted to achieve the scientific task.
A Benchmark can have many metrics and it is important to document the objective that is supposed to be achieved while utilizing a specific metric.

The goal of such metrics includes to identify the scientific accuracy of the benchmark in order to compare different models, parameters and datasets used in the benchmark.
Metrics can also be used to identify tradeoffs in for example speed or accuracy.
Most importantly, metrics can be used to establish a ranking of various benchmarks performed in similar circumstances under constraints.
They help communicate the strengths and weaknesses of the benchmark in regards to the metric chosen.

\subsection{Benchmark Constraints}
\label{subsec:towards-constraints}

In many cases it is necessary to provide constraints to the benchmark as it will be establishing rules for conducting them and their evaluation.
This may include limits to training, inference, model size or the data used. 
Introducing constraints my foster (a) fairness wile executing the benchmark (b) introducing operational real-world limitations, and (c) support and simplify the experiment setup. 

\subsection{Results}
\label{subsec:towards-res}

A benchmark must produce results that are easy to comprehend to allow identifying the outcome of the task performed by the benchmark.
This includes scientific accuracy of the task performed, but also a number of information allowing comparison while using different methods, models, and parameters.
To simplify comparison it is desirable to create if possible comparative scores (which the accuracy is one of them), charts, tables and an understanding of the scientific error analysis.
Through these efforts one can than identify through comparison (a) progress over time, (b) inform stakeholders about model capabilities, (c) identify limitations of the method including parameters used in the benchmark, and (d) establish a potential leader board for selecting suitable candidates that may be applicable to similar scientific tasks.

\input{3-graph}
\input{4-graph-mlcommons}

\begin{comment}
    
Geoffrey noted many sources already available for Science benchmarks

MLCommons Science Working Group

MLCommons HPC Working Group

FastML Inference benchmarks for HEP covering different interesting regions in latency-datarate space  230322 mlcommons science.pdf t https://arxiv.org/abs/2207.07958 https://github.com/fastmachinelearning/fastml-science 

NSF HDR ML Challenge https://www.nsfhdr.org/mlchallenge 

UVA FAIR SBI Initiative including some of 
CaloChallenge packaged for easy use
CaloChallenge 2022: A Community Challenge for Fast Calorimeter Simulation http://arxiv.org/abs/2410.21611 https://arxiv.org/abs/2406.12898 

Google WeatherBench2 https://sites.research.google/weatherbench/ https://arxiv.org/abs/2308.15560 

SciML-Bench from RAL, UK https://github.com/stfc-sciml/sciml-bench

There are also Big Data benchmarks such as BigDataBench https://www.benchcouncil.org/BigDataBench

Links pointed out that may be useful:
please add here, also add small paragraph what they contribute
Life Science Benchmarking Examples (Gavin Farrell)
https://openebench.bsc.es/ - registry hosting benchmarking efforts ons specific life science challenges
Challenge examples include protein prediction for intrinsically disordered proteins i.e. the ones not possible to currently predict in AlphaFold due to their flexibility - https://caid.idpcentral.org/challenge (CAID)
Also the CASP challenge also related to protein prediction: https://predictioncenter.org/index.cgi 
Also the Dream challenges in bioinformatics where they target benchmarking and optimizing specific issues: example \url{https://dreamchallenges.org/olfactory-mixtures-prediction-challenge/} 
\url{https://sab.noaa.gov/wp-content/uploads/4.0-DL4NWP_NOAAResponse_Nov2024.pdf} 

\end{comment}

\begin{comment}
This document collects the ideas about what is needed for researchers to familiarize themselves with benchmarking. In particular, it will help by supporting the MLCommons community to educate the next generation of researchers to familiarize them with benchmarking. 
 
\TODO{IN THE NEXT SECTIONS WE WANT TO DESCRIBE EACH OF THEM  IN MORE DETAIL}

\TODO{INTEGRATE THE HIERARCHY DIAGRAMS}

\TODO{INTEGRATE THE TABLES AND MAKE A SURVEY OF BENCHMARKS}

\TODO{Integrate \url{https://www.nature.com/articles/s42254-022-00441-7}, also paper with Gregor}

\TODO{OLDER NOTES:}

HPC benchmarking
TOP500
Green 500 and HPC innovation: requires monitoring of instantaneous power consumption (levels 1, 2, 3)
Desktop benchmarking
specint/float (int8..in32 and FP64 down to FP8)
Processors and their cores: sockets and NUMA issues
GPUs: on-node accelerator connectivity
filesystem
. . .
Compute center benchmarking
Which resources and applications are we interested in, 
Which dimensionalities are measured and what is the objective
\end{comment}

\section{Carpentry}
\label{subsec:towards-carp}

\TODO{This section may be replicated with the definition section so we may have to merke it}

To understand what AI Benchmark carpentry is about we need to first look at how the term has been introduced and is now commonly associated with software carpentry. After a more detailed analysis of software carpentry we define the term benchmark carpentry.

\subsection{Software Carpentry}
\label{subsec:towards-swCarp}

Software Carpentry was original conceived \cite{wilson2014software} to teach researchers fundamental computational and software development skills to researchers in scientific fields.
Thus, non-computer scientists would improve the use and development of software they need for conducting their own research.

Today a global community effort has sprung up since 2018~\cite{softwarecarpentry2024} that provides a number of training material and sessions to the community to assist in this effort.
Recently additional areas other than software such as data and library carpentry have been added.
Together this includes:

\begin{itemize}
  \item \textbf{Software Carpentry Core Lessons:} 
 Teach researchers foundational computing skills to enhance their productivity and efficiency in research tasks. This includes lessons in 
  Programming with Python, Version Control with Git, The Unix Shell, Programming with R, Building Programs with Python, Automation and Make

  \item \textbf{Data Carpentry Lessons:}
  Teach researchers skills necessary to work effectively and reproducibly with data. This includes
  Data Analysis and Visualization in R for Social Scientists, Data Analysis and Visualization in Python for Social Scientists, Data Cleaning with OpenRefine, Spreadsheets for Data Organization, SQL for Data Management, Ecology Workshop (R, spreadsheets, OpenRefine, SQL), Genomics Workshop, Geospatial Data Workshop

  \item \textbf{Library Carpentry Lessons:}
  Teach how to develop software to develop software libraries.
  
  \item \textbf{Other Carpentries Lessons:}
   Additional lessons available include High-Performance Computing (HPC Carpentry), Cloud Computing, FAIR Data and Software, Machine Learning for Domain Scientists
  
  \item \textbf{Instructor Training} Educate individuals to teach coding and data workshops effectively.
\end{itemize}
