
\section{Definitions}
\label{sec:definition}

In this section, we introduce some of the terminology used throughout this work in order to work towrads a formal definition of AI benchmarks.

\subsection{What is benchmarking?}
\label{sec:definition-benchmarking}


Benchmarking is the process of comparing performance measurements for a particular product or system offered by different entities. This can take several forms, including competitive benchmarks that compare different offerings, functional benchmarks that evaluate outcomes across various approaches, and process-oriented benchmarks—an extension of functional benchmarks—that focus on the workflows applied during evaluation. 

The goals of benchmarking include identifying performance gaps, establishing baseline expectations, driving innovation, and supporting continuous improvement over both short and long time horizons. Benchmarking has been extensively used in computer engineering and science—across both industry and academia—to measure the performance of computing equipment and the applications running on such systems. These efforts aim to improve understanding of important characteristics relevant to specific communities, including hardware and software (e.g., operating systems, databases, and applications).


\subsection{Lessons learned from traditional HPC benchmarking}

Traditional high-performance computing (HPC) benchmarking includes:  

\begin{enumerate}
\item[a.] \textit{synthetic benchmarks} that simulate characteristic community workloads, as exemplified by the TOP500 and Green500 benchmarks;  
\item[b.] \textit{application benchmarks} that represent real-world applications to measure end-to-end performance, such as SPEC HPC; and  
\item[c.] \textit{scientific application benchmarks} that emphasize the accuracy of computational methods in solving domain-specific scientific problems.
\end{enumerate}


Important design and applicability criteria for benchmarks include relevance and representativeness for the field, fairness, repeatability, cost-effectiveness, scalability, and transparency~\cite{wikipedia:benchmarking}. One caveat is that vendors may optimize hardware specifically for these benchmarks, potentially neglecting new real-world problems and emerging challenges not captured by traditional benchmark suites.  

Therefore, it is essential to provide a diverse set of benchmarks so that different communities can evaluate and interpret results in terms of the performance metrics most relevant to their specific needs.  

HPC benchmarking has traditionally focused on supercomputing performance comparisons, targeting compute performance~\cite{Dongarra1989LinpackReport,Dongarra2016HPCG}, memory, communication, and storage performance~\cite{PerfKitBenchmarker,IO500}. With the resurgence of AI and machine learning—including deep learning—it is now appropriate to explore additional lessons for benchmarking drawn from these domains.  

HPC benchmarks are often executed under controlled conditions, such as those maintained by system administrators, to ensure exclusive access to hardware and eliminate interference from other users or applications. This approach allows for measurement of the best achievable performance and is frequently used to guide system procurement decisions. However, such conditions do not reflect the shared nature of most computing environments, which often include factors such as queue wait times and multi-user workloads sharing hardware resources concurrently.


\subsection{What is democratization?}
\label{sec:definition-democratization}

We believe that it is important  to not only allow the experts and power users o participate in benchmarking efforts but to  lowering barriers to entry — making powerful benchmarks and tools, knowledge, and infrastructure available to everyone, not just those with specialized resources or expertise.

For benchmarking this means in particular to improve

\begin{enumerate}
\item[a.] {\bf Accessibility:} Making benchmarks easier to use, enforcing  open-source licensing.

\item[b.] {\bf Open participation:} Encouraging community contributions through open-source development (e.g on GitHub in shared repositories with transparent governance).

\item [c.] {\bf Knowledge sharing:} Providing tutorials, documentation, and educational resources so that non-experts can effectively use and modify the benchmarks.

\item[d.] {\bf Affordability:} Reducing cost barriers not only by introducing open source benchmarks, but to allow benchmarks to be offered at various scales and not only for leadership class computing resources.

\end{enumerate}


\subsection{AI Software Democratization}

One of the major success stories in the field of artificial intelligence is the emergence of AI-specific software libraries such as TensorFlow, PyTorch, and Jupyter Notebooks. These tools have democratized machine learning and data science by making advanced computational capabilities accessible to students, researchers, and small organizations that previously lacked the resources to develop such tools from scratch.

\subsection{AI Hardware Democratization}

One must recognize that a significant amount of progress in AI research is conducted on campus computers that are much smaller than hyperscale AI machines or leadership-class government systems. Furthermore, many scientists have begun to use {\em desktop} computers equipped with high-powered graphics cards. Hence, it is important to have meaningful AI benchmarks available that allow for comparisons across different scales.



\subsection{Benchmark carpentry}
\label{sec:benchmark-carpentry}

\TODO{why is benchmarking so hard and what can we do to simplify it. Can we learn from Software carpentry?}

From our observations in the educational and scientific communities we observed that similar efforts need to be placed on benchmarking.
This is the more important as AI applications take enormous amount of resources and proper scaling and utilization of such applications require a much deeper understanding of their time and space efforts.
The hope is that from similar benchmarks not only lessons can be learned by the scientist about their own applications but if needed their own benchmarks can be developed to more precisely estimate cost and efforts.
In addition having reproducible and portable benchmarks allows the selection and comparison of suitable hardware for the effort.

We distinguish in general hardware, software, and applictaion components that significantly impact benchmarks.

On the hardware side we deal with compute oriented components such as \textbf{CPU}s and \textbf{GPU}s.
Benchmarking them in the traditional way include processing speed, core utilization, and instruction efficiency of a computer's central processing unit. 
However for AI we also need performance in parallel computation, and AI workloads derived from AI kernels and applications.

As many AI applications require a large amount of \text{data} to be moved between memory, disks, and the CPUs and GPUs memory to valuate the bandwidth, latency, and throughput of memory, disk and storage to understand its impact on system performance.
Hence, estimating and measuring the impact that, for example, assessing read/write speeds, IOPS, and access latency to identify bottlenecks in data storage systems is of importance.

Related to this is the \text{Network performance} to measure bandwidth, latency, and packet loss to ensure efficient data transfer across systems especially when parallel processing is used to address the scale needed to achieve good performance.

An additional component that needs to be integrated into the benchmark carpentry is to teach about \textbf{System Profiling and Monitoring} principles and tools so to measure real-time system metrics in order to detect performance bottlenecks and resource usage as well as potential patterns that provide more insight in the analysis of the application.

Obviously, \textbf{Interpreting Results, Bottleneck Analysis, and Performance Optimization} must be taught to interpret the benchmarks as well as identify limitations and improve through iterative strategies the overall performance where applicable.

To achieve comparable results the \textbf{Benchmark Design and Reproducibility} needs to be taught.
This includes fair, repeatable benchmarks that reflect real-world workloads and enable comparative analysis. Obviously we also need to address portability across different hardware and scales.